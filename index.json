[{"content":"Easy Rust — a book about rust\n Many companies and people now learn Rust, and they could learn faster with a book that has easy English. This textbook is for these companies and people to learn Rust with simple English.\n The Rust Programming Language — \u0026ldquo;Rust book\u0026rdquo;. It\u0026rsquo;s a standard choice for those who want to start with Rust. A bit difficult to read sometimes.\nThe Rust Standard Library — documentation of the standard library.\nRust by Example — \u0026ldquo;Rust by Example (RBE) is a collection of runnable examples that illustrate various Rust concepts and standard libraries\u0026rdquo;\nRust code snippets String basics // Statically allocated string slice let hello = \u0026quot;Hello\u0026quot;; // Equivalent to the previous one, a bit more verbose let hello_again: \u0026amp;'static str = \u0026quot;Hello\u0026quot;; // create a mutable empty String let mut s = String::new(); // A mutable empty String with a pre-allocated buffer let mut s_with_capacity = String::with_capacity(10); // Add a string slice to a String s.push_str(\u0026quot;foo\u0026quot;); // Convert from a string slice to a String let foo = \u0026quot;foo\u0026quot;.to_string(); // another way let bar = String::from(\u0026quot;bar\u0026quot;); // Coerce a String into \u0026amp;str with \u0026amp; let baz: \u0026amp;str = \u0026amp;bar;  Iteration fn main() { let a = [1, 2, 3]; // forward for x in a.iter() { print!(\u0026quot;{} \u0026quot;, x); } // backward for x in a.iter().rev() { print!(\u0026quot;{} \u0026quot;, x); } }  playground\nTesting fn f(x: i32) -\u0026gt; i32 { x * 2 } fn f_unwrap(x: Option\u0026lt;i32\u0026gt;) -\u0026gt; i32 { x.unwrap() } // The module is only built when testing #[cfg(test)] mod test { // loading functions from the parent space use super::f; use super::f_unwrap; // actual test #[test] fn test_f() { assert_eq!(f(12), 24); } // another test #[test] fn test_f_unwrap_ok() { f_unwrap(Some(12)); } // verify that function panics #[test] #[should_panic] fn test_f_unwrap_must_fail() { f_unwrap(None); } }  playground\n","permalink":"https://serge-m.github.io/posts/learning-rust/","summary":"Easy Rust — a book about rust\n Many companies and people now learn Rust, and they could learn faster with a book that has easy English. This textbook is for these companies and people to learn Rust with simple English.\n The Rust Programming Language — \u0026ldquo;Rust book\u0026rdquo;. It\u0026rsquo;s a standard choice for those who want to start with Rust. A bit difficult to read sometimes.\nThe Rust Standard Library — documentation of the standard library.","title":"Learning Rust"},{"content":"I started to learn rust, and I needed to set up some IDE to handle my exercises. It seems that VScode has pretty good support for Rust.\nHow to set up VS code to work with rust, including debugging. Install rust-analyzer extension and vscode-lldb extension:\ncode --install-extension matklad.rust-analyzer code --install-extension vadimcn.vscode-lldb  rust-analyzer provides a nice UI for running binaries and tests:\nAlso it gives a lot of hints:\nNote that rust-analyzer doesn\u0026rsquo;t work together with Rust extension (Rust for Visual Studio Code). You would have to uninstall or debug the later.\nWe needed vscode-lldb to support debugging. Now if you hit F5 VScode will suggest to create a launch configuration for it. The default one works well. After that one can use keyboard shortcuts to debug.\n{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;lldb\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Debug executable 'vscode_rust_template'\u0026quot;, \u0026quot;cargo\u0026quot;: { \u0026quot;args\u0026quot;: [ \u0026quot;build\u0026quot;, \u0026quot;--bin=vscode_rust_template\u0026quot;, \u0026quot;--package=vscode_rust_template\u0026quot; ], \u0026quot;filter\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;vscode_rust_template\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;bin\u0026quot; } }, \u0026quot;args\u0026quot;: [], \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;lldb\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Debug unit tests in executable 'vscode_rust_template'\u0026quot;, \u0026quot;cargo\u0026quot;: { \u0026quot;args\u0026quot;: [ \u0026quot;test\u0026quot;, \u0026quot;--no-run\u0026quot;, \u0026quot;--bin=vscode_rust_template\u0026quot;, \u0026quot;--package=vscode_rust_template\u0026quot; ], \u0026quot;filter\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;vscode_rust_template\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;bin\u0026quot; } }, \u0026quot;args\u0026quot;: [], \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; } ] } ","permalink":"https://serge-m.github.io/posts/ide-for-rust/","summary":"I started to learn rust, and I needed to set up some IDE to handle my exercises. It seems that VScode has pretty good support for Rust.\nHow to set up VS code to work with rust, including debugging. Install rust-analyzer extension and vscode-lldb extension:\ncode --install-extension matklad.rust-analyzer code --install-extension vadimcn.vscode-lldb  rust-analyzer provides a nice UI for running binaries and tests:\nAlso it gives a lot of hints:\nNote that rust-analyzer doesn\u0026rsquo;t work together with Rust extension (Rust for Visual Studio Code).","title":"IDE for Rust"},{"content":"Docker ARG vs ENV https://vsupalov.com/docker-arg-vs-env/\nRunning GUI apps in docker ROS GUI in docker: https://tuw-cpsg.github.io/tutorials/docker-ros/\nROS GUI with NVIDIA: https://github.com/dkarunakaran/rviz_docker, https://medium.com/intro-to-artificial-intelligence/rviz-on-docker-bdf4d0fca5b\nrocker A tool to run docker images with customized local support injected for things like nvidia support. And user id specific files for cleaner mounting file permissions.\nrocker on github\nBuild arguments example Dockerfile:\nFROM busybox ARG user USER $user # ...  A user builds this file by calling:\n$ docker build --build-arg user=what_user .  Use an ARG in Dockerfile FROM for dynamic image specification\n.dockerignore  Docker CLI will only look for .dockerignore file in the root directory of the context\n Official documentation\nSee also  Run docker as pytest fixture  OpenVPN server in cloud using docker   ","permalink":"https://serge-m.github.io/posts/notes-on-docker/","summary":"Docker ARG vs ENV https://vsupalov.com/docker-arg-vs-env/\nRunning GUI apps in docker ROS GUI in docker: https://tuw-cpsg.github.io/tutorials/docker-ros/\nROS GUI with NVIDIA: https://github.com/dkarunakaran/rviz_docker, https://medium.com/intro-to-artificial-intelligence/rviz-on-docker-bdf4d0fca5b\nrocker A tool to run docker images with customized local support injected for things like nvidia support. And user id specific files for cleaner mounting file permissions.\nrocker on github\nBuild arguments example Dockerfile:\nFROM busybox ARG user USER $user # ...  A user builds this file by calling:","title":"Notes on docker"},{"content":"Shortest path problem Dijkstra\u0026rsquo;s algorithm  fixes a single node as the \u0026ldquo;source\u0026rdquo; node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.\n Wiki Dijkstra\u0026rsquo;s algorithm\nComplexity analysis Wiki\nOn stack overflow: Why does Dijkstra\u0026rsquo;s algorithm use decrease-key?\nActually Implementing Dijkstra\u0026rsquo;s Algorithm - complexity analysis considering three components of the main loop: time per insert, extract_min, and change_priority operation:\n O(nT_ins + nT_min + m*T_change)\n Versions:\n  Textbook Dijkstra: the version commonly taught in textbooks where we assume that we have a priority queue with the \u0026ldquo;decrease key\u0026rdquo; operation. As we said, this often does not hold true in reality.\n  Linear-search Dijkstra: the most naive implementation, but which is actually optimal for dense graphs.\n  Lazy Dijkstra: practical version which does not use the \u0026ldquo;decrease key\u0026rdquo; operation at all, at the cost of using some extra space.\n  BST Dijkstra: version which uses a self-balancing binary search tree to implement the priority queue functionality, including the \u0026ldquo;decrease key\u0026rdquo; operation.\n  Theoretical Dijkstra: version that uses a Fibonacci heap for the priority queue in order to achieve the fastest possible runtime in terms of big-O notation. This is actually impractical due to the complexity and high constant factors of the Fibonacci heap.\n  Roughly, each of the 5 versions corresponds to a different data structure used to implement the priority queue. Throughout the post, let n be the number of nodes and m the number of edges. Here is summary of the resulting runtime and space complexities:\n  Textbook Dijkstra: indexed binary heap. Runtime: O(m*log n); space: O(n).\n  Linear-search Dijkstra: unordered array. Runtime: O(n^2); space: O(n).\n  Lazy Dijkstra: binary heap. Runtime: O(m*log n); space: O(m).\n  BST Dijkstra: self-balancing BST. Runtime: O(m*log n); space: O(n).\n  Theoretical Dijkstra: Fibonacci heap. Runtime: O(m + n*log n); space: O(n).\n  Floyd algorithm for k= 1,2, . . . , n do for i= 1,2, . . . , n do for j= 1,2, . . . , n do d[i, j]←min{d[i, j],d[i, k] +d[k, j]} end for end for end for The order of the loops is important.\nIncorrect implementations of the Floyd–Warshall algorithm give correct solutions after three repeats\n","permalink":"https://serge-m.github.io/posts/graph-algorithms/","summary":"Shortest path problem Dijkstra\u0026rsquo;s algorithm  fixes a single node as the \u0026ldquo;source\u0026rdquo; node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.\n Wiki Dijkstra\u0026rsquo;s algorithm\nComplexity analysis Wiki\nOn stack overflow: Why does Dijkstra\u0026rsquo;s algorithm use decrease-key?\nActually Implementing Dijkstra\u0026rsquo;s Algorithm - complexity analysis considering three components of the main loop: time per insert, extract_min, and change_priority operation:\n O(nT_ins + nT_min + m*T_change)","title":"Graph algorithms"},{"content":"Data strucures Sources:\n https://wiki.python.org/moin/TimeComplexity https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt  n is the number of elements in the container.\nk is either the value of a parameter, or the number of elements in the parameter.\nlist It is actually an array (implemented as an array).\n   Operation Average Case Amortized Worst Case Note     Copy O(n) O(n)    Append O(1) O(1) If the array grows beyond the allocated space it must be copied. In the worst case O(n)   Pop last O(1) O(1)    Pop intermediate O(n) O(n) Popping the intermediate element requires shifting all elements after k by one slot to the left using memmove. n - k elements have to be moved, so the operation is O(n - k).   Insert O(n) O(n)    Get Item O(1) O(1)    Set Item O(1) O(1)    Delete Item O(n) O(n)    Iteration O(n) O(n)    Get Slice O(k) O(k)    Del Slice O(n) O(n)    Set Slice O(k+n) O(k+n)    Extend O(k) O(k) If the array grows beyond the allocated space it must be copied. In the worst case O(n)   Sort O(n log n) O(n log n) sorting is implemented as timsort. Sorting is stable, requires O(n) additional memory.   Multiply O(nk) O(nk)    x in s O(n)     min(s), max(s) O(n)     Get Length O(1) O(1)     collections.deque A deque (double-ended queue) is represented internally as a doubly linked list. (Well, a list of arrays rather than objects, for greater efficiency.) Both ends are accessible, but even looking at the middle is slow, and adding to or removing from the middle is slower still.\n   Operation Average Case Amortized Worst Case     Copy O(n) O(n)   append O(1) O(1)   appendleft O(1) O(1)   pop O(1) O(1)   popleft O(1) O(1)   extend O(k) O(k)   extendleft O(k) O(k)   rotate O(k) O(k)   remove O(n) O(n)    set See dict \u0026ndash; the implementation is intentionally very similar.\n   Operation Average case Worst Case notes     copy O(n) O(n) s.copy()   iteration O(n) O(n) for x in my_set: ... source   x in s O(1) O(n)    Union s | t O(len(s)+len(t))     Intersection s\u0026amp;t O(min(len(s), len(t)) O(len(s) * len(t)) replace \u0026ldquo;min\u0026rdquo; with \u0026ldquo;max\u0026rdquo; if t is not a set   Multiple intersection s1\u0026amp;s2\u0026amp;..\u0026amp;sn (n-1)*O(l) where l is max(len(s1),..,len(sn))     Difference s-t O(len(s))     s.difference_update(t) O(len(t))     Symmetric Difference s^t O(len(s)) O(len(s) * len(t))    s.symmetric_difference_update(t) O(len(t)) O(len(t) * len(s))      As seen in the source code the complexities for set difference s-t or s.difference(t) (set_difference()) and in-place set difference s.difference_update(t) (set_difference_update_internal()) are different! The first one is O(len(s)) (for every element in s add it to the new set, if not in t). The second one is O(len(t)) (for every element in t remove it from s). So care must be taken as to which is preferred, depending on which one is the longest set and whether a new set is needed. To perform set operations like s-t, both s and t need to be sets. However you can do the method equivalents even if t is any iterable, for example s.difference(l), where l is a list.\n dict Implementation notes  Python dict uses open addressing to resolve hash collisions ( see dictobject.c#l665) Each slot in the table can store one and only one entry. Each logical slot contains information about hash, key and value When a new dict is initialized it starts with 8 slots. (see dictobject.c#l104) Adding an entry (key, value):  We start with some slot, i, that is based on the hash of the key. If that slot is empty, the entry is added to the slot. If the slot is occupied, CPython compares the hash AND the key of the entry in the slot against the hash and key of the current entry to be inserted ( implementation of lookup at dictobject.c#l687, ) respectively. If both match the entry already exists. If either hash or the key don\u0026rsquo;t match, it starts probing. Probing: The next slot is picked in a pseudo random order. The entry is added to the first empty slot in that sequence. ( implementation of lookup at dictobject.c#l687, ) for the algorithm for probing). Probing always terminates because the table is never full (see a note about resizing).   Similar process happens for the lookups. dict is resized if it is two-thirds full. This avoids slowing down lookups. (see USABLE_FRACTION) Sometimes (?) dictionaries share a table of the keys. For example  All object dictionaries of a single class can share a single key-table saving about 60% memory for such cases. (see dictnotes.txt))\n   Behavior  In the older versions of Python dictionaries did not preserve the order in which items were added to them. dict objects preserve order in the CPython 3.5 and 3.6. Order-preserving property is a standard language feature in Python 3.7+. Some libraries assume that dict order doesn\u0026rsquo;t matter. That can lead to some surprises. For example IPython and pandas. (may require verification for the newer versions). see Python Dictionaries Are Now Ordered. Keep Using OrderedDict     Operation Average Case Amortized Worst Case Note     k in d O(1) O(n)    Copy O(n) O(n)    Get Item O(1) O(n)    Set Item O(1) O(n) requires memory reallocation if capacity limit is reached.   Delete Item O(1) O(n)    Iteration O(n) O(n)     ","permalink":"https://serge-m.github.io/posts/data-structures-and-algorithms-in-python/","summary":"Data strucures Sources:\n https://wiki.python.org/moin/TimeComplexity https://www.ics.uci.edu/~pattis/ICS-33/lectures/complexitypython.txt  n is the number of elements in the container.\nk is either the value of a parameter, or the number of elements in the parameter.\nlist It is actually an array (implemented as an array).\n   Operation Average Case Amortized Worst Case Note     Copy O(n) O(n)    Append O(1) O(1) If the array grows beyond the allocated space it must be copied.","title":"Data Structures And Algorithms in Python"},{"content":"In school we round numbers like 0.5, 1123.5 towards the bigger number. It\u0026rsquo;s a \u0026ldquo;round half up\u0026rdquo; method.\nThat introduces an undesired bias some cases. For example if we have a large data set, and we aggregate some column containing a lot of .5 fractions. In order to adjust for it in many cases a rounding of 0.5 towards nearest even number is applied. It\u0026rsquo;s \u0026ldquo;Rounding half to even\u0026rdquo; or \u0026ldquo;banker\u0026rsquo;s rounding\u0026rdquo;.\nThis method is used in IEEE Standard for Floating-Point Arithmetic (IEEE 754). Python, numpy and pytorch use it as well.\nTruncation like int(0.5) and int(1.5) just keeps the integer part.\nExample \u0026gt;\u0026gt;\u0026gt; import torch \u0026gt;\u0026gt;\u0026gt; import math \u0026gt;\u0026gt;\u0026gt; import numpy as np  Defining an array with a lot of .5-s:\n\u0026gt;\u0026gt;\u0026gt; a = [x / 2. for x in range(10)] \u0026gt;\u0026gt;\u0026gt; a [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5]  Truncation just keeps the integer part:\n\u0026gt;\u0026gt;\u0026gt; list(map(int, a)) [0, 0, 1, 1, 2, 2, 3, 3, 4, 4]  Rounding introduces more even numbers:\n\u0026gt;\u0026gt;\u0026gt; list(map(round, a)) [0, 0, 1, 2, 2, 2, 3, 4, 4, 4]  Truncation in pytorch:\n\u0026gt;\u0026gt;\u0026gt; torch.tensor(a, dtype=torch.int) tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4], dtype=torch.int32) \u0026gt;\u0026gt;\u0026gt; torch.tensor(a, dtype=torch.float64).type(torch.int64) tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])  Rounding in pytorch:\n\u0026gt;\u0026gt;\u0026gt; torch.tensor(a, dtype=torch.float64).round() tensor([0., 0., 1., 2., 2., 2., 3., 4., 4., 4.], dtype=torch.float64)  Truncation and rounding in numpy works the same way:\n\u0026gt;\u0026gt;\u0026gt; np.array(a, dtype=np.int) array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4]) \u0026gt;\u0026gt;\u0026gt; np.round(a) array([0., 0., 1., 2., 2., 2., 3., 4., 4., 4.])  ","permalink":"https://serge-m.github.io/posts/rounding-in-python/","summary":"In school we round numbers like 0.5, 1123.5 towards the bigger number. It\u0026rsquo;s a \u0026ldquo;round half up\u0026rdquo; method.\nThat introduces an undesired bias some cases. For example if we have a large data set, and we aggregate some column containing a lot of .5 fractions. In order to adjust for it in many cases a rounding of 0.5 towards nearest even number is applied. It\u0026rsquo;s \u0026ldquo;Rounding half to even\u0026rdquo; or \u0026ldquo;banker\u0026rsquo;s rounding\u0026rdquo;.","title":"Rounding in Python"},{"content":"With the help of python and SymPy module one can do pretty neat computations. For example when I took a course about Robotic Preception on Coursera I had to find a cross product of two vectors v1 x v2 represented in a generic form:\nv1 = (a, b, c) v2 = (d, e, 0)  Normally I would write it down on a piece of paper and do the computations myself. Luckily python can help with that. Unfortunately it takes a bit of work to explain to SymPy what you want. But it is worth the trouble.\nFirst we install Sympy:\npip install sympy  Now we can switch to python/ipython/jupyter. Import the module\nfrom sympy import *  For vector representation we have to define a coordinate system:\nfrom sympy.vector import CoordSys3D C = CoordSys3D('C')  also we need a could of generic symbols:\na, b, c, d, e = symbols('a b c d e ')  Now we can define our vectors in that coordinate system using the symbols:\nv1 = a*C.i + b*C.j + c*C.k v2 = d*C.i + e*C.j + 0*C.k  And finally we can compute the cross product:\n\u0026gt;\u0026gt;\u0026gt; v1.cross(v2) (-c*e)*C.i + c*d*C.j + (a*e - b*d)*C.k  So the answer is (-ce, cd, ae-bd).\nAlternative operator for the cross product:\n\u0026gt;\u0026gt;\u0026gt; v1 ^ v2 (-c*e)*C.i + c*d*C.j + (a*e - b*d)*C.k  More info about vector operations: SymPy documentation\n","permalink":"https://serge-m.github.io/posts/symbolic-math-and-python/","summary":"With the help of python and SymPy module one can do pretty neat computations. For example when I took a course about Robotic Preception on Coursera I had to find a cross product of two vectors v1 x v2 represented in a generic form:\nv1 = (a, b, c) v2 = (d, e, 0)  Normally I would write it down on a piece of paper and do the computations myself.","title":"Symbolic math and python"},{"content":"fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.\nHere I want to reproduce some of the techniques in order to understand what is the effect they bring.\nI don\u0026rsquo;t want to use fastai library here for two reasons. First, for better understanding of the processes it is better to implement them by yourself.\nSecond, I think the library has certain disadvantages and it is better to stick to pytorch as close as possible. Fastai introduces too many layers of indirection between the user and pytorch. It is very convenient for the beginners or for standard use cases. But as soon as you need something more advance you basically have to digg through ALL the layers of fastai to get what you want. I started building another library that incorporates techniques from fast ai, but doesn\u0026rsquo;t force the user to stick to the library. Hopefully. Thus I am going to use that library for my tests.\nTask I am going to train an image classifier based on Resnet18 from pytorch. The classifier has to distinguish between the dog breeds from an excellent Imagewoof dataset. The dataset is designed by Jeremy Howard again to for learning purposes. the dataset is small enough to to fast experimentation. But it also large enough and sophisticated enough to make results and insights applicable for real datasets like Imagenet.\nBaseline I am going to start from SGD with 40 epochs, LR=0.1 for the first 30 epochs and 0.01 for epochs 30-40.\nInstalling dependencies:\n1  pip install pytorch-nn-tools==0.3.3 torch_lr_finder==0.2.1   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  import os import torch import torch.nn.parallel import torch.utils.data import torchvision import torchvision.datasets as datasets import torchvision.models as models import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt import torch.nn.functional as F from collections import defaultdict from typing import Dict, List, Callable, Union from pathlib import Path import json import time import datetime from fastprogress.fastprogress import master_bar, progress_bar from time import sleep from pytorch_nn_tools.visual import UnNormalize_, imagenet_stats from pytorch_nn_tools.train.metrics.processor import mod_name_train, mod_name_val, Marker from pytorch_nn_tools.train.metrics.processor import MetricAggregator, MetricLogger, MetricType from pytorch_nn_tools.train.progress import ProgressTracker from pytorch_nn_tools.convert import map_dict from pytorch_nn_tools.train.metrics.history_condition import HistoryCondition from pytorch_nn_tools.train.checkpoint import CheckpointSaver   Functions for generating datasets\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  def _train_dataset(path): normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], ) train_dir = os.path.join(path, \u0026#39;train\u0026#39;) train_dataset = datasets.ImageFolder( train_dir, transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ])) return train_dataset def _val_dataset(path): normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], ) val_dir = os.path.join(path, \u0026#39;val\u0026#39;) dataset = datasets.ImageFolder(val_dir, transforms.Compose( [transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ])) return dataset   Settings and data loaders:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  batch_size_train = 128 batch_size_val = 16 device = \u0026#39;cuda\u0026#39; data_path = \u0026#34;data/imagewoof2-320/\u0026#34; train_dataloader = torch.utils.data.DataLoader( dataset=_train_dataset(data_path), batch_size=batch_size_train, shuffle=True, num_workers=num_workers, ) val_dataloader = torch.utils.data.DataLoader( dataset=_val_dataset(data_path), batch_size=batch_size_val, shuffle=False, num_workers=num_workers, )   Accuracy function from pytorch-lightning:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def accuracy(output, target, topk=(1,)): \u0026#34;\u0026#34;\u0026#34;Computes the accuracy over the k top predictions for the specified values of k\u0026#34;\u0026#34;\u0026#34; with torch.no_grad(): maxk = max(topk) batch_size = target.size(0) _, pred = output.topk(maxk, 1, True, True) pred = pred.t() correct = pred.eq(target.view(1, -1).expand_as(pred)) res = [] for k in topk: correct_k = correct[:k].view(-1).float().sum(0, keepdim=True) res.append(correct_k.mul_(100.0 / batch_size)) return res   Some helper functions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  def to_device(batch, device): return batch.to(device) from fastprogress.fastprogress import master_bar, progress_bar class PBars: def __init__(self): self._main = None self._second = None def main(self, it, **kwargs): self._main = master_bar(it, **kwargs) return self._main def secondary(self, it, **kwargs): if self._main is None: raise RuntimeError(\u0026#34;Cannot instantiate secondary progress bar. The main progress bar is not set.\u0026#34;) self._second = progress_bar(it, parent=self._main, **kwargs) return self._second def main_comment(self, comment): self._main.main_bar.comment = comment def now_as_str(): now = datetime.datetime.now() return now.strftime(\u0026#34;%Y%m%d_%H%M%s_%f\u0026#34;) class DummyLogger: def debug(self, *args): print(*args)   TrainerIO is responsible for logging and checkpointing. That is the class that does IO on behalf of a trainer. It can 1) instantiate metric logger. 2) load last checkpoint and 3) save checkpoints\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  class TrainerIO: def __init__(self, log_dir: Union[Path, str], experiment_name: str, checkpoint_condition: Callable[[MetricType], bool]): self.log_dir = Path(log_dir) self.experiment_name = experiment_name self.path_experiment = self.log_dir.joinpath(experiment_name) self.path_checkpoints = self.path_experiment.joinpath(\u0026#34;checkpoints\u0026#34;) self.checkpoint_saver = CheckpointSaver(self.path_checkpoints, logger=DummyLogger()) self.checkpoint_condition = checkpoint_condition def create_metric_logger(self): path_logs = self.path_experiment.joinpath(f\u0026#34;{self.experiment_name}_{now_as_str()}\u0026#34;) metric_logger = MetricLogger(path_logs) return metric_logger def load_last(self, start_epoch: int, end_epoch: int, model, optimizer, scheduler) -\u0026gt; int: last = self.checkpoint_saver.find_last(start_epoch, end_epoch) if last is not None: print(f\u0026#34;found pretrained results for epoch {last}. Loading...\u0026#34;) self.checkpoint_saver.load(model, optimizer, scheduler, last) return last + 1 else: return start_epoch def save_checkpoint(self, metrics: MetricType, model, optimizer, scheduler, epoch): if self.checkpoint_condition(metrics): self.checkpoint_saver.save(model, optimizer, scheduler, epoch)   Now goes the trainer. In fastai and pytorch-lightning frameworks the trainers are implemented inside the library. The user has to use callbacks of inheritance in order to patch the standard trainer loop. That means the trainer has to be overly generic. For each case the user has to know how which callback to use and how the result of that callback influences the training logic. That look like a leaking abstraction to me. In that case the library doesn\u0026rsquo;t really abstract anything. The user has to know all the internals in order to do even moderately difficult things with it.\nI have a hypothesis that it is possible to inverse that dependency. I want the user to write a training loop on it\u0026rsquo;s own. I want my library to provide helper functions for that in order to make it super easy.\nThe biggest part of the training loop is IO: checkpointing, logging, data loading. There is very little amount of logic. I hypothesize that it may has no sense to extract it to a library. Let the user write it, but make it simple.\nIt\u0026rsquo;s not a finalized version btw. Some of the parts may be included into pytorch-nn-tools in the future.\nThe trainer has main function fit(...) that calls train_epoch(...) and validate_epoch(...).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  class Trainer: def __init__(self, device, trainer_io: TrainerIO, continue_training: bool = False): self.device = device self.continue_training = continue_training self.trainer_io = trainer_io self.pbars = PBars() def fit(self, model, optimizer, scheduler, start_epoch, end_epoch): metric_logger = self.trainer_io.create_metric_logger() model = model.to(self.device) if self.continue_training: start_epoch = self.trainer_io.load_last(start_epoch, end_epoch, model, optimizer, scheduler) progr_train = ProgressTracker() for epoch in self.pbars.main(range(start_epoch, end_epoch)): metric_aggregator = MetricAggregator() self.train_epoch( train_dataloader, progr_train, model, optimizer, scheduler, metric_proc=mod_name_train+metric_aggregator+metric_logger, pbars=self.pbars, report_step=10, tb_writer=metric_logger.writer ) self.validate_epoch( val_dataloader, model, metric_proc=mod_name_val+metric_aggregator+metric_logger, pbars=self.pbars, ) aggregated = map_dict(metric_aggregator.aggregate(), key_fn=lambda key: f\u0026#34;avg.{key}\u0026#34;) metric_logger({ **aggregated, **{f\u0026#34;lr_{i}\u0026#34;: lr for i, lr in enumerate(scheduler.get_last_lr())}, Marker.EPOCH: epoch, }) self.pbars.main_comment(f\u0026#34;{aggregated}\u0026#34;) self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch) scheduler.step() # for epoch-based scheduler metric_logger.close() def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, pbars, report_step=1, tb_writer=None): model.train() for batch_idx, (images, target) in enumerate(progr.track(pbars.secondary(data_loader))): optimizer.zero_grad() images = to_device(images, self.device) target = to_device(target, self.device) output = model(images) loss = F.cross_entropy(output, target) loss.backward() optimizer.step() if progr.cnt_total_iter % report_step == 0: with torch.no_grad(): acc1, acc5 = accuracy(output, target, topk=(1, 5)) metric_proc({ \u0026#39;loss\u0026#39;: loss, \u0026#39;acc1\u0026#39;: acc1, \u0026#39;acc5\u0026#39;: acc5, Marker.ITERATION: progr.cnt_total_iter, **{f\u0026#34;lr_{i}\u0026#34;: lr for i, lr in enumerate(scheduler.get_last_lr())}, }) # scheduler.step() # for batch based scheduler def validate_epoch(self, data_loader, model, metric_proc, pbars): model.eval() with torch.no_grad(): for images, target in pbars.secondary(data_loader): images = to_device(images, self.device) target = to_device(target, self.device) output = model(images) loss = F.cross_entropy(output, target) acc1, acc5 = accuracy(output, target, topk=(1, 5)) metric_proc(dict(loss=loss, acc1=acc1, acc5=acc5))   Finally the main part\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  recommended_lr = 0.1 num_epochs = 40 model = models.resnet18(pretrained=False) optimizer = torch.optim.SGD([ { \u0026#39;name\u0026#39;: \u0026#39;main_model\u0026#39;, \u0026#39;params\u0026#39;: model.parameters(), \u0026#39;lr\u0026#39;: 0.1, \u0026#39;momentum\u0026#39;: 0.9, \u0026#39;weight_decay\u0026#39;: 1e-4, } ]) scheduler = torch.optim.lr_scheduler.LambdaLR( optimizer, lambda epoch: 0.1 ** (epoch // 30) ) trainer_io = TrainerIO( log_dir=\u0026#34;./logs/\u0026#34;, experiment_name=f\u0026#34;experiment1_onecycle_lr{recommended_lr}_adamw\u0026#34;, checkpoint_condition=HistoryCondition( \u0026#39;avg.val.acc1\u0026#39;, lambda hist: len(hist) == 1 or hist[-1] \u0026gt; max(hist[:-1]) ) ) trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False) trainer.fit( model, optimizer, scheduler, start_epoch=0, end_epoch=num_epochs )   In order to run the trainer the user has to define TrainerIO, model, scheduler and optimizer. All those parts are interconnected and essential for the process.\nWe achieve accuracy for top 1 about 69% and for top 5 about 96 %. That is in line with the values that other people get for that dataset: Imagewoof Leaderboard\ntop1 accuracy\ntop5 accuracy\nLR Finder One of the greatest things I found in fastai is learning rate finder. It is a technique that helps to set up the initial (base) learning rate for the models.\nThe idea is to do the iteration of the training for gradually increasing learning rate. The learning rate value where we see the fastest descent of the loss is a good chioce.\nI am not going to write that LR finder myself and I use the existing module pytorch-lr-finder, github. That module works with standard pytorch concepts. If I would like to use the LRFinder from fast ai or pytorch lightning, I would be forced to use their trainers, optimizers etc.\nLet\u0026rsquo;s create a new model and optimizer. We start with a very low learning rate: 1e-8. The lr_finder will do 50 iterations up until lr=10.:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  model = models.resnet18(pretrained=False) optimizer = torch.optim.AdamW([ { \u0026#39;name\u0026#39;: \u0026#39;main_model\u0026#39;, \u0026#39;params\u0026#39;: model.parameters(), \u0026#39;lr\u0026#39;: 1e-8, \u0026#39;weight_decay\u0026#39;: 1e-4, } ]) from torch_lr_finder import LRFinder criterion = torch.nn.CrossEntropyLoss() lr_finder = LRFinder(model, optimizer, criterion, device=device) lr_finder.range_test(train_dataloader, val_loader=None, end_lr=10, num_iter=50, step_mode=\u0026#34;exp\u0026#34;) _, recommended_lr = lr_finder.plot(log_lr=False) lr_finder.reset()   My recommended LR is about 0.02. I can launch with the same optimizer, lr schedule, but with the new base LR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  recommended_lr = 0.02 model = models.resnet18(pretrained=False) optimizer = torch.optim.SGD([ { \u0026#39;name\u0026#39;: \u0026#39;main_model\u0026#39;, \u0026#39;params\u0026#39;: model.parameters(), \u0026#39;lr\u0026#39;: recommended_lr, \u0026#39;momentum\u0026#39;: 0.9, \u0026#39;weight_decay\u0026#39;: 1e-4, } ]) scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=recommended_lr, epochs=num_epochs, steps_per_epoch=len(train_dataloader) )   Here also we have to patch our trainer. Scheduler step now happens for each training batch, not per epoch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  class Trainer: # ..... def fit(self, model, optimizer, scheduler, start_epoch, end_epoch): # .... for epoch in self.pbars.main(range(start_epoch, end_epoch)): # ..... self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch) # not needed any more # scheduler.step()  metric_logger.close() def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, pbars, report_step=1, tb_writer=None): model.train() for batch_idx, (images, target) in enumerate(progr.track(pbars.secondary(data_loader))): # ... # now we do scheduler step for every batch scheduler.step() def validate_epoch(self, data_loader, model, metric_proc, pbars): # stays the same # ...   Let\u0026rsquo;s run:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  trainer_io = TrainerIO( log_dir=\u0026#34;./logs/\u0026#34;, experiment_name=f\u0026#34;experiment_base_lr{recommended_lr}\u0026#34;, checkpoint_condition=HistoryCondition( \u0026#39;avg.val.acc1\u0026#39;, lambda hist: len(hist) == 1 or hist[-1] \u0026gt; max(hist[:-1]) ) ) trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False) trainer.fit( model, optimizer, scheduler, start_epoch=0, end_epoch=num_epochs )   train loss. lr=0.1 vs lr=0.02 found by LRFinder\nTop 1 Accuracy improved significantly from 0.69 to 0.78!\ntop 1 accuracy on validation. lr=0.1 vs lr=0.02 found by LRFinder\nOne Cycle learning rate Another technique is to have a special LR schedule. We start small, ramp it up quickly to the maximum and then gradually decrease.\nHere is how the LR graphs look like:\nlearning rate schedules. Gray line is a \u0026ldquo;one cycle\u0026rdquo;\nWe don\u0026rsquo;t get accuracy increase here. But we don\u0026rsquo;t need to hand craft the learning rate schedule any more. That is positive.\nHopefully one cycle brings some numerical stability.\nAccuracy top 1 for one cycle. Gray line is a \u0026ldquo;one cycle\u0026rdquo;\nAdamW Instead of SGD let\u0026rsquo;s use Adam optimizer. More specifically AdamW version of it. It was also advertised in fastai as a good practice.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  model = models.resnet18(pretrained=False) optimizer = torch.optim.AdamW([ { \u0026#39;name\u0026#39;: \u0026#39;main_model\u0026#39;, \u0026#39;params\u0026#39;: model.parameters(), \u0026#39;lr\u0026#39;: recommended_lr, \u0026#39;weight_decay\u0026#39;: 1e-4, } ]) scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=recommended_lr, epochs=num_epochs, steps_per_epoch=len(train_dataloader) ) trainer_io = TrainerIO( log_dir=\u0026#34;./logs/\u0026#34;, experiment_name=f\u0026#34;experiment1_onecycle_lr{recommended_lr}_adamw\u0026#34;, checkpoint_condition=HistoryCondition( \u0026#39;avg.val.acc1\u0026#39;, lambda hist: len(hist) == 1 or hist[-1] \u0026gt; max(hist[:-1]) ) ) trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False) trainer.fit( model, optimizer, scheduler, start_epoch=0, end_epoch=num_epochs )   That helps a lot. We go from 79.5% to 82.6% of accuracy:\nAccuracy top 1 for AdamW. Dark red line is a AdamW\nTrain loss for AdamW. Dark red line is a AdamW\nConclusion We used a fraction of tricks from fastai to improve the performance of our training. They all seem to be useful. Only OneCycle scheduler didn\u0026rsquo;t bring an evident benefits in the accuracy. However it brings some simplicity for developer.\nThe numbers are still pretty far from the values on the leaderboard, but we didn\u0026rsquo;t use all the tricks of course.\nAlso here we verified the viability of pytorch-nn-tools library.\n","permalink":"https://serge-m.github.io/posts/effect-of-techniques-from-fastai/","summary":"fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.\nHere I want to reproduce some of the techniques in order to understand what is the effect they bring.","title":"Effect of techniques from Fast.ai"},{"content":"ROS nodes Point Cloud IO https://github.com/ANYbotics/point_cloud_io\n two nodes for reading and writing PointCloud2 from/to ply, pcd formats  point_cloud_assembler from laser_assembler http://wiki.ros.org/laser_assembler\nThis node assembles a stream of sensor_msgs/PointCloud2 messages into larger point clouds. The aggregated point cloud can be accessed via a call to assemble_scans service.\nhttps://github.com/ros-perception/laser_assembler\nTutorial\nOctomap http://octomap.github.io/\nSeems like a standard solution to convert point clouds to a map in several formats\npointcloud_to_laserscan http://wiki.ros.org/pointcloud_to_laserscan\npcl_ros http://wiki.ros.org/pcl_ros\n This package provides interfaces and tools for bridging a running ROS system to the Point Cloud Library. These include ROS nodelets, nodes, and C++ interfaces.\n It contains for example pointcloud_to_pcd, pcd_to_pointcloud, bag_to_pcd.\n","permalink":"https://serge-m.github.io/posts/point-cloud-processing/","summary":"ROS nodes Point Cloud IO https://github.com/ANYbotics/point_cloud_io\n two nodes for reading and writing PointCloud2 from/to ply, pcd formats  point_cloud_assembler from laser_assembler http://wiki.ros.org/laser_assembler\nThis node assembles a stream of sensor_msgs/PointCloud2 messages into larger point clouds. The aggregated point cloud can be accessed via a call to assemble_scans service.\nhttps://github.com/ros-perception/laser_assembler\nTutorial\nOctomap http://octomap.github.io/\nSeems like a standard solution to convert point clouds to a map in several formats\npointcloud_to_laserscan http://wiki.ros.org/pointcloud_to_laserscan\npcl_ros http://wiki.","title":"Point cloud processing"},{"content":"Libraries   Standard multiprocessing\n  Pebble - pretty close to the standard one, but with a bit nicer interface\n  Dask - well maintained and (almost) drop-in replacement of numpy and pandas:\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Arrays implement the Numpy API import dask.array as da x = da.random.random(size=(10000, 10000), chunks=(1000, 1000)) x + x.T - x.mean(axis=0) # Dataframes implement the Pandas API import dask.dataframe as dd df = dd.read_csv(\u0026#39;s3://.../2018-*-*.csv\u0026#39;) df.groupby(df.account_id).balance.sum() # Dask-ML implements the Scikit-Learn API from dask_ml.linear_model \\ import LogisticRegression lr = LogisticRegression() lr.fit(train, test)     mptools - seems like an abandoned project. The autor had a nice article though: Things I Wish They Told Me About Multiprocessing in Python\n  Ray\nRelated article: 10x Faster Parallel Python Without Python Multiprocessing\n   Progress bar for parallel tasks Often one need to run some slow function in parallel in order to speed up the computation. In user facing apps it\u0026rsquo;s important to visualize the progress. One can use multiprocessing.Pool and tqdm for it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import multiprocessing import numpy as np import tqdm def slow_operation(a): \u0026#34;\u0026#34;\u0026#34; Slow operation, return value is not needed in main. For example the function opens a file, processes it and dumps the results to a new location. \u0026#34;\u0026#34;\u0026#34; z = np.random.random([1000, 1000]) for i in range(50): z = z * (z - 0.5) return z   One cannot apply tqdm to the Pool.map because the whole processing happens before the tqdm can iterate the result.\nHowever it\u0026rsquo;s possible to use Pool.imap or Pool.imap_unordered if the order is not important.\n1 2 3 4 5  def parallel_with_imap_unordered(): with multiprocessing.Pool(6) as pool: for _ in tqdm.tqdm(pool.imap_unordered(slow_operation, range(100)), total=100): pass   It\u0026rsquo;s nice that we don\u0026rsquo;t need to do pool.join() here because imap* waits for all the tasks to complete.\nDon\u0026rsquo;t forget to set chunk size:\n For very long iterables using a large value for chunksize can make the job complete much faster than using the default value of 1.\n Alternatively we can use callbacks to update a global progress bar in a main process:\n1 2 3 4 5 6 7 8 9 10 11  def parallel_with_callback(): pbar = tqdm.tqdm(total=100) def update(*a): pbar.update() pool = multiprocessing.Pool(6) for i in range(pbar.total): pool.apply_async(slow_operation, args=(i,), callback=update) pool.close() pool.join()   That method requires pool.join to wait for all the processes to finish.\nUsing Pipes for parallel stateful processes Let\u0026rsquo;s consider the following task. We have to implement a controller. The controller defines a processing graph with 4 interconnected stages:\n  detector\n  size_estimator (depends on detector)\n  classifier (depends on detector)\n  aggregator (depends on size_estimator and classifier)\n  This could be a model for some computer vision pipeline and controller processes frames coming from a camera.\nSequential version of the controller could look like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Controller: def __init__(self): self.detector = Detector() self.size_estimator = Processor(fn=_compute_size) self.classifier = Processor(fn=_obj_to_class) self.aggregator = Aggregator() self.stats = [] def __call__(self, frame): objects = self.detector(frame) sizes = self.size_estimator(objects) classes = self.classifier(objects) stat = self.aggregator(sizes, classes) self.stats.append(stat) def finish(self): pass   Usage of the controller:\n1 2 3 4 5 6 7 8 9 10  def execute_test(controller): num_frames = 10 t = time.time() for i in range(num_frames): frame = np.empty((100, 100), dtype=\u0026#39;uint8\u0026#39;) controller(frame) controller.finish() t = time.time() - t print(f\u0026#34;FPS: {num_frames / t}\u0026#34;) return t   As I have mentioned above two stages - size estimation and classification - can be executed in parallel. A standard solution for that could be multiprocessing.Pool with a function like map or imap. That works if our processing stages are stateless and the initialization is cheap It is not always the case.\nIf classifier requires a costly initialization, e.g. loading a big neural network into memory, it would be nice to have it initialized only once. We can do it in a separate process. If we were using a programming language other than Python, we could use threads for it. But in python we have GIL.\nController has to send data to another process and receive the results. Communication between parallel processes is a dangerous thing. Let\u0026rsquo;s try to use multiprocessing.Pipe for that.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  class ParallelPipeController: def __init__(self, size_estimator_factory=lambda: Processor(fn=_compute_size)): self.pipe_size_estimator, pipe_size_worker = Pipe() self.detector = Detector() self.size_estimator = Process( target=pipe_worker, args=(pipe_size_worker, size_estimator_factory), daemon=True ) self.classifier = Processor(fn=_obj_to_class) self.aggregator = Aggregator() self.stats = [] self.size_estimator.start() def __call__(self, frame): objects = self.detector(frame) self.pipe_size_estimator.send(objects) classes = self.classifier(objects) try: sizes = self.pipe_size_estimator.recv() except EOFError as e: raise RuntimeError(\u0026#34;Unable to get data from process. Probably exception occurred\u0026#34;) from e stat = self.aggregator(sizes, classes) self.stats.append(stat) def finish(self): self.pipe_size_estimator.send(None) self.size_estimator.join()   pipe_worker would look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def pipe_worker(pipe, factory: Callable): try: print(\u0026#34;worker started\u0026#34;) processor = factory() while True: msg = pipe.recv() if msg is None: break result = processor(msg) pipe.send(result) print(\u0026#34;worker exited correctly\u0026#34;) except Exception: print(\u0026#34;An exception occurred. We notify the main process by closing our end of the pipe.\u0026#34; \u0026#34;It would be nicer to send some info to the main process.\u0026#34;) pipe.close()   Comparing those two controllers:\n1 2 3 4  print(\u0026#34;sequential\u0026#34;) execute_test(Controller()) print(\u0026#34;parallel\u0026#34;) execute_test(ParallelPipeController())   Sample output:\nsequential FPS: 1.7753397948365568 parallel worker started worker exited correctly FPS: 2.820896038347416  Full code is here.\nThere are some alternative workarounds to deal with initialization in standard multiprocessing. They usually require some global variables. See for example:\n  Stack overflow how to use initializer to set up my multiprocess pool?\n  Multiprocessing.Pool - Pass Data to Workers w/o Globals: A Proposal\n  Processing KeyboardInterrupt in workers Apparently there are some issue with KeyboardInterrupt and multiprocessing:\n when workers are idle, Python’s KeyboardInterrupt is not handled correctly by the multiprocessing module, which results in not only a lot of stacktraces spewed to the console, but also means the parent process will hang indefinitely.\n Python: Using KeyboardInterrupt with a Multiprocessing Pool, 2011.\n","permalink":"https://serge-m.github.io/posts/python-multiprocessing/","summary":"Libraries   Standard multiprocessing\n  Pebble - pretty close to the standard one, but with a bit nicer interface\n  Dask - well maintained and (almost) drop-in replacement of numpy and pandas:\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Arrays implement the Numpy API import dask.array as da x = da.random.random(size=(10000, 10000), chunks=(1000, 1000)) x + x.","title":"Python - Multiprocessing"},{"content":"3D Packing for Self-Supervised Monocular Depth Estimation by Vitor Guizilini, pdf at arxiv, 2020\nLearning\nDepth estimator fD : I → D Ego motion estimator: fx : (It, IS) → xt → S  Depth Estimator They predict an inverse depth and use a packnet architecture.\nInverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have more information\nProbably they assume that the scene is rigid and there is no moving objects. It is likely to give some errors for the moving objects. How do they deal with the moving objects?\n Ego motion estimator They use a rather simple CNN from SfMLearner.\n Loss function Loss function consists of three parts. 1. appearance loss 2. depth smoothness loss 3. velocity scaling loss\n  SfMLearner paper: Unsupervised Learning of Depth and Ego-Motion from Video by Berkley and google\npdf\nOne of the previous works, that became a foundation for 3d-pack.\nOfficial website.\nGithub: https://github.com/tinghuiz/SfMLearner.\nImplementation in pytorch: SfmLearner-Pytorch\n  Depth from Videos in the Wild Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras by Ariel Gordon et al, 2019.\nhttps://research.google/pubs/pub48440/\npdf\ngithub\nHere they learn not only depth and R/t but also intrinsics of the camera\n struct2depth Depth Prediction Without the Sensors: Leveraging Structure for UnsupervisedLearning from Monocular Videos\nPaper by google\nsite\ncode\nbased on vid2depth\n vid2depth another paper by google.\ngithub\nbased on sfm learner\n monodepth2 - Digging Into Self-Supervised Monocular Depth Estimation arxiv\ngithub\nsome method that 3D Packing use as a competitor.\n RealMonoDepth: Self-Supervised Monocular Depth Estimation for General Scenes self-supervised from stereo and mono.\nby deepai: https://deepai.org/publication/realmonodepth-self-supervised-monocular-depth-estimation-for-general-scenes.\nThey claim to be better than monodepth2 and mode generalized than depth_from_video_in_the_wild \u0026quot;Depth from Videos in the Wild\u0026quot;. However they require camera calibration and median depth to be estimated prior to processing with an external tool. (COLMAP). Not really \u0026quot;in the wild\u0026quot;.\nThey were able to train on multiple scenes with different depth ranges. The method still requires a static scene for the training. For example they use data from Mannequin Challenge to train their models. Then the network can be used on dynamic scenes with people.\nThe code is not available so far.\n ","permalink":"https://serge-m.github.io/posts/self-supervised-depth-and-ego-motion/","summary":"3D Packing for Self-Supervised Monocular Depth Estimation by Vitor Guizilini, pdf at arxiv, 2020\nLearning\nDepth estimator fD : I → D Ego motion estimator: fx : (It, IS) → xt → S  Depth Estimator They predict an inverse depth and use a packnet architecture.\nInverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have more information","title":"Self-supervised depth and ego motion estimation"},{"content":"In 2020 which architecture should I use for my image classification/tracking/segmentation/\u0026hellip; task?\nI was asked on an interview that and I didn\u0026rsquo;t have a prepared answer.\nI made a small research and want to write down some thoughts.\nMost of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015\nHere is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017. It is a bit dated already.\nEfficientNet A recent paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le, 2019 explores the scaling of model hyperpameters to achieve computational effectiveness and performance in standard image classification tasks.\nThe architecture is pretty simple. They focus on the scaling parameters of the network rather than finding good building blocks.\nPretrained weights can be found for Pytorch and Tensortflow:\n  Repo for pytorch: pytorch-image-models\n  another repo for pytorch: imgclsmob\n  The performance with transfer learning seems pretty good. I have seen some doubts about using it on mobile devices. The gists is that EfficientNet is optimized with respect to the number of parameters, not actual Addition/Multiplications or FLOPS on the device. Here is a notebook with some experiments and critique of the efficient net: on google colab, on github\nGreat discussion thread on fast.ai: https://forums.fast.ai/t/efficientnet/46978/79\nMNasNet MnasNet paper MnasNet: Platform-Aware Neural Architecture Search for Mobile, Tan et. al 2019\nGood - Pareto analysis, provide a range of networks for different speed/quality tradeoff. AutoML. They use MobilenetV2 as a base.\nThose guise optimized for real mobile hardware.(see picture) They claim to be 1.8 times faster than Mobilenet V2 with the same performance.\n  our slightly larger MnasNet-A3 modelachieves better accuracy than ResNet-50 [9], but with4.8×fewerparameters and10×fewermultiply-add cost\n  They also beat YOLOv2 for object detection on COCO in terms of quality.\nMobileNet V2 Paper: MobileNetV2: Inverted Residuals and Linear Bottlenecks, Sander rt al, CVPR 2018\nArxive version\nFocus on the mobile platforms.\nThey implemented several important improvements to reduce the complexity:\n  Depthwise Separable Convolutions - instead of using C_out filters of size W x H x C_in we apply C_in filters of size W x H x 1 and then C_out filters of size 1 x 1 x C_in. Some kid of factorization.\n The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a1×1convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the in-put channels.\n   Linear Bottlenecks - for dimensionality reduction\n  Inverted residuals. Well, nothing is really inverted. Just skip connections are binding low-resoltion bottleneck layers instead of upscaled layers of hi resolution.\n  RegNet Designing Network Design Spaces paper by Facebook AI Research (FAIR).\nMedium paper: RegNet or How to methodologically design effective networks.\nA class of models that is designed for fast training and inference. The implementation and the trained weights can be found at pycls repo: https://github.com/facebookresearch/pycls\nThe authors take a rather broad space of possible architectures an sample architectures from that space. They train and evaluate the sampled architectures to find what works best.\nThey claim the network 5 times faster than EfficientNet for some configurations. For the smaller networks the difference is much lower (~2 times). That fact is in sync with the experiments from EffResNetComparison.ipynb (see above).\nInteresting example from RegNet or How to methodologically design effective networks:\n MNASNets including MobileNets and EfficientNets extensively use Depthwise convolutions to achieve SoTA performances. These convolutions could be understood as group convolutions with group width of 1. The fact that AnyNetXb populations showed that g \u0026gt;1 is best, does not conflict with this fact. That such networks can and do perform excellently is not under question. The paper is empirically showing, with statistical backing to back the claim, that as a design space, g = 1 might be best avoided even though the MNAS search has found particular instances in which there are good performing models to build upon.\n One more vitation from Facebook AI RegNet Models Outperform EfficientNet Models, Run 5x Faster on GPUs\n While it is common to see modern mobile networks employ inverted bottlenecks, researchers noticed that using inverted bottlenecks degrades performance. The best models do not use either a bottleneck or an inverted bottleneck.\n ResNeSt ResNeSt: Split-Attention Networks paper\nresnest code\nA pretty recent paper. The authors propose to split the channels into groups, process them with a separate sets of convolutions, and then concat them.\nThey achieve a comparable (and better) performance with EfficientNet, while having less parameters and having better frame rate.\nConclusion I would start with ResNest given it\u0026rsquo;s great performance.\nIf I want to have a smaller network with less parameters I would go for Efficient net. That would probably make sense if I run without a GPU. The architecture of EfficientNet is proven to work well in several domains.\nRegnet is not trained for the same amount of time and reports lower accuracy. The comparison provided in the paper uses re-trained Efficient Net Weights. It\u0026rsquo;s unclear whether it will be comparable with on the full-blown training.\nThe backup plan is MNas and Mobilenet. If I need to run on a smaller device I would be more careful and considered MNasNet or Mobilenet V2/V3.\n","permalink":"https://serge-m.github.io/posts/which-backbone-to-choose/","summary":"In 2020 which architecture should I use for my image classification/tracking/segmentation/\u0026hellip; task?\nI was asked on an interview that and I didn\u0026rsquo;t have a prepared answer.\nI made a small research and want to write down some thoughts.\nMost of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015\nHere is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017.","title":"Which pretrained backbone to choose"},{"content":"command line arguments is a standard and one of the most common ways to pass parameters to a python script. There exist a list of python libraries that help with that task. Here I am going to list some of them.\nargparse The default choice for the python developer. The module is included in python standard library and comes together with any python distribution.\nExample of usage:\n#!/usr/bin/env bash import argparse def hello(count, name): \u0026quot;\u0026quot;\u0026quot;Simple program that greets NAME for a total of COUNT times.\u0026quot;\u0026quot;\u0026quot; for x in range(count): print('Hello %s!' % name) if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--count', help='Number of greetings.', type=int, default=1) parser.add_argument('name', help='The person to greet.') args = parser.parse_args() hello(args.count, args.name)  $ ./test.py john --count 2 Hello john! Hello john! $ ./test.py john Hello john! $ ./test.py --name john usage: test.py [-h] [--count COUNT] name test.py: error: unrecognized arguments: --name   click custom module that I now use by default. Provides a pretty concise interface that turns function parameters in to command line parameters. Another advantage is that it solves the problem with ambiguous and non-obvious rules of argument parsing in argparse.\nSome known drawbacks of click: issues with unicode:\n Click supports Python 3, but like all other command line utility libraries, it suffers from the Unicode text model in Python 3. All examples in the documentation were written so that they could run on both Python 2.x and Python 3.4 or higher. see python3-limitations for more information\nExample of usage:\n#!/usr/bin/env python3 import click \u0026#64;click.command() \u0026#64;click.option('--count', default=1, help='Number of greetings.') \u0026#64;click.option('--name', help='The person to greet.') def hello(count, name): \u0026quot;\u0026quot;\u0026quot;Simple program that greets NAME for a total of COUNT times.\u0026quot;\u0026quot;\u0026quot; for x in range(count): print('Hello %s!' % name) if __name__ == '__main__': hello()  Running :\n$ ./test.py --name john --count 3 Hello john! Hello john! Hello john! $ ./test.py --name john Hello john! $ ./test.py john Usage: test.py [OPTIONS] Try 'test.py --help' for help. Error: Missing option '--name'.  As you can see you cannot mix positional and named arguments. To define positional arguments you have to use argument\n#!/usr/bin/env python3 import click \u0026#64;click.command() \u0026#64;click.option('--count', default=1, help='Number of greetings.') \u0026#64;click.argument('name', required=True, ) def hello(count, name): \u0026quot;\u0026quot;\u0026quot;Simple program that greets NAME for a total of COUNT times.\u0026quot;\u0026quot;\u0026quot; for x in range(count): print('Hello %s!' % name) if __name__ == '__main__': hello()  $ ./test.py john --count Error: --count option requires an argument $ ./test.py john --count 3 Hello john! Hello john! Hello john! $ ./test.py --name john --count 3 Usage: test.py [OPTIONS] NAME Try 'test.py --help' for help. Error: no such option: --name  How to make a flag: \u0026#64;click.option('--shout', is_flag=True)    google's fire https://github.com/google/python-fire\nExamples from the official documentation :\nimport fire def add(x, y): return x + y def multiply(x, y): return x * y if __name__ == '__main__': fire.Fire()  $ python example.py add 10 20 30 $ python example.py multiply 10 20 200  another example:\nimport fire class Calculator(object): def add(self, x, y): return x + y def multiply(self, x, y): return x * y if __name__ == '__main__': fire.Fire(Calculator)  $ python example.py add 10 20 30 $ python example.py multiply 10 20 200  the library also turns function parameters into command line arguments. Types are deducted automatically. That can be bad sometimes. However one has to check the validity of the input anyway.\n The types of the arguments are determined by their values, rather than by the function signature where they're used. You can pass any Python literal from the command line: numbers, strings, tuples, lists, dictionaries, (sets are only supported in some versions of Python). You can also nest the collections arbitrarily as long as they only contain literals.  ","permalink":"https://serge-m.github.io/posts/parameters-parsing-for-python-applications/","summary":"command line arguments is a standard and one of the most common ways to pass parameters to a python script. There exist a list of python libraries that help with that task. Here I am going to list some of them.\nargparse The default choice for the python developer. The module is included in python standard library and comes together with any python distribution.\nExample of usage:\n#!/usr/bin/env bash import argparse def hello(count, name): \u0026quot;\u0026quot;\u0026quot;Simple program that greets NAME for a total of COUNT times.","title":"Parameters parsing for python applications"},{"content":"Nice history log in console git log --all --decorate --oneline --graph  How to remember:\n\u0026quot;A Dog\u0026quot; = git log --all --decorate --oneline --graph\nSample result:\n$ git log --all --decorate --oneline --graph * e4689e3 (HEAD -\u0026gt; master, origin/master, origin/HEAD) tree iteration * 4c3385d using stoi * 0588a47 gitignore for cpp * 8f3f0e6 removed boost, add readme * f225b06 simple expression interpreter * 1ccdda1 chain of responsibility * f37eb7a restructure of the old code, removed large db file | * 3bfdf78 (tag: 0.1, origin/old-training-task-with-data, old-training-task-with-data) fixed readme for task2 | * c0c934f more readme's | * c514699 better readme | * 3e092be added readme, added basic error processing, added logging | * 786fa09 cleaning up | * 96fd9a7 main script for task1 | * b1b0e53 first version of task1 | * 8cf1529 safety check for download, refactoring | * 807cecd draft for task1 | * 68fcdf8 updated readme for task2 | * 7e8b8ae added readme, |/ * d5bd5a4 structure of the directory * d293704 added more tests * aefe72f added gitignore * 06ff547 Initial version for task2   Set date/time for git commits export GIT_COMMITTER_DATE=\u0026quot;2020-01-01 12:00:00\u0026quot;; git commit --date \u0026quot;$GIT_COMMITTER_DATE\u0026quot; -m \u0026quot;commit message\u0026quot;   Caching credentials Remember credentials for 1 hour (3600 seconds):\ngit config --global credential.helper 'cache --timeout=3600'  Disabling cache:\ngit config --global --unset credential.helper   Export a repo to an archive without .git git archive --prefix=ADDITIONAL_PREFIX_INSIDE_ARCHIVE YOUR_BRANCH -o OUTPUT_FILE.zip  tar archive is also supported. More info at git-archive doc.\n ","permalink":"https://serge-m.github.io/posts/git-cheat-sheet/","summary":"Nice history log in console git log --all --decorate --oneline --graph  How to remember:\n\u0026quot;A Dog\u0026quot; = git log --all --decorate --oneline --graph\nSample result:\n$ git log --all --decorate --oneline --graph * e4689e3 (HEAD -\u0026gt; master, origin/master, origin/HEAD) tree iteration * 4c3385d using stoi * 0588a47 gitignore for cpp * 8f3f0e6 removed boost, add readme * f225b06 simple expression interpreter * 1ccdda1 chain of responsibility * f37eb7a restructure of the old code, removed large db file | * 3bfdf78 (tag: 0.","title":"Git cheat sheet"},{"content":"I often need to perform some operations on videos or image sequences. Usually I use linux and ffmpeg, and sometimes I struggle to remember all the commands. Here is a collection of recipes that I usually use with a bit of explanations.\nVideo conversions Cut a range of frames Cut a range of frames (100, 130) from a video and save it to mp4 with a good quality using x264 codec:\nffmpeg -i ./input_video.mp4 -vf \u0026quot;select=between(n\\,100\\,130)\u0026quot; -vsync 0 -vcodec libx264 -crf 15 -an ./output.mp4  Explanation:\n-vf \u0026quot;select=between(n\\,100\\,130)\u0026quot; - cut according to the frame numbers. Slash symbols are essential. -vsync 0 - some synchronization magic. Without it some duplicated/ freezed frames are added. -crf 15 - good quality, large output file. set to a bigger value if you need more compression. -an removes audio from the file. otherwise some black frames are added. I think there is a way to cut audio track as well, I just don't need it usually.  Cut according to time (between seconds 5.5 and 122):\nffmpeg -y -i ./input_video.mp4 -vf \u0026quot;select=between(t\\,5.5\\,122)\u0026quot; -vsync 0 -vcodec libx264 -crf 15 -an ./output.mp4   Resize video ffmpeg -i \u0026quot;input.mp4\u0026quot; -vf scale=\u0026quot;720:480\u0026quot; -vcodec libx264 -y output.mp4  Crop:\nffmpeg -i in.mp4 -filter:v \u0026quot;crop=80:60:200:100\u0026quot; -c:a copy out.mp4  (audio is coped as is)\nCrop to the size divisible by 2 (for x264 encoding)\nffmpeg -i \u0026quot;input.mp4\u0026quot; -filter:v \u0026quot;crop=(floor(iw/2)*2):(floor(ih/2)*2):0:0\u0026quot; -vcodec libx264 -crf 15 -y output.mp4   Compile images into video with a given framerate ffmpeg -framerate 5 -i \u0026quot;input_%04d.jpg\u0026quot; -vcodec libx264 -crf 15 -r 30 -y vis__compiled.mp4  Images are assumed to have frame rate of 5. Video is saved with frame rate 30 (with duplicate frames).\nMore info here\n Convert to jpeg ffmpeg -i ./input_video.mp4 -qmax 1 -qmin 1 -start_number 0 output_image_%05d.jpg  Converting indexed a sequence of images to a sequence of images (png to jpeg compression), starting from 80th frame and saving starting from 80th index:\nffmpeg -start_number 80 -i input_image_%09d.png -qmin 1 -qscale:v 1.5 -start_number 80 output_image_%09d.jpg  Here we specified -qmin 1 to allow -qscale:v to be lower than 2. 2 is a default minimum. The compression is visually close to lossless.\n Stack images/videos # stack horizontally ffmpeg -i input0.mp4 -i input1.mp4 -filter_complex hstack=inputs=2 output.mp4 # stack vertically ffmpeg -i input0.mp4 -i input1.mp4 -filter_complex vstack=inputs=2 output.mp4   Strip metadata (EXIF) from multiple images Single file in-place\nmogrify -strip filename.jpg  Multiple files (linux)\nfind . -name \u0026quot;*.jpg\u0026quot; | sort | xargs -I {} mogrify -strip {}    See also  FFmpeg libav tutorial - learn how media works from basic to transmuxing, transcoding and more\nffmpeg-libav-tutorial on github\nHow to Write a Video Player in Less Than 1000 Lines\n   ","permalink":"https://serge-m.github.io/posts/image-and-video-processing-recipes/","summary":"I often need to perform some operations on videos or image sequences. Usually I use linux and ffmpeg, and sometimes I struggle to remember all the commands. Here is a collection of recipes that I usually use with a bit of explanations.\nVideo conversions Cut a range of frames Cut a range of frames (100, 130) from a video and save it to mp4 with a good quality using x264 codec:","title":"Image and video processing recipes"},{"content":"CMake in VSCode VSCode is a free open source IDE with a lot of nice features. In addition one can chose from a variety of extensions. Looks like Cmake-tools kind of works, but the hotkeys and some settings are far from intuitive.\nIn my previous attempt I ended up removing cmake tools plugin and moving forward with custom task.json and launch.json scripts. Here is a template I made back then. https://github.com/serge-m/vscode_cmake_template.\nRecently I managed to make it more of less convenient (maybe some updates played a role here as well). Here are some settings I needed to make it usable and comparable to CLion.\nSeparate build directory per build type If you have only one directory for your build you will need to rebuild everything when you switch between debug and release. In order to have a separate build directory per build type (Debug/Release) add the following to your settings.json:\n \u0026quot;cmake.buildDirectory\u0026quot;: \u0026quot;${workspaceRoot}/build-${buildType}\u0026quot; By default VSCode and CmakeTools use ninja as a generator for cmake. Often one need to use make. Add\n \u0026quot;cmake.generator\u0026quot;: \u0026quot;Unix Makefiles\u0026quot; to switch from ninja to make.\n Debugging settings In launch.json update \u0026quot;program\u0026quot; parameter:\n \u0026quot;program\u0026quot;: \u0026quot;${command:cmake.launchTargetPath}\u0026quot;, Cmake Tools will substitute the corresponding path. (from docs of CmakeTools)\n Configure cmake settings add cmake.configureSettings to settings.json:\n\u0026quot;cmake.configureSettings\u0026quot;: { \u0026quot;USE_MYMATH\u0026quot;: \u0026quot;ON\u0026quot; }    Getting started with Gtest For any decent project in Cpp it's good to set up testing system. For C++ the standard solution is to use Gtest.\nInstalling Gtest in your ubuntu is not very straightforward. You have to install the package containing source code and then compile it with cmake with sudo rights.\nsudo apt-get install libgtest-dev sudo apt-get install cmake # install cmake cd /usr/src/gtest sudo cmake CMakeLists.txt sudo make # copy or symlink libgtest.a and libgtest_main.a to your /usr/lib folder sudo cp *.a /usr/lib  Many projects use alternative approach. They clone GTest sources from github into the source tree of the project and compile it in-place. That requires some custom scripting in your cmake files.\nSee also: Getting started with Google Test (GTest) on Ubuntu    Handling dependencies in CMake If i need to import a compiled 3rdparty library to my cmake project:\nadd_library(some_library SHARED IMPORTED) set_property(TARGET some_library PROPERTY IMPORTED_LOCATION \u0026quot;${CMAKE_CURRENT_SOURCE_DIR}/lib/some_library.so\u0026quot;) set_property(TARGET some_library PROPERTY INTERFACE_INCLUDE_DIRECTORIES \u0026quot;${CMAKE_CURRENT_SOURCE_DIR}/include/\u0026quot;)   CMake with Conan and Catch2 Here is a sample CMake project that uses Conan for dependency management and Catch2 for testing:\ntst_conan\n Python bindings for C++ code Hybrid Python/C++ packages, revisited. an approach using pybind11 and cmake.\n ","permalink":"https://serge-m.github.io/posts/cpp-and-cmake/","summary":"CMake in VSCode VSCode is a free open source IDE with a lot of nice features. In addition one can chose from a variety of extensions. Looks like Cmake-tools kind of works, but the hotkeys and some settings are far from intuitive.\nIn my previous attempt I ended up removing cmake tools plugin and moving forward with custom task.json and launch.json scripts. Here is a template I made back then. https://github.","title":"C++ and CMake"},{"content":"I was looking for a system monitoring solution that can notify my in case of abnormal CPU/netowrk/disk usage via email or telegram.\nIt seems there are several popular solutions:\nNagios Munin Icinga Spiceworks Monit Cacti Zabbix Glances Monitorix  See also:\nhttps://askubuntu.com/questions/784781/system-monitoring-with-email-notifications  ","permalink":"https://serge-m.github.io/posts/system-monitoring/","summary":"I was looking for a system monitoring solution that can notify my in case of abnormal CPU/netowrk/disk usage via email or telegram.\nIt seems there are several popular solutions:\nNagios Munin Icinga Spiceworks Monit Cacti Zabbix Glances Monitorix  See also:\nhttps://askubuntu.com/questions/784781/system-monitoring-with-email-notifications  ","title":"System monitoring"},{"content":"Ideas for multistage NN training.\nThere is some research on continuous learning without catastrophic forgetting . For example ANML: Learning to Continually Learn (ECAI 2020) arxiv code video\nThe code for the paper is based on another one: OML (Online-aware Meta-learning) ~ NeurIPS19 code video\nOML paper derives some code from MAML:\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks pdf official tf code, also includes some links to other implementations. pytorch code\nThe target for me is to create a multistage network that is suitable for online learning from a stream of natural data, e.g. video or timeseries. That setup is very similar to the online few shot learning they explore in ANML or OML. I think it makes sense to understand the background paper (MAML) first. So I would start with reading and understanding it\u0026rsquo;s code and reproducing the results.\nHowever there is a paper MAML++ pdf code that is a better version of MAML. It\u0026rsquo;s much easier to train MAML++ than the original MAML. That paper also has a bit nicer code structure from the first glance. I would start from it.\nPlan:\n  download code for MAML++, download omniglot data set, run experiment to reproduce the results of MAML++ paper.\n  while it\u0026rsquo;s being trained, scan through MAML++ and (optionally) MAML. Understand the code\n  design an experiment for online learning, maybe one reproducing with OML/ANML. adapt the code accordingly. Run training\n  Here are some further options:\n  design a multistage architecture an implement it\n  or convert the network into depth prediction or camera parameters estimation in online mode\n    There is also a Reptile paper with code that can be useful. It proposes some simplification of gradient descent steps from MAML.\nVideo with a review of several methods of continual learning: Continual Learning in Neural Networks by Pulkit Agarwal\n","permalink":"https://serge-m.github.io/posts/multistage-nn-training-experiment/","summary":"Ideas for multistage NN training.\nThere is some research on continuous learning without catastrophic forgetting . For example ANML: Learning to Continually Learn (ECAI 2020) arxiv code video\nThe code for the paper is based on another one: OML (Online-aware Meta-learning) ~ NeurIPS19 code video\nOML paper derives some code from MAML:\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks pdf official tf code, also includes some links to other implementations.","title":"Multistage NN training experiment"},{"content":"fast.ai library has a pretty easy to use yet powerful capabilities for semantic image segmentation. By default all the classes are treated the same. The network is trained to predict all the labels.\nSometimes it\u0026rsquo;s important to provide non-complete labeling. That means for some areas the label is undefined. The performance of the network should exclude that areas in the loss and accuracy computation. That allows the network predict any other class in those areas.\nHow to exclude certain class (\u0026ldquo;unlabeled area\u0026rdquo;) from the loss function?\nThe loss for image segmentation is refined as CrossEntropyFlat(axis=1) with the following classes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  def CrossEntropyFlat(*args, axis:int=-1, **kwargs): \u0026#34;Same as `nn.CrossEntropyLoss`, but flattens input and target.\u0026#34; return FlattenedLoss(nn.CrossEntropyLoss, *args, axis=axis, **kwargs) class FlattenedLoss(): \u0026#34;Same as `func`, but flattens input and target.\u0026#34; def __init__(self, func, *args, axis:int=-1, floatify:bool=False, is_2d:bool=True, **kwargs): self.func,self.axis,self.floatify,self.is_2d = func(*args,**kwargs),axis,floatify,is_2d functools.update_wrapper(self, self.func) def __repr__(self): return f\u0026#34;FlattenedLoss of {self.func}\u0026#34; @property def reduction(self): return self.func.reduction @reduction.setter def reduction(self, v): self.func.reduction = v @property def weight(self): return self.func.weight @weight.setter def weight(self, v): self.func.weight = v def __call__(self, input:Tensor, target:Tensor, **kwargs)-\u0026gt;Rank0Tensor: input = input.transpose(self.axis,-1).contiguous() target = target.transpose(self.axis,-1).contiguous() if self.floatify: target = target.float() input = input.view(-1,input.shape[-1]) if self.is_2d else input.view(-1) return self.func.__call__(input, target.view(-1), **kwargs)   To exclude some class from the loss function we can follow the advice from the fast ai forum:\nhasLabel = (t != UNLABELED).float() loss = mse(p * hasLabel, t * hasLabel) More specifically one can create a copy of the FlattenedLoss and patch it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  class FlattenedLossWithUnlabeled(): \u0026#34;Same as `func`, but flattens input and target.\u0026#34; def __init__(self, func, *args, axis:int=-1, floatify:bool=False, is_2d:bool=True, **kwargs): self.func,self.axis,self.floatify,self.is_2d = func(*args,**kwargs),axis,floatify,is_2d functools.update_wrapper(self, self.func) def __repr__(self): return f\u0026#34;FlattenedLoss of {self.func}\u0026#34; @property def reduction(self): return self.func.reduction @reduction.setter def reduction(self, v): self.func.reduction = v @property def weight(self): return self.func.weight @weight.setter def weight(self, v): self.func.weight = v def __call__(self, input:Tensor, target:Tensor, **kwargs)-\u0026gt;Rank0Tensor: ###### Start ############### hasLabel = (t != UNLABELED) input = input * hasLabel target = target * hasLabel ###### End ############### input = input.transpose(self.axis,-1).contiguous() target = target.transpose(self.axis,-1).contiguous() if self.floatify: target = target.float() input = input.view(-1,input.shape[-1]) if self.is_2d else input.view(-1) return self.func.__call__(input, target.view(-1), **kwargs)   Now use that class in your learner:\n1 2 3  learn = your_learner(data, my_model, wd=wd, loss_func=FlattenedLossWithUnlabeled(CrossEntropyLoss, axis=1) )   ","permalink":"https://serge-m.github.io/posts/image-segmentation-with-unlabeled-areas-with-fast-ai/","summary":"fast.ai library has a pretty easy to use yet powerful capabilities for semantic image segmentation. By default all the classes are treated the same. The network is trained to predict all the labels.\nSometimes it\u0026rsquo;s important to provide non-complete labeling. That means for some areas the label is undefined. The performance of the network should exclude that areas in the loss and accuracy computation. That allows the network predict any other class in those areas.","title":"Image segmentation with unlabeled areas with fast.ai"},{"content":"For the robocar contest in Berlin I started a project of building an autonomous toy car (scale 1:10). The goal of the contest was to show the fastest lap driving autonomously. The track had 8-shape with lane boundaries marked with white tape.\nUnfortunately the competition got cancelled. But that gave me an opportunity to switch from my 1st gen car to 2nd gen version.\nHere is my first version:\n The second version is built upon a stock RC car Absima Abs1\n The default choice of the software for the robot was a project called donkeycar. Donkeycar implements a driving stack in python. The AI is based on end-to-end machine learning model.\nMy first car was built with that stack and I was not quite happy about it. The code quality and version compatibility was far from ideal. Knowing that stack is unlikely to be useful outside of the contest.\nTherefore I decided to build the second version on top of ROS to learn that framework on the way. I found project omicron that became a basis for my car.\nProject omicron had a goal to reimplement donkeycar functionality in ROS. I had patched it significantly to make it work for my hardware.\nHardware Donkeycar standard architecture looks like this:\nI decided to introduce an intermediate layer of arduino that will (hopefully) protect my raspberry from undesired influence of high-current electornics of the car.\nSoftware Robocar runs on Robot Operating System (ROS). I use Ubuntu image with preinstalled ROS from ubiquity. To run the the code is cloned on the image and I run some launch scripts manually via ssh terminal. In the future I\u0026rsquo;ll probably add some more user-friendly interface for that.\nThe system consists of the following nodes.\n  raspicam_node - captures the images from Raspberry Pi camera and publishes to a ROS topic\n  ai_driver - takes the image and produces steering and throttle values. The values are normalized. The range is (-1, 1).\n  steering_translator - an auxiliary node that converts normalized steering to PWM signal for the servos.\n  arduino bridge - listens to the converted steering topic and publishes the data to the arduino via serial port (USB). In addition it serves as a bridge for the PWM signals captured by arduino from RC-receiver.\n  Arduino sketch takes care of several things:\n  measuring PWM signal from RC-receiver,\n  publishing this received PWM to ROS\n  listening for PWM commands produced by ai_driver and translated by steering_translator\n  listening for a special topic that controls which PWM to use (one from radio or one from ai_driver)\n  shuts down throttle if there is no RC signal (safety measure)\n    Main repository: https://github.com/serge-m/sergem_robocar\nWiki\nSome collected data to train your AI driver\nAckermann steering for the next version of robocar I was looking for Ackerman steering model for Gazebo+ROS to develope a new version of the robot.\n  https://github.com/trainman419/ackermann_vehicle-1\nfrom a blog post\noriginal model https://github.com/jbpassot/ackermann_vehicle\n  wiki article https://en.wikipedia.org/wiki/Ackermann_steering_geometry\n  one more repo https://github.com/benwarrick/simple_ackermann\n  another with a gif https://github.com/froohoo/ackermansteer\n  Serious rally car https://github.com/AutoRally/autorally\n  Another one serious https://github.com/prl-mushr/mushr#component-documentation\n  ","permalink":"https://serge-m.github.io/posts/robocar/","summary":"For the robocar contest in Berlin I started a project of building an autonomous toy car (scale 1:10). The goal of the contest was to show the fastest lap driving autonomously. The track had 8-shape with lane boundaries marked with white tape.\nUnfortunately the competition got cancelled. But that gave me an opportunity to switch from my 1st gen car to 2nd gen version.\nHere is my first version:\n The second version is built upon a stock RC car Absima Abs1","title":"Robocar project"},{"content":"ROS on raspberry pi There is a compiled image for RPi by ubiquity that has ROS kinetic: https://downloads.ubiquityrobotics.com/pi.html.\nIt seems for me too old. It\u0026rsquo;s 2020, there are ubuntu 18, ros melodic and ros2, next year the support of python2.7 will be discontinued. Meh\u0026hellip;\nIt is possible to have ROS melodic on Raspberry Pi 3 B+. See in the next sections.\nInstalling tensorflow for ROS on raspberry pi Alternatively one can try to install it from wheels: https://www.tensorflow.org/install/pip?lang=python3#package-location\nThe dependencies are better to install using pip and piwheels.org:\nsudo pip3 install --extra-index-url=https://www.piwheels.org/simple -U tensorflow keras Issues h5py I encountered an error about h5py:\nImportError: libhdf5_serial.so.100: cannot open shared object file: No such file or directory installing\nsudo apt-get install libhdf5-dev sudo apt-get install libhdf5-serial-dev didnt help.\nSome version mismatch\nI could resolve it by compiling h5py with my version:\nHDF5_VERSION=1.8.16 sudo pip3 install --no-binary=h5py h5py load_img keras_preprocessing.image.utils.load_img results in the error:\n File \u0026quot;/home/ubuntu/sergem_robocar/py3/lib/python3.5/site-packages/keras_preprocessing/image/utils.py\u0026quot;, line 108, in load_img raise ImportError('Could not import PIL.Image. ' ImportError: Could not import PIL.Image. The use of `load_img` requires PIL. pillow is installed.\nThe error may be caused by missing jpeg libraries. Try to load PIL in a separate python and check if it works. I got the following error.\n1 2 3 4 5 6  \u0026gt;\u0026gt;\u0026gt; from PIL import Image as pil_image Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; File \u0026#34;/home/ubuntu/sergem_robocar/py3/lib/python3.5/site-packages/PIL/Image.py\u0026#34;, line 95, in \u0026lt;module\u0026gt; from . import _imaging as core ImportError: libjpeg.so.62: cannot open shared object file: No such file or directory   Easily solvable by\nsudo apt install libjpeg62 Name clash in dynamic_reconfigure Probably it is written somewhere but I haven\u0026rsquo;t read the docs\u0026hellip;\nI am developign a node for robot steering translation. I got an error:\n$ rosrun steering_translator steering_translator.py Traceback (most recent call last): File \u0026quot;/home/user/omicron_robocar/catkin_ws/src/steering_translator/src/steering_translator.py\u0026quot;, line 5, in \u0026lt;module\u0026gt; import steering_translator.cfg File \u0026quot;/home/user/omicron_robocar/catkin_ws/src/steering_translator/src/steering_translator.py\u0026quot;, line 5, in \u0026lt;module\u0026gt; import steering_translator.cfg ImportError: No module named cfg I did everything according to the tutorials How to Write Your First .cfg File and Setting up Dynamic Reconfigure for a Node (python), just changed couple of names. The problem was in the naming of the package (steering_translator) and the executable file (steering_translator.py). They are getting mixed up on the execution. The solution was to rename python file to steering_translator_node.py.\nROS melodic on Raspberry Pi 3 B+ Although there is a pre-built ros-kinetic image from Ubuquity, I would like to run ROS melodic on my Raspberry Pi 3 b+ with Raspberry Camera support.\nYou can find the image I built here.\nmd5 checksum:\na65a6968eb7cdf39655eaee9ee82b38c 20200207_ubuntu_armhf_ros_melodic.img.gz  To use it you need to download it and write on SD card as with any other image for RPi.\nCredentials:\n  User: ubuntu\n  password: robotrobot\n  wifi hotspot SSID: ub_rpi_net\n  wifi password: robotwifi\n  Change the passwords as soon as you logged in.\nIf you want to build the image yourself, you can follow the instructions bellow.\nPrepare ubuntu 18 LTS for Raspberry Pi   go to Ubuntu PaspberryPi, download 18.04 LTS for armhf architecture: ubuntu-18.04.3-preinstalled-server-armhf+raspi3.img.xz\nI tried to run arm64 version but encountered issues with building ROS node for raspberry pi. With armhf it works.\n  write the image to SD card\n  to enable camera open /boot/config.txt on SD card and make sure the following lines are there:\nstart_x=1 gpu_mem=128 # at least, or more if you wish    insert the flash card into the Raspberry\n  Connecting Raspberry   Switch on the Raspberry\n  It turned out that Raspberry pi cannot connect to my laptop directly with the default ethernet settings (something is wrong with my DHCP settings).\n  One option is to use router:\n Connect RPi to a router with a cable connect your laptop to the same router    Another option is to change the settings of the connection on the laptop so that the connection in shared. This way IP gets automatically assigned to both laptop and Raspberry.   find the ip address of the connected Raspberry with one of the commands:\n nmap -sn YOUR_ROUTER_IP/24 or nmap -sn YOUR_LAPTOP_IP/24. the command may differ depending on the DHCP settings of your network. In my case it is nmap -sn 192.168.125.1/24 For direct connection: arp -a    ssh -X ubuntu@RASPBERRY_IP, use password ubuntu.\n -X option allows to run gui apps via ssh.      Update the system sudo add-apt-repository ppa:ubuntu-raspi2/ppa # for standard RPi tools sudo apt update \u0026amp;\u0026amp; sudo apt upgrade # this will take some time  If the update doesn\u0026rsquo;t work the reason could be time synchronization between ubuntu servers and raspberry py. Try to install ntp: sudo apt install ntp\nRemove unnecessary tools Removing cloud-init:\nsudo apt-get purge cloud-init # archive file for a backup sudo tar --remove-files -cvzf ~/backup-cloud-init.tar.gz /etc/cloud/ /var/lib/cloud/ sudo reboot # restart  Install ROS Melodic Follow the standard instructions from Ubuntu install of ROS Melodic. I was installing sudo apt install -y ros-melodic-ros-base to save some time and disk space.\nSetting up a node for Raspberry Camera I would like to use the standard RPi camera with ROS. There is a node raspicam_node by Ubiquity, but there is no compiled version for ROS melodic that I could install using apt. Let\u0026rsquo;s compile it. I have checked out kinetic branch, revision 3209cd801b1006c8959540f4efc3774ee19aaa78.\nmkdir -p ~/catkin_ws/src cd ~/catkin_ws/src git clone https://github.com/UbiquityRobotics/raspicam_node cd ~/catkin_ws/ source /opt/ros/melodic/setup.bash  Open file /etc/ros/rosdep/sources.list.d/30-ubiquity.list and add there the line:\nyaml https://raw.githubusercontent.com/UbiquityRobotics/rosdep/master/raspberry-pi.yaml  Now update dependencies:\nrosdep update cd ~/catkin_ws rosdep install --from-paths src --ignore-src --rosdistro=melodic -y  Build:\ncatkin_make install  Install rqt_image_view to see the results:\nsudo apt install -y ros-melodic-rqt-image-view  Install tools to work with raspberry camera:\nsudo apt install -y libraspberrypi-bin libraspberrypi-bin-nonfree  For example, now you can make a photo using raspi camera:\nraspistill -o img.jpg  Test run of the raspicam_node:\nsource ~/catkin_ws/devel/setup.bash roslaunch raspicam_node camerav2_1280x960.launch  You have to see the following line in the end:\n[ INFO] [1581100897.421303654]: Video capture started  Press Ctrl+C to stop the node.\nIf you get an error like this:\nsetting /run_id to fb66ef3e-49d5-11ea-9db0-b827eb318c11 process[rosout-1]: started with pid [23520] started core service [/rosout] process[raspicam_node-2]: started with pid [23523] mmal: mmal_vc_component_create: failed to create component 'vc.ril.camera' (1:ENOMEM) mmal: mmal_component_create_core: could not create component 'vc.ril.camera' (1) mmal: Failed to create camera component [ERROR] [1581099401.593581701]: Failed to create camera component [ERROR] [1581099401.596934388]: init_cam: Failed to create camera component  that mean you haven\u0026rsquo;t enable camera. Look at the instructions above about config.txt file.\nNow you can try to run camera node in the background and run rqt_image_view to check if the camera works:\nroslaunch raspicam_node camerav2_1280x960.launch \u0026amp; rosrun rqt_image_view rqt_image_view  Result should look like this:\nEnabling wifi connection Let\u0026rsquo;s say you want Raspberry pi to automatically connect to your home wifi network.\nInstall NetworkManager:\nsudo apt install -y network-manager  enable network manager\nsudo systemctl enable NetworkManager sudo systemctl start NetworkManager  add a connection to your Wifi router\nnmcli device wifi rescan nmcli device wifi list nmcli device wifi connect SSID-Name password PASSWORD  Enable hot spot Wifi on RaspberryPi (ubuntu 18.04) Enabling hotspot on raspberry allows connection via Wifi without having a router or ethernet cable.\ninstall and enable network manager as described above\nCreate hotspot connection:\nsudo nmcli dev wifi hotspot ifname wlan0 ssid ub_rpi_net password \u0026quot;robotwifi\u0026quot;  Enable autoconnection and enable the connection:\nsudo nmcli connection modify Hotspot connection.autoconnect True nmcli con up Hotspot  You can change the default IP addresses for the Hotspot (optional):\nsudo nmcli connection modify Hotspot ipv4.addresses 192.168.111.1/24 sudo nmcli connection modify Hotspot ipv4.gateway 192.168.111.1  restart network manager to enable the changes:\nsudo systemctl restart NetworkManager  Your /etc/NetworkManager/system-connections/Hotspot file will look like this:\nubuntu@ubuntu:~/catkin_ws$ sudo cat /etc/NetworkManager/system-connections/Hotspot [connection] id=Hotspot uuid=a1103c90-07ab-4a2b-ba54-e267a716df2d type=wifi permissions= timestamp=1581101574 [wifi] mac-address=B8:27:EB:64:D9:44 mac-address-blacklist= mode=ap seen-bssids=B8:27:EB:64:D9:44; ssid=ub_rpi_net [wifi-security] group=ccmp; key-mgmt=wpa-psk pairwise=ccmp; proto=rsn; psk=robotwifi [ipv4] address1=192.168.111.1/24,192.168.111.1 dns-search= method=shared [ipv6] addr-gen-mode=stable-privacy dns-search= method=ignore  After that you should be able to connect your laptop to the hotspot and ssh to another address: ssh -X ubuntu@192.168.111.1\nCreating an image of the SD card for recovery One can create the image using the standard Ubuntu Disks app on your laptop. Alternatively we can use dd with saves some clicks, but more dangerous is a way.\nAssuming /dev/mmcblk0 is your SD card. Creating an image for the whole SD card:\ndd bs=1M if=/dev/mmcblk0 | gzip -c --fast| dd of=/path/to/image.gz  That is the safest option.\nIn my case the SD card is 32 Gb and the disk space occupied is about 6Gb. To save some disk space and decrease the image size I resize the partition \u0026ldquo;writable\u0026rdquo; to something about 5.5 Gb, and then create a shorter version that should also work:\ndd bs=1M count=\u0026lt;size_in_MBs\u0026gt; if=/dev/mmcblk0 | gzip -c --fast| dd of=/path/to/image.gz  size_in_MBs is 6000 for me to cover both partitions.\nHow to restore the image.\ndd if=/path/to/image.gz | gunzip -c | dd bs=1M of=/dev/mmcblk0 Be careful. There is no safety net for this command. You can corrupt your data. Better use \u0026ldquo;Disks\u0026rdquo; app to be on the safe side.\nThat\u0026rsquo;s it. We have created an image!\nExtra notes: There is some cloud-init function in that Ubuntu image but I didn\u0026rsquo;t manage to make it work. It seems some people are more successful with that: https://gitlab.com/Bjorn_Samuelsson/raspberry-pi-cloud-init-wifi.git\nSome guides:   https://roboticsbackend.com/install-ros-on-raspberry-pi-3/\n  https://www.youtube.com/watch?v=VFuHG-Ho4Fk\n  https://roboticsweekends.blogspot.com/2017/12/how-to-install-ros-on-raspberry-pi-2-or.html\n  https://wiki.ros.org/ROSberryPi/Installing%20ROS%20Kinetic%20on%20the%20Raspberry%20Pi\n  Rosserial Getting a lot of errors\n[INFO] [1583529448.984198]: wrong checksum for topic id and msg [ERROR] [1583529515.481192]: Lost sync with device, restarting... [INFO] [1583529515.484453]: Requesting topics... may be caused by some blocking operations in arduino. I was using a standard library for Adafruit PCA9685. The library uses some synchronous calls from Wire.h. It seems that was causing issues.\nCmake + Clion + ROS In order to have ROS environment and dependencies loaded to CLion one has to launch CLion in the sourced environment.\nsource ./devel/setup.bash sh PATH_TO_CLION/bin/clion.sh  Then open a ROS project: File -\u0026gt; Open -\u0026gt; select YOUR_ROS_WORKSPACE/src/CMakeLists.txt -\u0026gt; Open as Project.\n","permalink":"https://serge-m.github.io/posts/ros-experience/","summary":"ROS on raspberry pi There is a compiled image for RPi by ubiquity that has ROS kinetic: https://downloads.ubiquityrobotics.com/pi.html.\nIt seems for me too old. It\u0026rsquo;s 2020, there are ubuntu 18, ros melodic and ros2, next year the support of python2.7 will be discontinued. Meh\u0026hellip;\nIt is possible to have ROS melodic on Raspberry Pi 3 B+. See in the next sections.\nInstalling tensorflow for ROS on raspberry pi Alternatively one can try to install it from wheels: https://www.","title":"ROS experience"},{"content":"Lenovo e490 is a cheaper and less performant alternative to the famous T series.\nThe laptop came to me with a deformed box but I decided to give it a try and so far so good.\nI have bought a basic version and upgraded RAM for it. Here are some photos about disassembly process.\nTo open the case one hast to unscrew several screws and accurately open the flexible cover using some non-sharp tool (like guitar pick).\nInternals:\nInternals with added memory plane: There is also an empty ssd slot that is filled with a plastic placeholder: How to replace battery / Disassembly Video:\n   ","permalink":"https://serge-m.github.io/posts/lenovo-e490/","summary":"Lenovo e490 is a cheaper and less performant alternative to the famous T series.\nThe laptop came to me with a deformed box but I decided to give it a try and so far so good.\nI have bought a basic version and upgraded RAM for it. Here are some photos about disassembly process.\nTo open the case one hast to unscrew several screws and accurately open the flexible cover using some non-sharp tool (like guitar pick).","title":"Lenovo E490 memory upgrade"},{"content":"Arduino board Arduino Nano v2.3 manual (pdf)\nArduino and shift register 74HC595 74HC595 Datasheet (pdf)\n74HC595 Explanation\nYoutube tutorial with buttons:\n Tutorial with arduino - I was using that.\nMy version on a breadboard:\n Code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  //the pins we are using int latchPin = 2; int clockPin = 3; int dataPin = 4; void setup() { //set all the pins used to talk to the chip  //as output pins so we can write to them  pinMode(latchPin, OUTPUT); pinMode(clockPin, OUTPUT); pinMode(dataPin, OUTPUT); } void loop() { for (int i = 0; i \u0026lt; 16; i++) { //take the latchPin low so the LEDs don\u0026#39;t change while we are writing data  digitalWrite(latchPin, LOW); //shift out the bits  shiftOut(dataPin, clockPin, MSBFIRST, i); //take the latch pin high so the pins reflect  //the data we have sent  digitalWrite(latchPin, HIGH); // pause before next value:  delay(50); } }   Alternatives  More powerful shift register: TPIC6B595 PWM: TLC5940, TLC5947 PWM with i2c: PCA9685, PCA9635  PWM via shift register PWM Through a 74HC595 Shift Register at forum.arduino.cc\nIssues with upload to Arduino Nano v3 Trying to connect to my Arduino clone (JOY-IT Arduino-compatible Nano V3 Board with ATmega328P-AU) from Ubuntu 18 I got a strange error messages.\nThere are three ways you can install Arduin IDE in ubuntu 18:\n sudo apt install arduino - install version 1.0.5 as on 2019-07-04 arduino-mhall119 1.8.5 via snap and Ubuntu Software app arduino IDE 1.8.9 from https://www.arduino.cc/en/guide/linux  First of all I had to add a current user to dialout group and logout-login \u0026ndash; that\u0026rsquo;s fine:\n1  sudo usermod -a -G dialout \u0026lt;username\u0026gt;   Then all the tutorials require for checking\n1  ls -l /dev/ttyACM*   And I don\u0026rsquo;t have that. I have USB0 instead:\n1 2  sudo ls -l /dev/ttyUSB* crw-rw---- 1 root dialout 188, 0 Jul 14 14:11 /dev/ttyUSB0   When I tried to upload some simple programms to the board I got: *\navrdude: ser_open(): can't open device \u0026quot;/dev/ttyUSB0\u0026quot;: Permission denied  For version from sudo apt install:  arduino nano avrdude: ser_open(): can't open device \u0026quot;COM1\u0026quot;: No such file or directory That version was way too old.\n For the version from website I got  Build options changed, rebuilding all Sketch uses 994 bytes (3%) of program storage space. Maximum is 30720 bytes. Global variables use 9 bytes (0%) of dynamic memory, leaving 2039 bytes for local variables. Maximum is 2048 bytes. avrdude: stk500_recv(): programmer is not responding avrdude: stk500_getsync() attempt 1 of 10: not in sync: resp=0x00 avrdude: stk500_recv(): programmer is not responding avrdude: stk500_getsync() attempt 2 of 10: not in sync: resp=0x00 That was almost fine.\nTo fix that I had to chose Tools -\u0026gt; Processor -\u0026gt; ATmega 328P (old bootloader) instead of just \u0026ldquo;ATmega 328P\u0026rdquo;\n","permalink":"https://serge-m.github.io/posts/arduino-experiments/","summary":"Arduino board Arduino Nano v2.3 manual (pdf)\nArduino and shift register 74HC595 74HC595 Datasheet (pdf)\n74HC595 Explanation\nYoutube tutorial with buttons:\n Tutorial with arduino - I was using that.\nMy version on a breadboard:\n Code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  //the pins we are using int latchPin = 2; int clockPin = 3; int dataPin = 4; void setup() { //set all the pins used to talk to the chip  //as output pins so we can write to them  pinMode(latchPin, OUTPUT); pinMode(clockPin, OUTPUT); pinMode(dataPin, OUTPUT); } void loop() { for (int i = 0; i \u0026lt; 16; i++) { //take the latchPin low so the LEDs don\u0026#39;t change while we are writing data  digitalWrite(latchPin, LOW); //shift out the bits  shiftOut(dataPin, clockPin, MSBFIRST, i); //take the latch pin high so the pins reflect  //the data we have sent  digitalWrite(latchPin, HIGH); // pause before next value:  delay(50); } }   Alternatives  More powerful shift register: TPIC6B595 PWM: TLC5940, TLC5947 PWM with i2c: PCA9685, PCA9635  PWM via shift register PWM Through a 74HC595 Shift Register at forum.","title":"Arduino Experiments"},{"content":"So, what else is there except for opencv\u0026hellip;\nCCV CCV website, github\nCCV 0.7 comes with a sub-10% image classifier, a decent face detector.\n It runs on Mac OSX, Linux, FreeBSD, Windows*, iPhone, iPad, Android, Raspberry Pi. In fact, anything that has a proper C compiler probably can run ccv. The majority (with notable exception of convolutional networks, which requires a BLAS library) of ccv will just work with no compilation flags or dependencies.\n List of algorithms:\n TLD: Track Learn Detect SWT: Stroke Width Transform SIFT: Scale Invariant Feature Transform SCD: SURF-Cascade Detection ICF: Integral Channel Features HTTP: A REST-ful API DPM: Deformable Parts Model ConvNet: Deep Convolutional Networks Cache: We are Terrible Magicians BBF: Brightness Binary Feature  DLib  Dlib is a modern C++ toolkit containing machine learning algorithms and tools for creating complex software in C++ to solve real world problems. It is used in both industry and academia in a wide range of domains including robotics, embedded devices, mobile phones, and large high performance computing environments. Dlib\u0026rsquo;s open source licensing allows you to use it in any application, free of charge.\n Face recognition Uniform-LBP github\nReverse image search dsys/match Scalable reverse image search built on Kubernetes and Elasticsearch.\ngithub\n","permalink":"https://serge-m.github.io/posts/computer-vision-libraries/","summary":"So, what else is there except for opencv\u0026hellip;\nCCV CCV website, github\nCCV 0.7 comes with a sub-10% image classifier, a decent face detector.\n It runs on Mac OSX, Linux, FreeBSD, Windows*, iPhone, iPad, Android, Raspberry Pi. In fact, anything that has a proper C compiler probably can run ccv. The majority (with notable exception of convolutional networks, which requires a BLAS library) of ccv will just work with no compilation flags or dependencies.","title":"Computer vision libraries"},{"content":"Basics Connect as user postgres:\n1  psql -U postgres   Connect to a specific database:\n\\c database_name; Quit the psql:\n\\q List all databases:\n\\l Lists all tables in the current database:\n\\dt List all users:\n\\du Create a new role username with a password:\n1  CREATE ROLE username NOINHERIT LOGIN PASSWORD password;   Managing tables Create a new table or a temporary table\n1 2 3 4 5 6 7  CREATE [TEMP] TABLE [IF NOT EXISTS] table_name( pk SERIAL PRIMARY KEY, c1 type(size) NOT NULL, c2 type(size) NULL, ... );   Add a new column to a table:\n1  ALTER TABLE table_name ADD COLUMN new_column_name TYPE;   Drop a column in a table:\n1  ALTER TABLE table_name DROP COLUMN column_name;   Rename a column:\n1  ALTER TABLE table_name RENAME column_name TO new_column_name;   Set or remove a default value for a column:\n1  ALTER TABLE table_name ALTER COLUMN [SET DEFAULT value | DROP DEFAULT]   Add a primary key to a table.\n1  ALTER TABLE table_name ADD PRIMARY KEY (column,...);   Remove the primary key from a table.\n1 2  ALTER TABLE table_name DROP CONSTRAINT primary_key_constraint_name;   Rename a table.\n1  ALTER TABLE table_name RENAME TO new_table_name;   Credentials via environment variables 1 2 3  PGHOST=10.1.1.1 \\  PGUSER=user PGPASSWORD=password \\  psql my_db -c \u0026#34;select * from my_table\u0026#34;   See also  PostgreSQL Cheat Sheet How to non interactively provide password for the PostgreSQL interactive terminal  ","permalink":"https://serge-m.github.io/posts/postgres-cheatsheet/","summary":"Basics Connect as user postgres:\n1  psql -U postgres   Connect to a specific database:\n\\c database_name; Quit the psql:\n\\q List all databases:\n\\l Lists all tables in the current database:\n\\dt List all users:\n\\du Create a new role username with a password:\n1  CREATE ROLE username NOINHERIT LOGIN PASSWORD password;   Managing tables Create a new table or a temporary table\n1 2 3 4 5 6 7  CREATE [TEMP] TABLE [IF NOT EXISTS] table_name( pk SERIAL PRIMARY KEY, c1 type(size) NOT NULL, c2 type(size) NULL, .","title":"Postgres cheatsheet"},{"content":"Explanatory data analysis requires interactive code execution. In case of spark and emr it is very convenient to run the code from jupyter notebooks on a remote cluster. EMR allows installing jupyter on the spark master. In order to do that configure \u0026quot;Applications\u0026quot; field for the emr cluster to contain also jupyter hub. For example:\n\u0026quot;Applications\u0026quot;: [ { \u0026quot;Name\u0026quot;: \u0026quot;Ganglia\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;3.7.2\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;Spark\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;2.4.0\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;Zeppelin\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;0.8.0\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;JupyterHub\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;0.9.4\u0026quot; } ], Set KeepJobFlowAliveWhenNoSteps: true and set up all the required python libraries using bootstrap scripts.\nAfter cluster is created and bootstrapped you can get an address of the master in the aws console. Create a ssh tunnel to the port 9443 on the master. it is a port of jupyterhub:\nexport HOST=\u0026lt;public address of your master\u0026gt; ssh -i ~/.ssh/your_private_key.pem -L 9443:$HOST:9443 hadoop@$HOST Now when you connect to your local 9443 port it is redirected to the master\u0026rsquo;s 9443 port.\nOpen the browser an connect to https://localhost:9443/hub/login. HTTPS and the endpoint is important!\nDefault user name is jovyan and the password is jupyter. Use with care.\nEnjoy!\nTroubleshooting Long running pyspark kernel is terminated after approximately 1 hour with the error:\nAn error was encountered: Invalid status code '404' from http://ip-123-123-123-123.eu-west-1.compute.internal:8998/sessions/1 with error payload: \u0026quot;Session '1' not found.\u0026quot; To fix it add the following to your emr cluster Configurations:\n[{'classification': 'livy-conf','Properties': {'livy.server.session.timeout':'5h'}}] source\nSee also  PySpark SQL Cheat Sheet (pdf)  ","permalink":"https://serge-m.github.io/posts/jupyter-notebooks-on-emr/","summary":"Explanatory data analysis requires interactive code execution. In case of spark and emr it is very convenient to run the code from jupyter notebooks on a remote cluster. EMR allows installing jupyter on the spark master. In order to do that configure \u0026quot;Applications\u0026quot; field for the emr cluster to contain also jupyter hub. For example:\n\u0026quot;Applications\u0026quot;: [ { \u0026quot;Name\u0026quot;: \u0026quot;Ganglia\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;3.7.2\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;Spark\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;2.4.0\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;Zeppelin\u0026quot;, \u0026quot;Version\u0026quot;: \u0026quot;0.","title":"Jupyter notebooks on EMR"},{"content":"How to install spark locally Considering spark without hadoop built-in.\n Download hadoop unpack to /opt/hadoop/ Download spark without hadoop, unpack to /opt/spark Install java. Set JAVA_HOVE environment variable. For example: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 create environment variables required for spark to run. One can put those in .bashrc  export HADOOP_HOME=/opt/hadoop export SPARK_DIST_CLASSPATH=$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/* Now you can run pyspark for example:\n$ /opt/spark/bin/pyspark Python 2.7.12 (default, Nov 12 2018, 14:36:49) [GCC 5.4.0 20160609] on linux2 Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/opt/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 2019-01-31 08:36:02 WARN Utils:66 - .... 2019-01-31 08:36:02 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address 2019-01-31 08:36:03 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Setting default log level to \u0026quot;WARN\u0026quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2019-01-31 08:36:04 WARN Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.3.2 /_/ Using Python version 2.7.12 (default, Nov 12 2018 14:36:49) SparkSession available as 'spark'. \u0026gt;\u0026gt;\u0026gt; exit() Running pyspark in jupyter notebooks locally To open interactive pyspark session in jupyter notebooks do this:\nexport PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS='notebook' /opt/spark/bin/pyspark Here jupyter server running locally connects to the spark running locally.\nTroubleshooting if you get\n/opt/spark/bin$ ./pyspark Python 2.7.12 (default, Nov 12 2018, 14:36:49) [GCC 5.4.0 20160609] on linux2 Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. Error: A JNI error has occurred, please check your installation and try again Exception in thread \u0026quot;main\u0026quot; java.lang.NoClassDefFoundError: org/slf4j/Logger at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.privateGetMethodRecursive(Class.java:3048) at java.lang.Class.getMethod0(Class.java:3018) at java.lang.Class.getMethod(Class.java:1784) at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526) Caused by: java.lang.ClassNotFoundException: org.slf4j.Logger at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 7 more Traceback (most recent call last): File \u0026quot;/opt/spark/python/pyspark/shell.py\u0026quot;, line 38, in \u0026lt;module\u0026gt; SparkContext._ensure_initialized() File \u0026quot;/opt/spark/python/pyspark/context.py\u0026quot;, line 300, in _ensure_initialized SparkContext._gateway = gateway or launch_gateway(conf) File \u0026quot;/opt/spark/python/pyspark/java_gateway.py\u0026quot;, line 93, in launch_gateway raise Exception(\u0026quot;Java gateway process exited before sending its port number\u0026quot;) Exception: Java gateway process exited before sending its port number that means that you haven\u0026rsquo;t set SPARK_DIST_CLASSPATH (spark cannot find slf4j which is needed for logging)\nSee also  Get Started with PySpark and Jupyter Notebook in 3 Minutes How to set up PySpark for your Jupyter notebook Downloading spark and getting started with python notebooks (jupyter) locally on a single computer Exception: Java gateway process exited before sending the driver its port number How to access s3a:// files from Apache Spark?  ","permalink":"https://serge-m.github.io/posts/spark-on-a-local-machine/","summary":"How to install spark locally Considering spark without hadoop built-in.\n Download hadoop unpack to /opt/hadoop/ Download spark without hadoop, unpack to /opt/spark Install java. Set JAVA_HOVE environment variable. For example: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 create environment variables required for spark to run. One can put those in .bashrc  export HADOOP_HOME=/opt/hadoop export SPARK_DIST_CLASSPATH=$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/* Now you can run pyspark for example:\n$ /opt/spark/bin/pyspark Python 2.7.12 (default, Nov 12 2018, 14:36:49) [GCC 5.4.0 20160609] on linux2 Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information.","title":"Spark on a local machine"},{"content":"Libraries for GPIO Node JS Python Using RPi.GPIO  How to Exit GPIO programs cleanly, avoid warnings and protect your Pi Setting up RPi.GPIO, numbering systems and inputs On using hardware PWM without sudo due to permissions for /dev/gpiomem: discussion  General pigpio The library also provides a service. It can be useful if you don\u0026rsquo;t want to give root access to the client applications and want to control PWM for example.\nHere is how to create a system service for pigpiod:\nCreate file /etc/systemd/system/pigpiod.service:\n[Unit] Description=pigpiod service [Service] ExecStart=/usr/bin/pigpiod -g -n localhost RestartSec=5 Restart=always [Install] WantedBy=multi-user.target run and check status\n1 2  sudo systemctl start pigpiod.service sudo systemctl status pigpiod.service   should be something like this:\n● pigpiod.service - pigpiod service Loaded: loaded (/etc/systemd/system/pigpiod.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-09-19 12:22:30 UTC; 1s ago Main PID: 10074 (pigpiod) CGroup: /system.slice/pigpiod.service └─10074 /usr/bin/pigpiod -l -g Now enable the service:\nsudo systemctl enable pigpiod.service Now pigpriod has to run on startup of your raspberry pi and provide interface to gpio:\npigs w 4 1 If something went wrong and the pigpiod service doesn\u0026rsquo;t start you will see\n1 2 3  $ pigs w 4 1 socket connect failed   ","permalink":"https://serge-m.github.io/posts/gpio-controls-for-rasbperry-pi/","summary":"Libraries for GPIO Node JS Python Using RPi.GPIO  How to Exit GPIO programs cleanly, avoid warnings and protect your Pi Setting up RPi.GPIO, numbering systems and inputs On using hardware PWM without sudo due to permissions for /dev/gpiomem: discussion  General pigpio The library also provides a service. It can be useful if you don\u0026rsquo;t want to give root access to the client applications and want to control PWM for example.","title":"GPIO controls for Rasbperry Pi"},{"content":"Here are some recipies to make ubuntu installed on USB drive to work faster.\n[1]\n[2]\nreducing swapping Add these lines to /etc/sysctl.conf, and reboot.\nvm.swappiness = 0 vm.dirty_background_ratio = 20 vm.dirty_expire_centisecs = 0 vm.dirty_ratio = 80 vm.dirty_writeback_centisecs = 0 More caching while writting on disk Add noatime,commit=120,\u0026hellip; to /etc/fstab entries for / and /home\n","permalink":"https://serge-m.github.io/posts/disk-usage-ubuntu/","summary":"Here are some recipies to make ubuntu installed on USB drive to work faster.\n[1]\n[2]\nreducing swapping Add these lines to /etc/sysctl.conf, and reboot.\nvm.swappiness = 0 vm.dirty_background_ratio = 20 vm.dirty_expire_centisecs = 0 vm.dirty_ratio = 80 vm.dirty_writeback_centisecs = 0 More caching while writting on disk Add noatime,commit=120,\u0026hellip; to /etc/fstab entries for / and /home","title":"Reducing disk usage in Ubuntu"},{"content":"Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.\nDocker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark\nExamples Running SparkPi sample program (one of the examples from the docs of Spark):\ndocker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:\necho -e \u0026quot;import pyspark\\n\\nprint(pyspark.SparkContext().parallelize(range(0, 10)).count())\u0026quot; \u0026gt; count.py docker run --rm -it -p 4040:4040 -v $(pwd)/count.py:/count.py gettyimages/spark bin/spark-submit /count.py Here we create a file with a python program outside of the docker. During docker run we map this file to the file inside the docker container with path /count.py and the we execute bin/spark-submit command that executes our code.\nYou can also run PySpark in interactive mode:\n$ docker run --rm -it -p 4040:4040 gettyimages/spark bin/pyspark Python 3.5.3 (default, Jan 19 2017, 14:11:04) [GCC 6.3.0 20170118] on linux Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. 2018-07-29 20:03:59 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Setting default log level to \u0026quot;WARN\u0026quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.3.1 /_/ Using Python version 3.5.3 (default, Jan 19 2017 14:11:04) SparkSession available as 'spark'. \u0026gt;\u0026gt;\u0026gt; sc \u0026lt;SparkContext master=local[*] appName=PySparkShell\u0026gt; \u0026gt;\u0026gt;\u0026gt; Now you can enter commands and evaluate your code in interactive mode.\nRunning a cluster with docker-compose One can use docker-compose.yaml file from https://github.com/gettyimages/docker-spark.git to run a cluster locally.\ndocker-compose.yaml file looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  master:image:gettyimages/sparkcommand:bin/spark-class org.apache.spark.deploy.master.Master -h masterhostname:masterenvironment:MASTER:spark://master:7077SPARK_CONF_DIR:/confSPARK_PUBLIC_DNS:localhostexpose:- 7001- 7002- 7003- 7004- 7005- 7006- 7077- 6066ports:- 4040:4040- 6066:6066- 7077:7077- 8080:8080volumes:- ./conf/master:/conf- ./data:/tmp/dataworker:image:gettyimages/sparkcommand:bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077hostname:workerenvironment:SPARK_CONF_DIR:/confSPARK_WORKER_CORES:2SPARK_WORKER_MEMORY:1gSPARK_WORKER_PORT:8881SPARK_WORKER_WEBUI_PORT:8081SPARK_PUBLIC_DNS:localhostlinks:- masterexpose:- 7012- 7013- 7014- 7015- 7016- 8881ports:- 8081:8081volumes:- ./conf/worker:/conf- ./data:/tmp/data  Run it with command.\ndocker-compose up It uses configs for master and worker nodes from conf directory.\nAccessing S3 from local Spark I want to do experiments locally on spark but my data is stored in the cloud - AWS S3. If I deploy spark on EMR credentials are automatically passed to spark from AWS. But locally it is not the case. In the simple case one can use environment variables to pass AWS credentials:\ndocker run --rm -it -e \u0026quot;AWS_ACCESS_KEY_ID=YOURKEY\u0026quot; -e \u0026quot;AWS_SECRET_ACCESS_KEY=YOURSECRET\u0026quot; -p 4040:4040 gettyimages/spark bin/spark-shell Loading credentials from ~/.aws/credentials If you want to use AWS S3 credentials from ~/.aws/credentials you have to do some configuration. in the previous cluster example one have to specify credentials provider. Add the following line to spark-defaults.conf:\nspark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.profile.ProfileCredentialsProvider Let\u0026rsquo;s say we want to run some code like this:\n1 2 3 4 5 6 7 8  from pyspark.sql import SparkSession spark = SparkSession \\ .builder \\ .getOrCreate() data = spark.read.parquet(\u0026#34;s3a://your_bucket/serge-m-test/data.parquet\u0026#34;) data.show()   Now if you configure the rest properly and run the cluster you can access your s3 data from local spark/docker container.\nWithout the configuration we would get the following error:\nCaused by: com.amazonaws.AmazonClientException: No AWS Credentials provided by BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint That basically means that spark has three credentials proveders: BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider. But none of them worked.\n EnvironmentVariableCredentialsProvider - one that loads the credentials from environment variables InstanceProfileCredentialsProvider - one that works on AWS instances. BasicAWSCredentialsProvider - I don\u0026rsquo;t know what it is.  What we need is ProfileCredentialsProvider. It reads the credentials from ~/.aws directory.\nMore about spark and aws aws hadoop libraries (copying)\nApache Spark and Amazon S3 — Gotchas and best practices\nabout profile credentials provider 1 about profile credentials provider 2\nabout hadoop aws s3 access\nabout drivers\nanalytics with airflow and spark\n","permalink":"https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/","summary":"Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.\nDocker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark\nExamples Running SparkPi sample program (one of the examples from the docs of Spark):\ndocker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:","title":"Spark in Docker with AWS credentials"},{"content":"We had a production system written by mathematicians, 50 different stakeholders with conflicting targets, five leadership changes during last year, a dozen of microservices, AWS costs of 10 thousands per week,\nhole galaxy of legacy databases, cron jobs, Celery, greenlets, … Also, unstable API as a dependency, 10 Gb of text dumps as output, user input without validation, false alarms in monitoring, and two dozen unprotected public endpoints. Not that we needed all that for the work, but once you get locked into a serious agile development, the tendency is to push it as far as you can. The only thing that really worried me was the OKRs. There is nothing in the world more helpless and irresponsible and depraved than a man in the depths of an OKRs alignment, and I knew we\u0026rsquo;d get into that rotten stuff pretty soon.\n","permalink":"https://serge-m.github.io/posts/okrs/","summary":"We had a production system written by mathematicians, 50 different stakeholders with conflicting targets, five leadership changes during last year, a dozen of microservices, AWS costs of 10 thousands per week,\nhole galaxy of legacy databases, cron jobs, Celery, greenlets, … Also, unstable API as a dependency, 10 Gb of text dumps as output, user input without validation, false alarms in monitoring, and two dozen unprotected public endpoints. Not that we needed all that for the work, but once you get locked into a serious agile development, the tendency is to push it as far as you can.","title":"OKRs"},{"content":"Bokeh is a library for interactive visualization. One can use it in Jupyter notebooks.\nHere is the example.\nLets say we have a pandas dataframe with timestamps and some values:\n1 2 3 4 5 6 7 8 9 10  import pandas as pd from io import StringIO df = pd.read_csv(StringIO(\u0026#34;\u0026#34;\u0026#34;timestamp,value 2018-01-01T10:00:00,20 2018-01-01T12:00:00,10 2018-01-01T14:00:00,30 2018-01-02T10:30:00,40 2018-01-02T13:00:00,50 2018-01-02T18:00:40,10 \u0026#34;\u0026#34;\u0026#34;), parse_dates=[\u0026#34;timestamp\u0026#34;])   You can visualize it to a nice graph with zoom, selection, and mouse-over tooltips using the bokeh:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  from bokeh.plotting import figure, output_file, show, ColumnDataSource from bokeh.models import HoverTool from bokeh.io import output_notebook, show output_notebook() hover = HoverTool( tooltips=[ (\u0026#34;timestamp\u0026#34;, \u0026#34;@timestamp{%Y-%m-%d%H:%M:%S}\u0026#34;), (\u0026#34;value\u0026#34;, \u0026#34;@value\u0026#34;) ], formatters={ \u0026#39;timestamp\u0026#39; : \u0026#39;datetime\u0026#39;, # use \u0026#39;datetime\u0026#39; formatter for \u0026#39;date\u0026#39; field # use default \u0026#39;numeral\u0026#39; formatter for other fields } ) p = figure(plot_width=800, plot_height=400, tools=[hover, \u0026#39;box_zoom\u0026#39;, \u0026#39;wheel_zoom\u0026#39;, \u0026#39;pan\u0026#39;], title=\u0026#34;Mouse over the dots\u0026#34;, x_axis_type=\u0026#39;datetime\u0026#39;) p.line (\u0026#39;timestamp\u0026#39;, \u0026#39;value\u0026#39;, source=ColumnDataSource(data=df)) show(p)   You can use also\nfrom bokeh.io import output_file output_file(\u0026quot;./some-html.html\u0026quot;) to generate interactive html page with your graph\n","permalink":"https://serge-m.github.io/posts/bokeh-example/","summary":"Bokeh is a library for interactive visualization. One can use it in Jupyter notebooks.\nHere is the example.\nLets say we have a pandas dataframe with timestamps and some values:\n1 2 3 4 5 6 7 8 9 10  import pandas as pd from io import StringIO df = pd.read_csv(StringIO(\u0026#34;\u0026#34;\u0026#34;timestamp,value 2018-01-01T10:00:00,20 2018-01-01T12:00:00,10 2018-01-01T14:00:00,30 2018-01-02T10:30:00,40 2018-01-02T13:00:00,50 2018-01-02T18:00:40,10 \u0026#34;\u0026#34;\u0026#34;), parse_dates=[\u0026#34;timestamp\u0026#34;])   You can visualize it to a nice graph with zoom, selection, and mouse-over tooltips using the bokeh:","title":"Bokeh in jupyter notebooks for interactive plots"},{"content":"Problem There is click module that allows you to create comman dline interfaces for your python scripts. The advantages of click are\n nice syntax  1 2 3 4 5 6 7 8 9 10 11 12 13 14  import click @click.command() @click.option(\u0026#39;--count\u0026#39;, default=1, help=\u0026#39;Number of greetings.\u0026#39;) @click.option(\u0026#39;--name\u0026#39;, prompt=\u0026#39;Your name\u0026#39;, help=\u0026#39;The person to greet.\u0026#39;) def hello(count, name): \u0026#34;\u0026#34;\u0026#34;Simple program that greets NAME for a total of COUNT times.\u0026#34;\u0026#34;\u0026#34; for x in range(count): click.echo(\u0026#39;Hello %s!\u0026#39; % name) if __name__ == \u0026#39;__main__\u0026#39;: hello()    straightforward argument parsing strategy (in comparison to the standard argparse )  Click doesn\u0026rsquo;t support config files by default. There is a number of additional modules that implement this feature.\n click-config click-configfile click_config_file Nice implementation from stackoverflow: https://stackoverflow.com/a/46391887. No module, supports yaml. (Maybe one day I make a module out of it)  Comparison    feature click-config click-configfile click_config_file SO snippet     Last commit in the repository May 5, 2015 Sep 24, 2017 Jan 23, 2018 -   Supports ini yes yes yes easy to implement   Supports json no no yes with strange extension technique easy to implement   Supports yaml yes no yes with strange extension technique yes   implementation notes click-config provides a decorator that takes a python object and overwrites its attributes with values passed into the program via command line arguments. through context_settings usable by simply adding the appropriate decorator Default values from click.option don\u0026rsquo;t work as expected. They override values from the file. One have to check for defaults manually. Not sure if environment variables work nicely.    Syntax examples click-config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  from __future__ import print_function import click import click_config class config(object): class logger(object): level = \u0026#39;INFO\u0026#39; class mysql(object): host = \u0026#39;localhost\u0026#39; @click.command() @click_config.wrap(module=config, sections=(\u0026#39;logger\u0026#39;, \u0026#39;mysql\u0026#39;)) def main(): print(\u0026#39;log level: {}, mysql host: {}\u0026#39;.format( config.logger.level, config.mysql.host )) if __name__ == \u0026#39;__main__\u0026#39;: main()   click-configure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # BASIC SOLUTION FOR: Command that uses one or more configuration files. import click CONTEXT_SETTINGS = dict(default_map=ConfigFileProcessor.read_config()) @click.command(context_settings=CONTEXT_SETTINGS) @click.option(\u0026#34;-n\u0026#34;, \u0026#34;--number\u0026#34;, \u0026#34;numbers\u0026#34;, type=int, multiple=True) @click.pass_context def command_with_config(ctx, numbers): \u0026#34;\u0026#34;\u0026#34;Example for a command that uses an configuration file\u0026#34;\u0026#34;\u0026#34; pass ... if __name__ == \u0026#34;__main__\u0026#34;: command_with_config()   click_config_file 1 2 3 4 5  @click.command() @click.option(\u0026#39;--name\u0026#39;, default=\u0026#39;World\u0026#39;, help=\u0026#39;Who to greet.\u0026#39;) @click_config_file.configuration_option() def hello(name): click.echo(\u0026#39;Hello {}!\u0026#39;.format(name))   Stack overflow snippet 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Custom Class: def CommandWithConfigFile(config_file_param_name): class CustomCommandClass(click.Command): def invoke(self, ctx): config_file = ctx.params[config_file_param_name] if config_file is not None: with open(config_file) as f: config_data = yaml.load(f) for param, value in ctx.params.items(): if value is None and param in config_data: ctx.params[param] = config_data[param] return super(CustomCommandClass, self).invoke(ctx) return CustomCommandClass   Using Custom Class: 1 2 3 4 5  @click.command(cls=CommandWithConfigFile(\u0026#39;config_file\u0026#39;)) @click.argument(\u0026#34;arg\u0026#34;) @click.option(\u0026#34;--opt\u0026#34;) @click.option(\u0026#34;--config_file\u0026#34;, type=click.Path()) def main(arg, opt, config_file):   Test Code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # !/usr/bin/env python import click import yaml @click.command(cls=CommandWithConfigFile(\u0026#39;config_file\u0026#39;)) @click.argument(\u0026#34;arg\u0026#34;) @click.option(\u0026#34;--opt\u0026#34;) @click.option(\u0026#34;--config_file\u0026#34;, type=click.Path()) def main(arg, opt, config_file): print(\u0026#34;arg: {}\u0026#34;.format(arg)) print(\u0026#34;opt: {}\u0026#34;.format(opt)) print(\u0026#34;config_file: {}\u0026#34;.format(config_file)) main(\u0026#39;my_arg --config_file config_file\u0026#39;.split())   Test Results:\narg: my_arg opt: my_opt config_file: config_file ","permalink":"https://serge-m.github.io/posts/click-config-parsers/","summary":"Problem There is click module that allows you to create comman dline interfaces for your python scripts. The advantages of click are\n nice syntax  1 2 3 4 5 6 7 8 9 10 11 12 13 14  import click @click.command() @click.option(\u0026#39;--count\u0026#39;, default=1, help=\u0026#39;Number of greetings.\u0026#39;) @click.option(\u0026#39;--name\u0026#39;, prompt=\u0026#39;Your name\u0026#39;, help=\u0026#39;The person to greet.\u0026#39;) def hello(count, name): \u0026#34;\u0026#34;\u0026#34;Simple program that greets NAME for a total of COUNT times.\u0026#34;\u0026#34;\u0026#34; for x in range(count): click.","title":"Comparison of click-based config parsers for python"},{"content":"Some frequently used commands in Vim File explorer :Explore - opens the file explorer window. :E - the same  Visual commands \u0026gt; - shift right \u0026lt; - shift left y - yank (copy) marked text d - delete marked text ~ - switch case  Cut and Paste yy - yank (copy) a line 2yy - yank 2 lines yw - yank word y$ - yank to end of line p - put (paste) the clipboard after cursor P - put (paste) before cursor dd - delete (cut) a line dw - delete (cut) the current word x - delete (cut) current character  Search/Replace /pattern - search for pattern ?pattern - search backward for pattern n - repeat search in same direction N - repeat search in opposite direction :%s/old/new/g - replace all old with new throughout file :%s/old/new/gc - replace all old with new throughout file with confirmations  Identation settings Create the file\nvim ~/.vim/vimrc Add the configuration:\nfiletype plugin indent on set tabstop=4 set shiftwidth=4 set expandtab Restart vim.\nYou can replace all the tabs with spaces in the entire file with\n:%retab Working with multiple files :e filename - Edit a file in a new buffer :bnext (or :bn) - go to next buffer :bprev (of :bp) - go to previous buffer :bd - delete a buffer (close a file) :sp filename - Open a file in a new buffer and split window ctrl+ws - Split windows ctrl+ww - switch between windows ctrl+wq - Quit a window ctrl+wv - Split windows vertically ctrl+wj - switch wo the bottom window ctrl+wk - switch wo the top window  Paste without reformatting Vim provides the \u0026lsquo;paste\u0026rsquo; option to aid in pasting text unmodified from other applications. You can set it manually like:\n:set paste paste, then\n:set nopaste Upper/lower case transformation ~ - Toggle case of the character under the cursor, or all visually-selected characters.\n3~ Toggle case of the next three characters.\ng~3w Toggle case of the next three words.\ng~iw Toggle case of the current word (inner word – cursor anywhere in word).\ng~$ Toggle case of all characters to end of line.\ng~~ Toggle case of the current line (same as V~).\nThe above uses ~ to toggle case. In each example, you can replace ~ with u to convert to lowercase, or with U to convert to uppercase. For example:\nU Uppercase the visually-selected text. First press v or V then move to select text. If you don\u0026rsquo;t select text, pressing U will undo all changes to the current line.\ngUU Change the current line to uppercase (same as VU).\ngUiw Change current word to uppercase.\nu Lowercase the visually-selected text. If you don\u0026rsquo;t select text, pressing u will undo the last change.\nguu Change the current line to lowercase (same as Vu).\nOther versions:  vimCheatSheet another vim cheat scheet  My .vimrc (or ~/.vim/vimrc) filetype plugin indent on set tabstop=4 set shiftwidth=4 set expandtab ","permalink":"https://serge-m.github.io/posts/vim/","summary":"Some frequently used commands in Vim File explorer :Explore - opens the file explorer window. :E - the same  Visual commands \u0026gt; - shift right \u0026lt; - shift left y - yank (copy) marked text d - delete marked text ~ - switch case  Cut and Paste yy - yank (copy) a line 2yy - yank 2 lines yw - yank word y$ - yank to end of line p - put (paste) the clipboard after cursor P - put (paste) before cursor dd - delete (cut) a line dw - delete (cut) the current word x - delete (cut) current character  Search/Replace /pattern - search for pattern ?","title":"Vim cheat sheet"},{"content":"Given a text file we want to create another file containing only those lines that match a certain regular expression using python3\n1 2 3 4 5 6  import re with open(\u0026#34;./in.txt\u0026#34;, \u0026#34;r\u0026#34;) as input_file, open(\u0026#34;out.txt\u0026#34;, \u0026#34;w\u0026#34;) as output_file: for line in input_file: if re.match(\u0026#34;(.*)import(.*)\u0026#34;, line): print(line, file=output_file)   ","permalink":"https://serge-m.github.io/posts/print-lines-matching-regex/","summary":"Given a text file we want to create another file containing only those lines that match a certain regular expression using python3\n1 2 3 4 5 6  import re with open(\u0026#34;./in.txt\u0026#34;, \u0026#34;r\u0026#34;) as input_file, open(\u0026#34;out.txt\u0026#34;, \u0026#34;w\u0026#34;) as output_file: for line in input_file: if re.match(\u0026#34;(.*)import(.*)\u0026#34;, line): print(line, file=output_file)   ","title":"Select lines matching regular expression in python"},{"content":"Some useful resources about Airflow:\nETL best practices with Airflow\nSeries of articles about Airflow in production:\n Part 1 - about usecases and alternatives Part 2 - about alternatives (Luigi and Paitball) Part 3 - key concepts Part 4 - deployment, issues  More notes about production About start_time: Why isn’t my task getting scheduled?\nOne cannot specify datetime.now() as start_date. Instead one has to provide datetime object without timezone. Probably UTC time is required. You can do something like this:\n1 2 3 4 5  from datetime import datetime, timedelta import pytz start_date_local = datetime(2018,1,1, 10,11, tzinfo=pytz.timezone(\u0026#39;Europe/Minsk\u0026#39;)) # your time, date and time zone go here  start_date_for_airflow = start_date_local.astimezone(pytz.utc).replace(tzinfo=None) # we convert to UTC and remove timezone   Running with LocalExecutor and Postgres from docker We will run postgres in docker:\ndocker run -it -p 5432:5432 --name postgres -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres -d postgres Now we can specify in airflow.cfg:\nsql_alchemy_conn = postgresql://postgres:postgres@localhost:5432/postgres and\nexecutor = LocalExecutor Then run\nairflow initdb airflow scheduler How to specify AWS region for EmrCreateJobFlowOperator There is no parameter in EmrCreateJobFlowOperator to specify\nAWS region where the cluster has to be deployed.\nInternally EmrCreateJobFlowOperator uses EmrHook where get_client_type('emr') is called. get_client_type has a default paramater region_name=None. That means there is no way to set this parameter in code.\nOne has to configure it using airflow configurations. Here we create connection aws_my with AWS region eu-west-1:\nairflow connections -a --conn_id 'aws_my' --conn_uri 'aws:' --conn_extra '{\u0026quot;region_name\u0026quot;: \u0026quot;eu-west-1\u0026quot;}' Now we can use new connection for EmrCreateJobFlowOperator to create Spark cluster on EMR.\ncluster_creator = EmrCreateJobFlowOperator( task_id='create_job_flow', job_flow_overrides={}, # your cluster configurations go here aws_conn_id='aws_my', # \u0026lt;---- this is important emr_conn_id='emr_default', dag=your_dag ) Done\n","permalink":"https://serge-m.github.io/posts/airflow/","summary":"Some useful resources about Airflow:\nETL best practices with Airflow\nSeries of articles about Airflow in production:\n Part 1 - about usecases and alternatives Part 2 - about alternatives (Luigi and Paitball) Part 3 - key concepts Part 4 - deployment, issues  More notes about production About start_time: Why isn’t my task getting scheduled?\nOne cannot specify datetime.now() as start_date. Instead one has to provide datetime object without timezone.","title":"Workflow management with Apache Airflow"},{"content":"Conversion from calendar week to date Sometimes one has to convert a date written as year, calendar week (CW), and day of week to an actual date with month and date. The behaviour in the begin/end of a year may be not straightforward. For example according to ISO 8601 monday date of the CW 1 year 2019 is 31 January 2018. As far as I can see there is no standard function for conversion in python.\nI use the following hacky code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  def year_cw_day_to_date(year: int, calendar_week: int, day_of_week: int): \u0026#34;\u0026#34;\u0026#34; year: integer number, e.g. 1986 calendar_week: integer number, from 1 to 53 day_of_week: integer starting from 1, so that 1 is Monday, 2 is Tuesday, ..., 7 is Sunday \u0026gt;\u0026gt;\u0026gt; year_cw_day_to_date(2019, 1, 1) datetime.date(2018, 12, 31) \u0026gt;\u0026gt;\u0026gt; year_cw_day_to_date(2019, 1, 7) datetime.date(2019, 1, 6) \u0026gt;\u0026gt;\u0026gt; year_cw_day_to_date(2018, 52, 7) datetime.date(2018, 12, 30) \u0026gt;\u0026gt;\u0026gt; year_cw_day_to_date(2018, 53, 2) datetime.date(2019, 1, 1) \u0026gt;\u0026gt;\u0026gt; year_cw_day_to_date(2018, 53, 0) Traceback (most recent call last): ... ValueError: day_of_week must be in range from 1 to 7 \u0026#34;\u0026#34;\u0026#34; if day_of_week not in range(1, 8): raise ValueError(\u0026#34;day_of_week must be in range from 1 to 7\u0026#34;) string_representation = f\u0026#34;{year}-{calendar_week}-{day_of_week}\u0026#34; return datetime.datetime.strptime(string_representation, \u0026#34;%G-%V-%u\u0026#34;).date()   It prints the week-based date to a string and then parses it using %V and %u format from python 3 (docs) Therefore you don\u0026rsquo;t need to implement ISO logic of dates calculation. Hopefully such a funciton apppear in standard library.\nPython 2 doesn\u0026rsquo;t have these %V and %u implemented. :(\nMeasuring elapsed time Python standard library has a set of functions to measure elapsed time.\nOne can get information about each function using time.get_clock_info(name)\n   Clock Adjustable Monotonic Resolution Tick Rate     process_time False True 1e-07 10,000,000   clock False True 4.665306263360271e-07 2,143,482   perf_counter False True 4.665306263360271e-07 2,143,482   monotonic False True 0.015625 64   time True False 0.015625 64    source of measurements: Python Clocks Explained, 2015.\ntime.perf_counter() gives the most accurate results when testing the difference between two times and pretty fast. timeit uses time.perf_counter() by default.\ntime.process_time() can be helpful to understand how long different parts of a program took to run.\n according to the PEP 0418, several modules use (or could use) time.monotonic(), including concurrent.futures, multiprocessing, queue, subprocess, telnet and threading modules to implement timeout.\n See also  How to add months to datetime object in python datetime module in python 3 time — Time access and conversions How to not leap in time using Python, 2020  ","permalink":"https://serge-m.github.io/posts/datetime-in-python/","summary":"Conversion from calendar week to date Sometimes one has to convert a date written as year, calendar week (CW), and day of week to an actual date with month and date. The behaviour in the begin/end of a year may be not straightforward. For example according to ISO 8601 monday date of the CW 1 year 2019 is 31 January 2018. As far as I can see there is no standard function for conversion in python.","title":"Datetime in Python"},{"content":"Awesome explanation of SQLAlchemy with examples and comparison to Django by Armin Ronacher: SQLAlchemy and You\nFlask-SQLAlchemy module Flask-SQLAlchemy is an extension for Flask that adds support for SQLAlchemy to your application.\nHow to add SQLAlchemy to Flask application:\nfrom flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) # configuration of the DB is read from flask configuration storage app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db' # here we define db object that keeps track of sql interactions db = SQLAlchemy(app) Now we are ready to define tables and objects using predefined db.Model class:\nclass User(db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True, nullable=False) email = db.Column(db.String(120), unique=True, nullable=False) def __repr__(self): return '\u0026lt;User %r\u0026gt;' % self.username Now in your endpoint handlers you do:\nfrom your_application import db def post_users_endpoint(): db.session.add(admin) db.session.add(guest) db.session.commit() How to access multiple databases from one Flask application There is a special mechanism for maintaining connections to multiple databases in your flask app: binds\nTo use it you have to adjust configurations like this:\nSQLALCHEMY_DATABASE_URI = 'postgres://localhost/main' SQLALCHEMY_BINDS = { 'users': 'mysqldb://localhost/users', 'appmeta': 'sqlite:////path/to/appmeta.db' } then you can specify binding key in your ORM-classes:\nclass User(db.Model): __bind_key__ = 'users' id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(80), unique=True) Flask tutorials Nice series of flask tutorials: Flask Mega-Tutorial.\nIncluding Oauth authentication In Flask.\nSee also  SQLAlchemy cheat sheet  Python SQLAlchemy Cheatsheet\n about abstractions in SQLalchemy - in Russian  ","permalink":"https://serge-m.github.io/posts/flask-and-sqlalchemy-explained/","summary":"Awesome explanation of SQLAlchemy with examples and comparison to Django by Armin Ronacher: SQLAlchemy and You\nFlask-SQLAlchemy module Flask-SQLAlchemy is an extension for Flask that adds support for SQLAlchemy to your application.\nHow to add SQLAlchemy to Flask application:\nfrom flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) # configuration of the DB is read from flask configuration storage app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:////tmp/test.db' # here we define db object that keeps track of sql interactions db = SQLAlchemy(app) Now we are ready to define tables and objects using predefined db.","title":"Flask and SQLAlchemy explained"},{"content":"Some settings I find useful for a workstation\nCPU monitoring on the main panel Default Ubuntu desktop seems to become finally convenient enough for me starting from Ubuntu 18.04. Only several tweaks are missing. Constantly available CPU/Mem/HDD/Network monitor is one of them. Here is how to install a small widget for a top panel in the default GNOME desktop environment.\n  sudo apt-get install gir1.2-gtop-2.0 gir1.2-networkmanager-1.0 gir1.2-clutter-1.0\n  Go to Ubuntu Software and then search for system monitor extension. Install one named system-monitor:\n Display system information in GNOME Shell status bar\n   After the installation you should see a new applet in the main pannel. One can find settings in the context menu.\n  Desktop, workspaces and gnome extensions settings in Ubuntu 18.04 only show applications of current workspace in launcher: sudo apt install dconf-editor Navigate to org \u0026gt; gnome \u0026gt; shell \u0026gt; extensions \u0026gt; dash-to-dock and check isolate-workspaces\nShow workspace indicator in the main pannel Enable \u0026ldquo;Workspace Indicator\u0026rdquo; extension.\nStatic number of workspaces Showing windows instead of notification \u0026ldquo;window is ready\u0026rdquo; Install extension noannoyance\nor use dconf:\nAlign windows to the corners or sides of the screen Default settings allow only alignment to the sides and full screen. To align windows to the corners I use Put windows gnome extension.\nAutocomplete in shell See autocomplete from history in terminal\nSet nemo as default file manager xdg-mime default nemo.desktop inode/directory application/x-gnome-saved-search now if you run xdg-open ./ or press Super+E nemo starts.\nsource\nAuto completion for fabric Add\nhave fab \u0026amp;\u0026amp; { _fab_completion() { COMPREPLY=() local cur tasks tasks=$(fab --shortlist 2\u0026gt;/dev/null) _get_comp_words_by_ref cur COMPREPLY=( $(compgen -W \u0026quot;${tasks}\u0026quot; -- ${cur}) ) } complete -F _fab_completion fab } to ~/.bash_completion.\nSource\nDefault permissions for directories By default when you call mkdir abc the following permissions are created\nu=rwx,g=rx,o=rx That means that other users, also other groups, will be able to read and execute your files. That is a legacy setting. you can change it by replacing\nUMASK 002 with\nUMASK 077 in file /etc/login.defs. Here is the quote from this file:\n# UMASK is the default umask value for pam_umask and is used by # useradd and newusers to set the mode of the new home directories. # 022 is the \u0026quot;historical\u0026quot; value in Debian for UMASK # 027, or even 077, could be considered better for privacy # There is no One True Answer here : each sysadmin must make up his/her # mind. # # If USERGROUPS_ENAB is set to \u0026quot;yes\u0026quot;, that will modify this UMASK default value # for private user groups, i. e. the uid is the same as gid, and username is # the same as the primary group name: for these, the user permissions will be # used as group permissions, e. g. 022 will become 002. Useful for server setup  Default permissions for directories Add service in linux configuring ssh  Appearance customization  Change background of the lock screen in Ubuntu 18 (stackoverflow)  Ubuntu freezes when RAM is full (DRAFT, from https://habr.com/en/company/selectel/blog/498526/ )\nКстати, никто не подскажет как вылечить зависание ubuntu при заполнении RAM?\nSergeyD today at 07:27 PM  +1\nВключить и настроить swap Включить использование zswap: https://wiki.archlinux.org/index.php/Zswap Установить и настроить earlyoom — осторожно, может прибить все процессы  ValdikSS today at 08:16 PM\n+2 1. Установить и настроить zram, проще всего через zram-tools. 2. Создать /etc/sysctl.d/60-dirty.conf со следующим содержимым:\nvm.dirty_bytes = 67108864 vm.dirty_background_bytes = 16777216\nvm.swappiness=100 vm.watermark_scale_factor=200\nУбедиться, что используется достаточно свежее ядро (5.3+).  Эти три пункта заметно улучшают ситуацию. Если еще полтора года назад, во времена 4.19, я бы однозначно советовал Windows 10 для маломощных компьютеров с малым количеством RAM, то с параметрами, описанными выше, я ошибся с копированием файла, скопировав многогигабайтный файл в RAM целиком, и система только немного замедлилась. Также могу запускать несколько виртуалок одновременно, RAM в настройках которых превышает количество физически установленной памяти, и все также, не побоюсь этого слова, быстро работает, при 5+ ГБ в swap (и zram, и zswap).\n","permalink":"https://serge-m.github.io/posts/ubuntu-linux-settings/","summary":"Some settings I find useful for a workstation\nCPU monitoring on the main panel Default Ubuntu desktop seems to become finally convenient enough for me starting from Ubuntu 18.04. Only several tweaks are missing. Constantly available CPU/Mem/HDD/Network monitor is one of them. Here is how to install a small widget for a top panel in the default GNOME desktop environment.\n  sudo apt-get install gir1.2-gtop-2.0 gir1.2-networkmanager-1.0 gir1.2-clutter-1.0\n  Go to Ubuntu Software and then search for system monitor extension.","title":"Ubuntu/linux settings"},{"content":"Pytest is a powerful tool for testing in python. Here are some notes about hands-on experience.\nRunning tests in pytest with/without a specified mark Execute\npytest -m \u0026quot;integration\u0026quot; if you want to run only tests that have \u0026ldquo;@pytest.mark.integration\u0026rdquo; annotation.\nSimilarly you can run only tests that don\u0026rsquo;t are not marked.\npytest -m \u0026quot;not your_mark\u0026quot; That command will test everything that is not marked as \u0026ldquo;your_mark\u0026rdquo;.\nHow to verify exception message using pytest One can use context manager pytest.raises to check if the exception has correct text inside. You have to check excinfo.value in the end.\n1 2 3 4 5  def test_exception_has_correct_message(): with pytest.raises(Exception) as excinfo: your_function() assert \u0026#39;Failed to establish a new connection\u0026#39; in str(excinfo.value)   excinfo here stays defined also outside of the context scope.\nRunning pytests on Travis CI Register on TravisCI.\nEnable your repo in the settings of Travis CI. URL: https://travis-ci.org/profile/YOUR_GITHUB_NAME\nPut configuration file for travis into the root of your github repository:\nlanguage: python python: - \u0026quot;3.5\u0026quot; branches: only: - master install: - echo \u0026quot;installing\u0026quot; # not used for the example script: - | pip install -r requirements.txt python -m pytest -m \u0026quot;not integration\u0026quot; # running everything except integration tests env: global: [Optional] Add a status image like this  to your readme.md file:\n[![Build Status](https://travis-ci.org/YOUR_GITHUB_NAME/YOUR_REPO_NAME.svg?branch=master)](https://travis-ci.org/YOUR_GITHUB_NAME/YOUR_REPO_NAME) Running pytest with code coverage reports One have to install pytest-cov module first. Then\npy.test --cov=\u0026lt;DIRECTORY_WITH_SOURCES\u0026gt; --cov-report html:htmlcov --cov-report term:skip-covered \u0026lt;DIRECTORY_WITH_TESTS\u0026gt; This command runs tests with code coverage report. Reports are printed in console (--cov-report term:skip-covered) and as html files in htmlcov directory (--cov-report html:htmlcov). To view html report now you can run for example google-chrome ./htmlcov/index.html.\nRestriction on code coverage You can set up pytest-cov so that is will fail if coverage is below a certain level. This can be done with --cov-fail-under parameter. It will give you a restiction for total coverage in a project. I find useful to have code coverage threshold for each module independently. So that if you forgot to test one of 10000 files it is still a failure. unfortunately it seems that separate fail-under per file is not implemented.\nPytest configuration files To avoid writing all the parameters in command line every time one can use configuration files for pytest and coverage reports.\n.coveragerc # in this file one can exclude some file from coverage report. Probably you want to exclude tests themselves [run] omit = \u0026lt;DIRECTORY_WITH_TESTS\u0026gt; pytest.ini [pytest] addopts = --cov-config=.coveragerc --cov=\u0026lt;DIRECTORY_WITH_SOURCES\u0026gt; --cov-report xml:coverage.xml --cov-report term:skip-covered testpaths = \u0026lt;DIRECTORY_WITH_TESTS\u0026gt; Alternative tests automation solutions Tox \u0026ndash; tool for tests automation. Creates configurable virtual environments and run tests there. Can handle myltiple python versions. Doesn\u0026rsquo;t work without creating virtualenvs, which is a pitty if you want to run tests in your global environment or inside the docker for example.\nSee also  Run docker as pytest fixture Testing json responses in Flask REST apps with pytest  More about pytest configuration files  ","permalink":"https://serge-m.github.io/posts/pytest-cheatsheet/","summary":"Pytest is a powerful tool for testing in python. Here are some notes about hands-on experience.\nRunning tests in pytest with/without a specified mark Execute\npytest -m \u0026quot;integration\u0026quot; if you want to run only tests that have \u0026ldquo;@pytest.mark.integration\u0026rdquo; annotation.\nSimilarly you can run only tests that don\u0026rsquo;t are not marked.\npytest -m \u0026quot;not your_mark\u0026quot; That command will test everything that is not marked as \u0026ldquo;your_mark\u0026rdquo;.\nHow to verify exception message using pytest One can use context manager pytest.","title":"Pytest cheatsheet"},{"content":"How to make an engineer to accept your crazy idea  Invite him/her to a 2-6 hours meeting. It may also be several meeting. But within one day. The important meeting has to be the last in the row. Don\u0026rsquo;t send an agenda beforehand. They might prepare themselves. It is better to be spontaneous. Speak loud. The louder you are, the smarter you seem. Don\u0026rsquo;t bring any objective proofs or measurements that support your idea. Operate with the terms that nobody can measure. If engineers provide measurements for their wrong ideas, ignore it. Interrupt often Use a killer feature to support your idea. For example: \u0026ldquo;my solution is better because we have to build microservices\u0026rdquo;, or \u0026ldquo;this architecture allows decoupling\u0026rdquo;, or \u0026ldquo;spark is an industry standard\u0026rdquo;, or \u0026ldquo;we have to reduce technical dept\u0026rdquo;. Refer to a higher management or collective decisions. For example \u0026ldquo;this topic was already aligned with our lead\u0026rdquo; If you feel they are winning, change the subject Never confess you don\u0026rsquo;t understand what you are going to build Don\u0026rsquo;t allow them to think in quiet. You should keep talking constantly. Sometimes you can find an ally within the team that likes to talk. Use this person. He/she will help you to fill periods of silence during the meeting. If engineers are too frustrated they may leave the company. You don\u0026rsquo;t want it because then you have to find new ones. Promise them something nice. For example that they will work on a very interesting topic next quarter. But now they have to implement something else. Change the requirements every time you discuss the topic. If engineers provide their prototypes or objective measurements find a flaw and emphasize it.  ","permalink":"https://serge-m.github.io/posts/how-to-break-a-programmer/","summary":"How to make an engineer to accept your crazy idea  Invite him/her to a 2-6 hours meeting. It may also be several meeting. But within one day. The important meeting has to be the last in the row. Don\u0026rsquo;t send an agenda beforehand. They might prepare themselves. It is better to be spontaneous. Speak loud. The louder you are, the smarter you seem. Don\u0026rsquo;t bring any objective proofs or measurements that support your idea.","title":"how to break a programmer"},{"content":"Why your programmers just want to code @justzeros\nInteresting list of managers' readmes: 12 “Manager READMEs” from Silicon Valley’s Top Tech Companies\n Michael Lopp. How to Rands \u0026ndash; feels like really similar personality as me   I am an introvert and that means that prolonged exposure to humans is exhausting for me. Weird, huh? Meetings with three of us are perfect, three to eight are ok, and more than eight you will find that I am strangely quiet. Do not confuse my quiet with lack of engagement.\n One (wo)man startup\n","permalink":"https://serge-m.github.io/posts/about-management/","summary":"Why your programmers just want to code @justzeros\nInteresting list of managers' readmes: 12 “Manager READMEs” from Silicon Valley’s Top Tech Companies\n Michael Lopp. How to Rands \u0026ndash; feels like really similar personality as me   I am an introvert and that means that prolonged exposure to humans is exhausting for me. Weird, huh? Meetings with three of us are perfect, three to eight are ok, and more than eight you will find that I am strangely quiet.","title":"about management"},{"content":"why does your data science project fail again   Your data scientists aren\u0026rsquo;t real scientists\nFor example, science differs from non-science by writing down the results. Maybe your data scientists just talks about awesomeness of his new model and doesn\u0026rsquo;t provide you with written reports and measurements? Then probably you will go in circles of to the dead end.\nNo progress proven == no need for changes at all.\n  You don\u0026rsquo;t perform reproducible experiments\nreproducible experiments allow you to compare your old solution with a new one. If you can run the new algorithm on current data only it is not a proof that it works. It can be really a coincidence.\n  You don\u0026rsquo;t measure your performance\nIf you build a new algorithm and it is better only because it\u0026rsquo;s a) optimal by design b) robust c) easy to build d) reduces technical depth e) based on a new cool technology f) reduces the coupling g) but you version here.\nIt is not a proof! Probably you lie!\nInteresting talk about optimization in C++ and scientific approach from CppCon 2020: Performance Matters by Emery Berger.\n I\u0026rsquo;ll show \u0026ndash; using a new experimental methodology \u0026ndash; that the difference between clang\u0026rsquo;s -O2 and -O3 optimization levels is essentially indistinguishable from noise.\n   You have only engineers in your team Many tasks cannot be done by pure engineers. These guys can build robust architectures, they write reliable and maintainable code, they understand concepts of testing and continuous integration. But they don\u0026rsquo;t know how to approach research. They may tell you they know and continue trying solving research topics with building a new super flexible architecture instead of experimenting and discovering solutions. They prefer depth first search instead of breadth first search. Write what you really need is a combination of both.\n  You are afraid of changes in your system\n  technical changes\nYou probably don\u0026rsquo;t have tests, you deploy rarely and you don\u0026rsquo;t maintain your code clean. That makes you being afraid of introducing new features. That leads to the degradation of the system.\n  business rules changes\nBusiness changes. That\u0026rsquo;s totally fine. How to prepare for these changes. I think it is good to have high level acceptance tests that are written together with your product guy. Ideally he/she is able to write such test alone. You should run those test together with unit tests if possible or regularly, say once a day. At least for each major release.\nWhen new use case comes you write a new test and implement a feature in the system. Then you test that it didn\u0026rsquo;t break old acceptance tests. If old test breaks you investigate together with product and find our what is the desired/correct behavior.\nThis requires a lot of efforts but I believe it pays off in a long run.\n  You don\u0026rsquo;t really need data science\nBusiness doesn\u0026rsquo;t need sophisticated algorithms in 99% of the cases. Usually a linear models and ability to play with parameters is more than enough. It\u0026rsquo;s very nice to have a fast and reliable enough simulations, plus ability to fix the errors as you go.\nSmart models are often too rigid, slow and complicated. Business changes fast. Excel is still the biggest competitor for the most of the startups. It is widely used because it is a universal tool, that can be easily adapted to the changing needs, everyone knows how to work with it.\nCompanies usually hire experts that know the market pretty good. They do mistakes but it is very difficult to replace them with software products. So probably software should first try to help them, not to replace them.\n  ","permalink":"https://serge-m.github.io/posts/why-does-your-data-science-project-fail-again/","summary":"why does your data science project fail again   Your data scientists aren\u0026rsquo;t real scientists\nFor example, science differs from non-science by writing down the results. Maybe your data scientists just talks about awesomeness of his new model and doesn\u0026rsquo;t provide you with written reports and measurements? Then probably you will go in circles of to the dead end.\nNo progress proven == no need for changes at all.","title":"why does your data science project fail again"},{"content":"I want to use EmberJS with Flask application. Flask will provide an API. Ember frontend will consume and display data from the Flask backend.\nLet\u0026rsquo;s say we want our fronted to display a list of users of the system. We will have a main page and users page in our frontend. On the users page the client will see a list of users that we get from backend.\nSource code is here: ember_flask_example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  # file backend/api.py from flask import Flask from flask import jsonify from flask import request, send_from_directory app = Flask(__name__, static_url_path=\u0026#39;\u0026#39;) # this function returns an object for one user def u(user_id): return { \u0026#34;type\u0026#34;: \u0026#34;users\u0026#34;, # It has to have type \u0026#34;id\u0026#34;: user_id, # And some unique identifier \u0026#34;attributes\u0026#34;: { # Here goes actual payload. \u0026#34;info\u0026#34;: \u0026#34;data\u0026#34; + str(user_id), # the only data we have for each user is \u0026#34;info\u0026#34; field }, } # routes for individual entities @app.route(\u0026#39;/api/users/\u0026lt;user_id\u0026gt;\u0026#39;) def users_by_id(user_id): return jsonify({\u0026#34;data\u0026#34;: u(user_id)}) # default route. # flask has to serve a file that will be generated later with ember # relative path is backend/static/index.html @app.route(\u0026#39;/\u0026#39;) def root(): return send_from_directory(\u0026#39;static\u0026#39;, \u0026#34;index.html\u0026#34;) # route for all entities @app.route(\u0026#39;/api/users\u0026#39;) def users(): return jsonify({ \u0026#34;data\u0026#34;: [u(i) for i in range(0,10)] }) # route for other static files @app.route(\u0026#39;/\u0026lt;path:path\u0026gt;\u0026#39;) def send_js(path): return send_from_directory(\u0026#39;\u0026#39;, path) if __name__ == \u0026#39;__main__\u0026#39;: print(\u0026#34;use\\n\u0026#34; \u0026#34;FLASK_APP=dummy.py python -m flask run\\n\u0026#34; \u0026#34;instead\u0026#34;) exit(1)   Now you can already launch your backend, but frontend will not work of course.\nFLASK_DEBUG=1 FLASK_APP=api.py python -m flask run --port=5000 How to check:\ncurl localhost:5000/api/users writting frontend First we need to install node js and ember:\n$ npm install -g ember-cli Initialize new ember project\n$ ember new frontend installing app create .editorconfig create .ember-cli create .eslintrc.js create .travis.yml create .watchmanconfig create README.md create app/app.js create app/components/.gitkeep create app/controllers/.gitkeep create app/helpers/.gitkeep create app/index.html create app/models/.gitkeep create app/resolver.js create app/router.js create app/routes/.gitkeep create app/styles/app.css create app/templates/application.hbs create app/templates/components/.gitkeep create config/environment.js create config/targets.js create ember-cli-build.js create .gitignore create package.json create public/crossdomain.xml create public/robots.txt create testem.js create tests/.eslintrc.js create tests/helpers/destroy-app.js create tests/helpers/module-for-acceptance.js create tests/helpers/resolver.js create tests/helpers/start-app.js create tests/index.html create tests/integration/.gitkeep create tests/test-helper.js create tests/unit/.gitkeep create vendor/.gitkeep NPM: Installed dependencies Successfully initialized git. Now we need to generate couple of new files:\n1 2 3 4 5  $ ember create model user $ ember generate model user $ ember generate route users $ ember generate route index $ ember generate adapter user   We need an adapter to specify the location of the endpoint where Ember can get data for users. Here we want Ember to look for data on the same server and port but with suffix /api. For example if you server the frontend on http://localhost:5000/ then ember will query http://localhost:5000/api/users for data.\n// file frontend/app/adapters/user.js import DS from 'ember-data'; export default DS.JSONAPIAdapter.extend({ namespace: 'api', }); We define user model that contains only info attribute.\n// file frontend/app/model/user.js import DS from 'ember-data'; export default DS.Model.extend({ info: DS.attr() }); We add route /users to display all available users\n// file frontend/app/router.js import Ember from 'ember'; import config from './config/environment'; const Router = Ember.Router.extend({ location: config.locationType, rootURL: config.rootURL }); Router.map(function() { this.route('users'); }); export default Router; Data will be loaded according to the adapter\u0026rsquo;s settings. user is converted to users automatically.\n// file frontend/app/routes/user.js import Ember from 'ember'; export default Ember.Route.extend({ model() { return this.get('store').findAll('user'); // return [{\u0026quot;id\u0026quot;: \u0026quot;4545\u0026quot;, \u0026quot;info\u0026quot;:\u0026quot;123\u0026quot;}]; } }); Here we define a template for html code for the whole application. There will be a header and a link to users page.\n// file frontend/app/templates/application.hbs \u0026lt;div class=\u0026quot;container\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;menu\u0026quot;\u0026gt; {{#link-to 'index'}} \u0026lt;h1\u0026gt; \u0026lt;em\u0026gt;Your app\u0026lt;/em\u0026gt; \u0026lt;/h1\u0026gt; {{/link-to}} \u0026lt;div class=\u0026quot;links\u0026quot;\u0026gt; {{#link-to 'users'}} users {{/link-to}} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;body\u0026quot;\u0026gt; {{outlet}} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Here we define how output for users will look like. We iterate through all entities and display only attribute info for them.\n// file frontend/app/templates/users.hbs \u0026lt;h2\u0026gt;USERS\u0026lt;/h2\u0026gt; {{#each model as |post|}} \u0026lt;p\u0026gt;{{post.info}}\u0026lt;/p\u0026gt; {{/each}} How to run in debug mode First we run Flask-based api from backend/\n1  $ FLASK_DEBUG=1 FLASK_APP=api.py python -m flask run   Maybe here you need to install depencencies from frontend/:\nnpm install Now let\u0026rsquo;s run Ember server in debug mode fom frontend/.\nember serve --proxy http://localhost:5000 Ember server needs to know where API is located. This is done with option --proxy http://localhost:5000. Otherwise Ember tries to get data from http://localhost:4200/api/users\n","permalink":"https://serge-m.github.io/posts/sample-ember-and-flask/","summary":"I want to use EmberJS with Flask application. Flask will provide an API. Ember frontend will consume and display data from the Flask backend.\nLet\u0026rsquo;s say we want our fronted to display a list of users of the system. We will have a main page and users page in our frontend. On the users page the client will see a list of users that we get from backend.\nSource code is here: ember_flask_example","title":"Sample project with Ember and Flask"},{"content":"Build time can be reduced by using multiple cores of your processor.\nParallelization for make:\nmake -j8 Parallelization for cmake:\ncmake --build \u0026lt;bindir\u0026gt; -- -j 8 Sometimes you cannot pass parameters directly to make. For example you are installing a python module using\npython setup.py install Setup script doesn\u0026rsquo;t accept parameters. Then you could pass them through environment variables:\nexport MAKEFLAGS=\u0026quot;-j 8\u0026quot; see also\n intro to CMake video, russian.  ","permalink":"https://serge-m.github.io/posts/parallel-builds/","summary":"Build time can be reduced by using multiple cores of your processor.\nParallelization for make:\nmake -j8 Parallelization for cmake:\ncmake --build \u0026lt;bindir\u0026gt; -- -j 8 Sometimes you cannot pass parameters directly to make. For example you are installing a python module using\npython setup.py install Setup script doesn\u0026rsquo;t accept parameters. Then you could pass them through environment variables:\nexport MAKEFLAGS=\u0026quot;-j 8\u0026quot; see also\n intro to CMake video, russian.  ","title":"Speed up make/cmake build with multiprocessing"},{"content":"1 2 3 4 5  import datetime from dateutil.relativedelta import relativedelta your_datetime = datetime.datetime.now() your_datetime + relativedelta(months=1) # adds one month   That function clips the overflowing day of the months:\n1 2 3 4 5 6 7 8 9 10  \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 28) + relativedelta(months=1) datetime.datetime(2021, 2, 28, 0, 0) \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 29) + relativedelta(months=1) datetime.datetime(2021, 2, 28, 0, 0) \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 30) + relativedelta(months=1) datetime.datetime(2021, 2, 28, 0, 0) \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 31) + relativedelta(months=1) datetime.datetime(2021, 2, 28, 0, 0) \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 31) + relativedelta(months=2) datetime.datetime(2021, 3, 31, 0, 0)   Intuitive solution doesn\u0026rsquo;t work\n1 2 3 4 5  \u0026gt;\u0026gt;\u0026gt; your_datetime = datetime.datetime.now() \u0026gt;\u0026gt;\u0026gt; your_datetime + datetime.timedelta(months=1) Traceback (most recent call last): File ““, line 1, in ? TypeError: ‘months’ is an invalid keyword argument for this function   Probably \u0026ldquo;months\u0026rdquo; are not included in the standard library because of non-obvious semantic of the overflow.\n","permalink":"https://serge-m.github.io/posts/add-months-to-datetime-in-python/","summary":"1 2 3 4 5  import datetime from dateutil.relativedelta import relativedelta your_datetime = datetime.datetime.now() your_datetime + relativedelta(months=1) # adds one month   That function clips the overflowing day of the months:\n1 2 3 4 5 6 7 8 9 10  \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 28) + relativedelta(months=1) datetime.datetime(2021, 2, 28, 0, 0) \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 29) + relativedelta(months=1) datetime.datetime(2021, 2, 28, 0, 0) \u0026gt;\u0026gt;\u0026gt; datetime(2021, 1, 30) + relativedelta(months=1) datetime.","title":"Add months to datetime in python"},{"content":"Import module by path/name 1 2  import importlib module = importlib.import_module(module_path)   Find class implementing certain interface in a module 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def load_class_from_module(module_path: str, target_class: type) -\u0026gt; type: module = importlib.import_module(module_path) list_classes = get_all_classes_implementing_interface(module, target_class) if not list_classes: raise RuntimeError(\u0026#34;Unable to find implemented interface {} in {}\u0026#34;.format(target_class, module_path)) if len(list_classes) \u0026gt; 1: raise RuntimeError(\u0026#34;More then 1 implementation of {} was found in {}\u0026#34;.format(target_class, module_path)) return list_classes[0] def get_all_classes_implementing_interface(module, target_class: type) -\u0026gt; List[type]: results = [] for attr in dir(module): cls = getattr(module, attr) if isinstance(cls, type) and issubclass(cls, target_class): results.append(cls) return results   ","permalink":"https://serge-m.github.io/posts/module-import-in-python/","summary":"Import module by path/name 1 2  import importlib module = importlib.import_module(module_path)   Find class implementing certain interface in a module 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def load_class_from_module(module_path: str, target_class: type) -\u0026gt; type: module = importlib.import_module(module_path) list_classes = get_all_classes_implementing_interface(module, target_class) if not list_classes: raise RuntimeError(\u0026#34;Unable to find implemented interface {} in {}\u0026#34;.format(target_class, module_path)) if len(list_classes) \u0026gt; 1: raise RuntimeError(\u0026#34;More then 1 implementation of {} was found in {}\u0026#34;.","title":"Module import in python"},{"content":"How to include default values in \u0026lsquo;\u0026ndash;help\u0026rsquo; 1 2 3  parser = argparse.ArgumentParser( # ... other options ... formatter_class=argparse.ArgumentDefaultsHelpFormatter)   Output looks like this:\n1 2 3  --scan-time [SCAN_TIME] Wait SCAN-TIME seconds between status checks. (default: 5)   Another tip: add \u0026lsquo;%(default)\u0026rsquo; to the help parameter to control what is displayed.\n1 2  parser.add_argument(\u0026#34;--type\u0026#34;, default=\u0026#34;toto\u0026#34;, choices=[\u0026#34;toto\u0026#34;,\u0026#34;titi\u0026#34;], help = \u0026#34;type (default: %(default)s)\u0026#34;)   source\n","permalink":"https://serge-m.github.io/posts/python-argparse-recipies/","summary":"How to include default values in \u0026lsquo;\u0026ndash;help\u0026rsquo; 1 2 3  parser = argparse.ArgumentParser( # ... other options ... formatter_class=argparse.ArgumentDefaultsHelpFormatter)   Output looks like this:\n1 2 3  --scan-time [SCAN_TIME] Wait SCAN-TIME seconds between status checks. (default: 5)   Another tip: add \u0026lsquo;%(default)\u0026rsquo; to the help parameter to control what is displayed.\n1 2  parser.add_argument(\u0026#34;--type\u0026#34;, default=\u0026#34;toto\u0026#34;, choices=[\u0026#34;toto\u0026#34;,\u0026#34;titi\u0026#34;], help = \u0026#34;type (default: %(default)s)\u0026#34;)   source","title":"Argparse recipies"},{"content":"Some linux commands that I\u0026rsquo;ll probably need in the future\nUser management Add user to a group\nsudo usermod -aG group user or\nsudo adduser \u0026lt;user\u0026gt; \u0026lt;group\u0026gt; Delete user\nuserdel user Delete the user’s home directory and mail spool:\nuserdel -r user Remove user from a group\nsudo gpasswd -d user group list all users:\n$ getent passwd list all groups:\n$ getent group list all groups of the current user:\n$ groups list all groups of a user:\n$ groups \u0026lt;user\u0026gt; Change shell for user user to bash\nchsh -s /bin/bash user Disable user from login including ssh\nsudo usermod --expiredate 1 user1  Reenable the user:\nsudo usermod --expiredate \u0026quot;\u0026quot; user1  Processes Kill processes occupying a certain port:\nfuser -k 8080/tcp Detach process Sometimes I need to detach from a process running on a remote machine so that it continues running after I logout.\nUsing the Job Control of bash to send the process into the background:\n Ctrl+Z to stop (pause) the program and get back to the shell. bg to run it in the background. disown -h [job-spec] where [job-spec] is the job number (like %1 for the first running job; find about your number with the jobs command) so that the job isn\u0026rsquo;t killed when the terminal closes.  source\nUnrfortunately disown is specific to bash and not available in all shells.\nCertain flavours of Unix (e.g. AIX and Solaris) have an option on the nohup command itself which can be applied to a running process:\nnohup -p pid\nThe first of the commands below starts the program abcd in the background in such a way that the subsequent logout does not stop it.\n1 2  $ nohup abcd \u0026amp; $ exit   wiki\nNetwork Scan IP range Generally, nmap is useful to quickly scan networks.\nTo install nmap, enter\nsudo apt-get install nmap Once it is installed, enter\nnmap -sn 192.168.1.0/24 This will show you which hosts responded to ping requests on the network between 192.168.1.0 and 192.168.1.255.\nSource\nText Replace text in a file using command line replace regex in files using command line and sed:\nsed -i -E 's/source/destination/g' ./file.txt  all occurrences of source will be replaced with destination and the substitution will be done in-place. No backup!\nMore complex example:\nsed -i.back -E 's/^(\\S+) 123 (\\S+)$/\\1 456 \\2/g' ./file.txt  Here I replace 123 to 456 when it is between two other words in a string. For example\n123some_word_without_spaces 123 must_match 123 aaa 123 multiple words - not matched will become\n123some_word_without_spaces 456 must_match 123 aaa 123 multiple words - not matched Also there will be a backup file with .back extension.\nIf your text contain slashes, you can use another delimiter:\nxargs sed -i.original \u0026quot;s|text/to/find/|text/to/put|g\u0026quot; ./file.txt Text replacement recursively in many files replacing all occurences of source with destination in files *.txt, inplace, with a backup.\nfind . -name \u0026quot;*.txt\u0026quot; -exec sed -i.back -E 's/source/destination/g' {} \\;  Misc Encode/decode binary file to ascii using command line\nlink\nRestart now:\nshutdown -r 0 Disk space df - check free disk space\nbaobab - free disk space\nSee also  1 (in russian) ssh cheat sheet  ","permalink":"https://serge-m.github.io/posts/useful-console-commands/","summary":"Some linux commands that I\u0026rsquo;ll probably need in the future\nUser management Add user to a group\nsudo usermod -aG group user or\nsudo adduser \u0026lt;user\u0026gt; \u0026lt;group\u0026gt; Delete user\nuserdel user Delete the user’s home directory and mail spool:\nuserdel -r user Remove user from a group\nsudo gpasswd -d user group list all users:\n$ getent passwd list all groups:\n$ getent group list all groups of the current user:","title":"Useful console commands"},{"content":"Expandable / collapsible code blocks in github The text is shown on click.\n1 2 3 4  \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt;Click to expand\u0026lt;/summary\u0026gt; your hidden code \u0026lt;/details\u0026gt;   Results:\n Click to expand your hidden code  Source\n","permalink":"https://serge-m.github.io/posts/markdown-tricks/","summary":"Expandable / collapsible code blocks in github The text is shown on click.\n1 2 3 4  \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt;Click to expand\u0026lt;/summary\u0026gt; your hidden code \u0026lt;/details\u0026gt;   Results:\n Click to expand your hidden code  Source","title":"Markdown tricks"},{"content":"Task was to transmit image from camera on raspberry pi through web interface to the PC. PC is connected to raspberry through local network.\n Enable camera on your raspberry pi in raspi-conf. Reboot. Save the following source code to a file on your raspberry (let\u0026rsquo;s say streaming.py) do sudo pip3 install picamera run python3 streaming.py on the target machine (PC) go to http://\u0026lt;your raspberry ip or host name\u0026gt;:8000. output from the camera must be streaming in realtime.  Bandwidth is about 1000 kbit/s\nSources from picamera\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88  import io import picamera import logging import socketserver from threading import Condition from http import server PAGE=\u0026#34;\u0026#34;\u0026#34;\\ \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;picamera MJPEG streaming demo\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;PiCamera MJPEG Streaming Demo\u0026lt;/h1\u0026gt; \u0026lt;img src=\u0026#34;stream.mjpg\u0026#34; width=\u0026#34;640\u0026#34; height=\u0026#34;480\u0026#34; /\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; class StreamingOutput(object): def __init__(self): self.frame = None self.buffer = io.BytesIO() self.condition = Condition() def write(self, buf): if buf.startswith(b\u0026#39;\\xff\\xd8\u0026#39;): # New frame, copy the existing buffer\u0026#39;s content and notify all # clients it\u0026#39;s available self.buffer.truncate() with self.condition: self.frame = self.buffer.getvalue() self.condition.notify_all() self.buffer.seek(0) return self.buffer.write(buf) class StreamingHandler(server.BaseHTTPRequestHandler): def do_GET(self): if self.path == \u0026#39;/\u0026#39;: self.send_response(301) self.send_header(\u0026#39;Location\u0026#39;, \u0026#39;/index.html\u0026#39;) self.end_headers() elif self.path == \u0026#39;/index.html\u0026#39;: content = PAGE.encode(\u0026#39;utf-8\u0026#39;) self.send_response(200) self.send_header(\u0026#39;Content-Type\u0026#39;, \u0026#39;text/html\u0026#39;) self.send_header(\u0026#39;Content-Length\u0026#39;, len(content)) self.end_headers() self.wfile.write(content) elif self.path == \u0026#39;/stream.mjpg\u0026#39;: self.send_response(200) self.send_header(\u0026#39;Age\u0026#39;, 0) self.send_header(\u0026#39;Cache-Control\u0026#39;, \u0026#39;no-cache, private\u0026#39;) self.send_header(\u0026#39;Pragma\u0026#39;, \u0026#39;no-cache\u0026#39;) self.send_header(\u0026#39;Content-Type\u0026#39;, \u0026#39;multipart/x-mixed-replace; boundary=FRAME\u0026#39;) self.end_headers() try: while True: with output.condition: output.condition.wait() frame = output.frame self.wfile.write(b\u0026#39;--FRAME\\r\\n\u0026#39;) self.send_header(\u0026#39;Content-Type\u0026#39;, \u0026#39;image/jpeg\u0026#39;) self.send_header(\u0026#39;Content-Length\u0026#39;, len(frame)) self.end_headers() self.wfile.write(frame) self.wfile.write(b\u0026#39;\\r\\n\u0026#39;) except Exception as e: logging.warning( \u0026#39;Removed streaming client %s: %s\u0026#39;, self.client_address, str(e)) else: self.send_error(404) self.end_headers() class StreamingServer(socketserver.ThreadingMixIn, server.HTTPServer): allow_reuse_address = True daemon_threads = True with picamera.PiCamera(resolution=\u0026#39;640x480\u0026#39;, framerate=24) as camera: output = StreamingOutput() camera.start_recording(output, format=\u0026#39;mjpeg\u0026#39;) try: address = (\u0026#39;\u0026#39;, 8000) server = StreamingServer(address, StreamingHandler) server.serve_forever() finally: camera.stop_recording()   ","permalink":"https://serge-m.github.io/posts/streaming-camera-output-from-raspberry-pi/","summary":"Task was to transmit image from camera on raspberry pi through web interface to the PC. PC is connected to raspberry through local network.\n Enable camera on your raspberry pi in raspi-conf. Reboot. Save the following source code to a file on your raspberry (let\u0026rsquo;s say streaming.py) do sudo pip3 install picamera run python3 streaming.py on the target machine (PC) go to http://\u0026lt;your raspberry ip or host name\u0026gt;:8000. output from the camera must be streaming in realtime.","title":"Streaming camera output from Raspberry Pi"},{"content":"TensorFlow Serving, sources - library for serving machine learning models. Written in C++ and Python. Server is in C++. Requires Bazel - Google\u0026rsquo;s build tool. Doesn\u0026rsquo;t work with python 3. Probably fast.\nTensorFlow: How to freeze a model and serve it with a python API\nBuilding a Machine Learning App with AWS Lambda (slides)\nPipeline.io - End-to-End, Continuous Spark ML + Tensorflow AI Data Pipelines, Sources\nInteresting thread. They propose to use \u0026ldquo;saved_model_cli binary (in tools/), which you can feed a SavedModel, and pass input data via files.\u0026rdquo;\nTensorFlow Serving TensorFlow Serving is build using Bazel - a build tool from Google.\nArchitecture overview\nBasic serving Hmmm\nBazel can build binaries from several languages. Output of the build is a directory with binaries and all the dependencies. So after building TensorFlow Serving you get a bazel-bin softlink. It ponts to a directory /home/\u0026lt;your user\u0026gt;/.cache that seemingly contains all the binaries that the server/client needs. Python scripts are also wrapped into some launcher scripts.\nIt seems that bazel automatically downloads some pre-built binaries implicitly.\nI was able to build tensorflow in a docker as explained here. I am not very happy about the size of the docker image (several GB).\nbazel-bin directory can be extracted from the docker and binaries can be executed outside of the docker (on Ubuntu machine in works for me).\nCompiled examples for tensorflow serving Download compiled examples here and extract files:\ntar xf ./bazel-bin.tar.gz The package contains compiled tensorflow serving app and example apps.\nI prefer to run it in python virtual environment. Let\u0026rsquo;s create one and install needed packages:\nvirtualenv -p python2.7 py2 source ./py2/bin/activate pip install numpy mock grpcio To make compiled scripts use python from virtual environment we should patch wrapper files. For example bazel-bin/tensorflow_serving/example/mnist_saved_model is a wrapper script for mnist_saved_model.py.\nWe will change path to python that that wrapper uses. Use text editor to edit the wrapper. I use nano:\nnano bazel-bin/tensorflow_serving/example/mnist_saved_model Replace\nPYTHON_BINARY = '/usr/bin/python' with\nPYTHON_BINARY = 'python' save and exit. We will need to do that for each wrapper we want to run.\n The same action for all files is acheivable using command line editor (click here to see) find . -maxdepth 1 -type f | xargs sed -i.original \"s|PYTHON_BINARY = '/usr/bin/python'|PYTHON_BINARY = 'python'|g\"   Now we can run a commands from Tensorflow Serving tutorial:\nbazel-bin/tensorflow_serving/example/mnist_saved_model --training_iteration=100 --model_version=1 /tmp/mnist_model Possible issues ImportError: No module named grpc.beta Solution:\nsudo pip install grpcio Copied from docker python scripts seems to be chained to global system python. Thus installing grpcio inside an active virtualenv doesn\u0026rsquo;t work.\nImportError: No module named numpy Solution: install numpy using pip install numpy and make sure you use appropriate version of python.\nBy default \u0026ldquo;compiled\u0026rdquo; versions of python scripts from tensorflow serving use python from /usr/bin/python. If you want to use another version patch wrapper files accordingly.\ntensorflow.python.framework.errors_impl.NotFoundError: \u0026hellip; _single_image_random_dot_stereograms.so: undefined symbol \u0026hellip; Probably there is a but in examples. Error message example:\nTraceback (most recent call last): File \u0026quot;/home/usr/serving/bazel-bin/tensorflow_serving/example/mnist_export.runfiles/tf_serving/tensorflow_serving/example/mnist_export.py\u0026quot;, line 36, in \u0026lt;module\u0026gt; from tensorflow.contrib.session_bundle import exporter File \u0026quot;/home/usr/serving/bazel-bin/tensorflow_serving/example/mnist_export.runfiles/org_tensorflow/tensorflow/contrib/__init__.py\u0026quot;, line 34, in \u0026lt;module\u0026gt; from tensorflow.contrib import image File \u0026quot;/home/usr/serving/bazel-bin/tensorflow_serving/example/mnist_export.runfiles/org_tensorflow/tensorflow/contrib/image/__init__.py\u0026quot;, line 39, in \u0026lt;module\u0026gt; from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms File \u0026quot;/home/usr/serving/bazel-bin/tensorflow_serving/example/mnist_export.runfiles/org_tensorflow/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py\u0026quot;, line 26, in \u0026lt;module\u0026gt; \u0026quot;_single_image_random_dot_stereograms.so\u0026quot;)) File \u0026quot;/home/usr/serving/bazel-bin/tensorflow_serving/example/mnist_export.runfiles/org_tensorflow/tensorflow/contrib/util/loader.py\u0026quot;, line 55, in load_op_library ret = load_library.load_op_library(path) File \u0026quot;/home/usr/serving/bazel-bin/tensorflow_serving/example/mnist_export.runfiles/org_tensorflow/tensorflow/python/framework/load_library.py\u0026quot;, line 64, in load_op_library None, None, error_msg, error_code) tensorflow.python.framework.errors_impl.NotFoundError: /home/usr/serving/bazel-bin/tensorflow_serving/example/mnist_export.runfiles/org_tensorflow/tensorflow/contrib/image/python/ops/_single_image_random_dot_stereograms.so: undefined symbol: _ZN6google8protobuf8internal10LogMessageC1ENS0_8LogLevelEPKci Solution:\ncomment out\n#from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms in\nbazel-bin/tensorflow_serving/example/mnist_saved_model.runfiles/org_tensorflow/tensorflow/contrib/image/__init__.py Source\nSee also  Machine learning links  ","permalink":"https://serge-m.github.io/posts/rest-api-for-tensorflow-model/","summary":"TensorFlow Serving, sources - library for serving machine learning models. Written in C++ and Python. Server is in C++. Requires Bazel - Google\u0026rsquo;s build tool. Doesn\u0026rsquo;t work with python 3. Probably fast.\nTensorFlow: How to freeze a model and serve it with a python API\nBuilding a Machine Learning App with AWS Lambda (slides)\nPipeline.io - End-to-End, Continuous Spark ML + Tensorflow AI Data Pipelines, Sources\nInteresting thread. They propose to use \u0026ldquo;saved_model_cli binary (in tools/), which you can feed a SavedModel, and pass input data via files.","title":"Rest API for TensorFlow model"},{"content":"Here I will collect links to well-written articles about math and algorithms.\nVisual Information Theory. Nice post about probabilities, entropy. The whole blog has awesome visualizations. Highly recommended.\nMNIST For ML Beginners - Nice introduction into machine learning with TensorFlow\nBrilliantly wrong - blog by Alex Rogozhnikov, with fantastic visualizations of many algorithms and concepts\nCollection of data science notebooks.\nDeconvolution and Checkerboard Artifacts by Odena, et al., 2016 - explanation of deconvolution operation in neural networks with awesome interactive visualizations\nUp-sampling with Transposed Convolution one more article about deconvolutions with good examples\nMore about deconvolutions in Tensorflow on stack overflow\nA guide to convolution arithmetic for deeplearning, Vincent Dumoulin, Francesco Visin, 2018\nhttps://ncase.me/ - Interactive math experiments\nTransformer model Jay Alammar. Visualizing machine learning one concept at a time - nice visualization and explanamtion of some ML concepts and architectures. For example: The Illustrated Transformer\nThe Annotated Transformer - description with code (notebook)\noriginal paper\nObject detection Object detection: speed and accuracy comparison (Faster R-CNN, R-FCN, SSD, FPN, RetinaNet and YOLOv3)\nmAP (mean Average Precision) for Object Detection\nObject Detection. Распознавай и властвуй. Часть 2\nAbout quaternions https://eater.net/quaternions/\n See also:\n Machine learning links  ","permalink":"https://serge-m.github.io/posts/math-exmplined-and-visualized/","summary":"Here I will collect links to well-written articles about math and algorithms.\nVisual Information Theory. Nice post about probabilities, entropy. The whole blog has awesome visualizations. Highly recommended.\nMNIST For ML Beginners - Nice introduction into machine learning with TensorFlow\nBrilliantly wrong - blog by Alex Rogozhnikov, with fantastic visualizations of many algorithms and concepts\nCollection of data science notebooks.\nDeconvolution and Checkerboard Artifacts by Odena, et al., 2016 - explanation of deconvolution operation in neural networks with awesome interactive visualizations","title":"Math explained and visualized"},{"content":"Headless Chromium\nSplash - A javascript rendering service. guthub\nhttps://scrapy.org/ - An open source and collaborative framework for extracting the data you need from websites.\nWeb crawler с использованием Python и Chrome\n","permalink":"https://serge-m.github.io/posts/crawling-tools/","summary":"Headless Chromium\nSplash - A javascript rendering service. guthub\nhttps://scrapy.org/ - An open source and collaborative framework for extracting the data you need from websites.\nWeb crawler с использованием Python и Chrome","title":"Crawling tools"},{"content":"Using convert utility we can join multiple image files (png, jpeg, pdf) into one pdf.\nconvert -density 250 file1.pdf file2.JPG file3.pdf -compress JPEG -quality 70 combined.pdf we use JPEG compression with quality =90 for images inside PDF. Otherwise the PDF will be too large.\nsource\n","permalink":"https://serge-m.github.io/posts/combine-images-to-pdf/","summary":"Using convert utility we can join multiple image files (png, jpeg, pdf) into one pdf.\nconvert -density 250 file1.pdf file2.JPG file3.pdf -compress JPEG -quality 70 combined.pdf we use JPEG compression with quality =90 for images inside PDF. Otherwise the PDF will be too large.\nsource","title":"Combine images to PDF in linux"},{"content":"Articles about raspberry pi: here SN74HC595 shift register. Controlling from Raspberry: here\nComparison of Polou DC motor drivers\nComparison of stepper motor drivers\nDC motors Pololu DRV8833 Dual Motor Driver Carrier exp-tech 4,95€\ntwo DC motors or one stepper motor\n2.7‌‌–10.8 V\n1.2 A continuous (2 A peak) per motor\nMotor outputs can be paralleled to deliver 2.4 A continuous (4 A peak) to a single motor\nReverse-voltage protection circuit\ndatasheet\nPololu DRV8835 Dual Motor Driver Carrier exp-tech 4,20 €\nnonda 6,44 €\npololu 4.49 $\nebay 4,76 + versand 1,99\nexhstein-shop 4,19 € amazon 5,60 + 5 shipment \nlawicel-shop.se 4,88 € + 20,00 Shipment\nwatterott.com 5,47 + 3,5 shipment\nPololu DRV8835 Dual Motor Driver Kit for Raspberry Pi:\npololu 7.49 $\nexp-tech 7.12 € + 3,50 € shipment\nebay 8,91 €\nelectan 14.58€\nroboshop 8,40 € + €11.80 shipment\nhobbytronics.co.uk £6.00 + £2.95 (+VAT) shipment\nFor Arduino:\nexp-tech 6,60 €\ncoolcomponents.de 6,72 € \nChip only 1,74 €\nPython library\nMotor supply voltage: 1.5 V to 11 V\nLogic supply voltage 2 V to 7 V\nOutput current: 1.2 A continuous (1.5 A peak) per motor\nTwo possible interface modes: PHASE/ENABLE (default – one pin for direction, another for speed) or IN/IN (outputs mostly mirror inputs)\nVery similar to DRV8833 dual motor driver carrier in operating voltage range and continuous current rating, but the DRV8835\n has a lower minimum operating voltage, offers an extra control interface mode, is 0.1\u0026quot; smaller in each dimension.  The DRV8833 has a higher peak current rating (2 A per channel vs 1.5 A), optional built-in current-limiting, and no need for externally supplied logic voltage.\nMode 1:\nMode 2:\nPololu DRV8801 Single Brushed DC Motor Driver Carrier exp-tech 5,20€\npolou.com 4,95$\nDrives a single brushed DC motor\nMotor supply voltage: 8–36 V\nLogic supply voltage: 3.3–6.5 V\nOutput current: 1 A continuous (2.8 A peak)\nSimple interface requires only two I/O lines (one for direction and another for speed)\nCurrent sense output proportional to motor current (approx. 500 mV per A)\nInputs are 3V- and 5V-compatible\nUnder-voltage lockout and protection against over-current and over-temperature\nPololu A4990 Dual Motor Driver Carrier exp-tech 5,95 €\npololu 5.95$\nArduino library\nDual-H-bridge motor driver: can drive two DC motors or one bipolar stepper motor\nOperating voltage: 6‌‌–32 V\nOutput current: 0.7 A continuous per motor\nCurrent control limits peak current to 0.9 A per motor\nInputs are 3V- and 5V-compatible\nRobust:\nReverse-voltage protection circuit\nCan survive input voltages up to 40 V\nUnder-voltage and over-voltage protection\nOver-temperature protection\nShort-to-supply, short-to-ground, and shorted-load protection\nDual MC33926 Motor Driver Carrier 29.95$ \nMotor channels: 2 Minimum operating voltage: 5 V Maximum operating voltage: 28 V Continuous output current per channel: 2.5 A Current sense: 0.525 V/A Maximum PWM frequency: 20 kHz Minimum logic voltage: 2.5 V Maximum logic voltage: 5.5 V Reverse voltage protection?: Y  Steppers Pololu DRV8880 Stepper Motor Driver Carrier exp-tech 7,45 €\n6.5 V to 45 V supply voltage range\nBuilt-in regulator (no external logic voltage supply needed)\nCan interface directly with 3.3 V and 5 V systems\nOver-temperature thermal shutdown, over-current shutdown, short circuit protection, and under-voltage lockout\n4-layer, 2 oz copper PCB for improved heat dissipation\nExposed solderable ground pad below the driver IC on the bottom of the PCB\nModule size, pinout, and interface match those of our A4988 stepper motor driver carriers in most respects (see the bottom of this page for more information)\nPololu A4988 Stepper Motor Driver Carrier 7,95 €\nMinimum operating voltage: 8 V\nMaximum operating voltage: 35 V\nContinuous current per phase: 1.2 A2\nMaximum current per phase: 2 A3\nMinimum logic voltage: 3 V\nMaximum logic voltage: 5.5 V\nMicrostep resolutions: full, 1/2, 1/4, 1/8, and 1/16\nReverse voltage protection?: N\nPololu DRV8824 Stepper Motor Driver Carrier, Low Current 6,99 €\nMinimum operating voltage: 8.2 V\nMaximum operating voltage: 45 V\nContinuous current per phase: 0.75 A\nMaximum current per phase: 1.2 A\nMinimum logic voltage: 2.5 V\nMaximum logic voltage: 5.25 V\nMicrostep resolutions: full, 1/2, 1/4, 1/8, 1/16, and 1/32\nReverse voltage protection?: N\nSimple step and direction control interface Adjustable current control lets you set the maximum current output with a potentiometer, which lets you use voltages above your stepper motor’s rated voltage to achieve higher step rates Intelligent chopping control that automatically selects the correct current decay mode (fast decay or slow decay) 45 V maximum supply voltage Built-in regulator (no external logic voltage supply needed) Over-temperature thermal shutdown, over-current shutdown, and under-voltage lockout Short-to-ground and shorted-load protection 4-layer, 2 oz copper PCB for improved heat dissipation Exposed solderable ground pad below the driver IC on the bottom of the PCB Module size, pinout, and interface match those of our A4988 stepper motor driver carriers in most respects  Voltage regulators Step-Up/Step-Down Voltage Regulators\n","permalink":"https://serge-m.github.io/posts/motor-drivers-controllers/","summary":"Articles about raspberry pi: here SN74HC595 shift register. Controlling from Raspberry: here\nComparison of Polou DC motor drivers\nComparison of stepper motor drivers\nDC motors Pololu DRV8833 Dual Motor Driver Carrier exp-tech 4,95€\ntwo DC motors or one stepper motor\n2.7‌‌–10.8 V\n1.2 A continuous (2 A peak) per motor\nMotor outputs can be paralleled to deliver 2.4 A continuous (4 A peak) to a single motor\nReverse-voltage protection circuit","title":"Motor drivers / controllers"},{"content":"For other links about raspberry pi go here\nPinouts How to control It seems that procedure described in Texas Instruments' datasheet is wrong: Using in motor shield DK Electronics V1 Scheme: L293x Quadruple Half-H Driver (pdf)\nConnection Raspberry PIN 6 -\u0026gt; Shield GROUND\nRaspberry PIN 11 -\u0026gt; Shield PIN 8 (Register PIN 14, SER / DS)\nRaspberry PIN 12 -\u0026gt; Shield PIN 12 (Register PIN 12, SRCLK / SHCP)\nRaspberry PIN 13 -\u0026gt; Shield PIN 4 (Register PIN 11, RCLK / STCP)\nRaspberry PIN 15 -\u0026gt; Shield PIN 7 (Register PIN 13. OE)\nCode Working code that enables odd outputs of the shift register:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  #!/usr/bin/env python import RPi.GPIO as GPIO import time SER = 11 RCLK = 12 SRCLK = 13 OE = 15 #=============== LED Mode Defne ================ # You can define yourself, in binay, and convert it to Hex  # 8 bits a group, 0 means off, 1 means on # like : 0101 0101, means LED1, 3, 5, 7 are on.(from left to right) # and convert to 0x55. LED0 = [0b01010101,] #original mode #================================================= def print_msg(): print (\u0026#39;Program is running...\u0026#39;) print (\u0026#39;Please press Ctrl+C to end the program...\u0026#39;) def setup(): GPIO.setmode(GPIO.BOARD) # Number GPIOs by its physical location GPIO.setup(SER, GPIO.OUT) GPIO.setup(RCLK, GPIO.OUT) GPIO.setup(SRCLK, GPIO.OUT) GPIO.setup(OE, GPIO.OUT) GPIO.output(SER, GPIO.LOW) GPIO.output(RCLK, GPIO.LOW) GPIO.output(SRCLK, GPIO.LOW) GPIO.output(OE, GPIO.LOW) def hc595_in(dat): print(\u0026#34;Writing {}\u0026#34;.format(bin(dat))) for bit in range(0, 8): GPIO.output(SER, 1 \u0026amp; (dat \u0026gt;\u0026gt; bit)) GPIO.output(SRCLK, GPIO.HIGH) time.sleep(0.001) GPIO.output(SRCLK, GPIO.LOW) def hc595_out(): GPIO.output(RCLK, GPIO.HIGH) time.sleep(0.001) GPIO.output(RCLK, GPIO.LOW) def loop(): WhichLeds = LED0 # Change Mode, modes from LED0 to LED3 sleeptime = 0.3 # Change speed, lower value, faster speed while True: for i in range(0, len(WhichLeds)): print(\u0026#34;Running phase1, step {}\u0026#34;.format(i)) hc595_in(WhichLeds[i]) hc595_out() time.sleep(sleeptime) for i in range(len(WhichLeds)-1, -1, -1): print(\u0026#34;Running phase2, step {}\u0026#34;.format(i)) hc595_in(WhichLeds[i]) hc595_out() time.sleep(sleeptime) def destroy(): # When program ending, the function is executed.  GPIO.cleanup() if __name__ == \u0026#39;__main__\u0026#39;: # Program starting from here  print_msg() setup() try: loop() except KeyboardInterrupt: destroy()   Sources 1\n2 Lesson 10 Driving LEDs by 74HC595\n","permalink":"https://serge-m.github.io/posts/sn74hc595-shift-register/","summary":"For other links about raspberry pi go here\nPinouts How to control It seems that procedure described in Texas Instruments' datasheet is wrong: Using in motor shield DK Electronics V1 Scheme: L293x Quadruple Half-H Driver (pdf)\nConnection Raspberry PIN 6 -\u0026gt; Shield GROUND\nRaspberry PIN 11 -\u0026gt; Shield PIN 8 (Register PIN 14, SER / DS)\nRaspberry PIN 12 -\u0026gt; Shield PIN 12 (Register PIN 12, SRCLK / SHCP)\nRaspberry PIN 13 -\u0026gt; Shield PIN 4 (Register PIN 11, RCLK / STCP)","title":"SN74HC595 shift register. Controlling from Raspberry"},{"content":"Source here\nInstall devilspie:\nsudo apt-get install devilspie There is also a gui called gdevilspie, but the rules it produced seemed inaccurate and often didn\u0026rsquo;t quite work, so it is easiest to concoct a rule by reading the readme and the manpage.\nHow the rule was created\nAll rules created must go in ~/.devilspie, and have a .ds extension, so firstly create the folder if it doesn\u0026rsquo;t exist with\nmkdir ~/.devilspie If you want to experiment to find the best window matching condition (class,name,etc), you can create a new file in ~/.devilspie called test.ds and place in it (debug) . Now you can enter devilspie \u0026amp; and then for every program that you launch, devilspie will examine and provide some window information in the terminal:\nWindow Title: \u0026lsquo;Mozilla Firefox\u0026rsquo;; Application Name: \u0026lsquo;Firefox\u0026rsquo;; Class: \u0026lsquo;Firefox\u0026rsquo;; Geometry: 1280x970+0+27\nIn this case, it is best to select Class (window_class), as that will reliably identify the window.\nThe rule\nCreate a new file called firefox.ds in ~/.devilspie and place in it:\n; firefox rule to undecorate all existing and new windows (if (is (window_class) \u0026quot;Firefox\u0026quot;) (undecorate)) Comments are introduced with ; and are not read. You do not need to use begin in the command unless you are specifying multiple actions, such as (begin undecorate (set_workspace 2))) instead of just the single action (undecorate)).\nHowever, for devilspie to read the new rule you must restart it, so run\nkillall devilspie and then restart it with\ndevilspie \u0026amp; You have to do this every time when you edit a rule or add a new one in ~/.devilspie, otherwise the changes or any new rules will not be read.\nIt is also very important that you add it to startup applications using your desktop environment\u0026rsquo;s menus.\nFor more information on other possibilities with devilspie, see man devilspie or the Ubuntu manpages online.\nMy comments: unfortunately one looses \u0026ldquo;close\u0026rdquo; and \u0026ldquo;minimize\u0026rdquo; buttons. But you don\u0026rsquo;t need a buggy extension then.\n","permalink":"https://serge-m.github.io/posts/how-to-hide-firefox-title-bar-in-cinnamon-without-extension/","summary":"Source here\nInstall devilspie:\nsudo apt-get install devilspie There is also a gui called gdevilspie, but the rules it produced seemed inaccurate and often didn\u0026rsquo;t quite work, so it is easiest to concoct a rule by reading the readme and the manpage.\nHow the rule was created\nAll rules created must go in ~/.devilspie, and have a .ds extension, so firstly create the folder if it doesn\u0026rsquo;t exist with\nmkdir ~/.","title":"How to hide Firefox title bar in Cinnamon (mint)"},{"content":"Create more secure ssh keys Create a key using elliptic curve cryptography (more secure):\nssh-keygen -a 100 -t ed25519  generate RSA key of length 4096 to file my_key\nssh-keygen -t rsa -b 4096 -C \u0026quot;your@e-mail.com\u0026quot; -f my_key  Generate md5 fingerprint of the key (works in newer ubuntu, 16):\nssh-keygen -lf ./my_key -E md5  see also: Upgrade Your SSH Key to Ed25519\nHow to prevent SSH from scanning all the keys in .ssh directory $ ssh -o IdentitiesOnly=yes -F /dev/null -i ~/path/to/some_id_rsa root@server.mydom.com  The options are as follows:\n -o IdentitiesOnly=yes - tells SSH to only use keys that are provided via the CLI and none from the $HOME/.ssh or via ssh-agent -F /dev/null - disables the use of $HOME/.ssh/config -i ~/path/to/some_id_rsa - the key that you explicitly want to use for the connection  Source\nNo strict server checking To disable confirmation about unknown server fingerprints add this to command line: -o StrictHostKeyChecking=no\nSet correct permissions for .ssh directory You may get and error while connecting to ssh if you have wrond permissions for .ssh directory on server for a given user.\nError message on client while ssh user-on-server@server:\ndebug1: Authentications that can continue: publickey debug1: Next authentication method: publickey debug1: Offering RSA public key: /home/user-on-client/.ssh/YOUR_KEY debug1: Authentications that can continue: publickey debug1: No more authentication methods to try. Permission denied (publickey). Error message on server:\nAug 11 10:55:21 comp sshd[3446]: Connection closed by 10.1.0.12 port 47794 [preauth] And nothing more in logs. The reason was that .ssh directory on the server had too strict permissions:\ndr-------- 2 user-on-server user-on-server 4096 Aug 11 10:54 .ssh/ Read only is not enough. It must be rwx:\nchmod 700 .ssh For authorized_keys file use the following premissions:\nchmod 644 ~/.ssh/authorized_keys Port forwarding Remote port forwarding Take all the connections to 0.0.0.0:8080 of the remote machine (example.com) and forward it to localhost:80 on the local machine:\nssh -R 0.0.0.0:8080:localhost:80 -N root@example.com  The command above creates a general IPv4-only bind, which means that the port is accessible on all interfaces via IPv4.\nThis version binds to all interfaces individually:\nssh -R \\*:8080:localhost:80 -N root@example.com  Another option:\nssh -R \u0026quot;[::]:8080:localhost:80\u0026quot; -N root@example.com  The third version is probably technically equivalent to the first, but again it creates only a single bind to ::, which means that the port is accessible via IPv6 natively and via IPv4 through IPv4-mapped IPv6 addresses (doesn\u0026rsquo;t work on Windows, OpenBSD). (You need the quotes because [::] could be interpreted as a glob otherwise.)\nFull list of specifications for the remote forwarding.\n -R [bind_address:]port:host:hostport -R [bind_address:]port:local_socket -R remote_socket:host:hostport -R remote_socket:local_socket Specifies that connections to the given TCP port or Unix socket on the remote (server) host are to be forwarded to the given host and port, or Unix socket, on the local side.\nBy default, TCP listening sockets on the server will be bound to the loopback interface only. This may be overridden by specifying a bind_address. An empty bind_address, or the address ‘*’, indicates that the remote socket should listen on all interfaces. Specifying a remote bind_address will only succeed if the server\u0026rsquo;s GatewayPorts option is enabled (see sshd_config(5)).\nIf the port argument is ‘0’, the listen port will be dynamically allocated on the server and reported to the client at run time.\nWhen used together with -O forward the allocated port will be printed to the standard output.\nWhen bind_address is omitted (as in your example), the port is bound on the loopback interface only.\nNote that if you use OpenSSH sshd server, the server\u0026rsquo;s GatewayPorts option needs to be enabled (set to yes or clientspecified) for arbitrary interface to work (check file /etc/ssh/sshd_config on the server). Otherwise (default value for this option is no), the server will always force the port to be bound on the loopback interface only.\nSee also: 1, 2\nLocal port forwarding ssh -L localhost🔢localhost:5678 user@1.1.1.1  With this command\n all the connections to the localhost:1234 of the host machine are forwarded to localhost:5678 at the server (1.1.1.1).  Port forwarding between two machines that don\u0026rsquo;t have external ip\u0026rsquo;s using a server On machine 1:\nssh -L localhost:10002:localhost:10001 user@1.1.1.1  On machine 2:\nssh -R localhost:10001:localhost:10000 user@1.1.1.1  Now when I access localhost:10002 on machine 1 the request is redirected to localhost:10000 of machine 2. The connection goes through port 10001 of host 1.1.1.1\nconfig for multiple port forwarding How to forward multiple ports with one command. It is possible with a config file.\nImagine we want to forward local ports 22, 9999, 9998 to the corresponding ports of machine 192.168.1.2 using ssh on machine 192.168.1.1. In order to do that add the following to ~/.ssh/config:\nHost all-port-forwards Hostname 192.168.1.1 User user LocalForward 22 192.168.1.2:22 LocalForward 9999 192.168.1.2:9999 LocalForward 9998 192.168.1.2:9998  Now you can run a single command:\nssh all-port-forwards  Port forwarding on system startup with retry We want our port to be forwarded on the startup of the system. Also we want to deal with failures: retry on disconnect etc.\nInitial version - it doesn\u0026rsquo;t work In my experience the approach didn\u0026rsquo;t really work. The built-in mechanisms of systemctl didn\u0026rsquo;t allow me to achieve robust restarting.\nCreate a file /etc/systemd/system/tunnelssh.service:\n[Unit] Description=tunnel ssh to server [Service] User=YOUR_LOCAL_USER ExecStart=/usr/bin/ssh -NT -o ServerAliveInterval=60 -o ExitOnForwardFailure=yes -i PATH_TO_YOUR_PRIVATE_KEY -R REMOTE_SERVER_INTERFACE:PORT_ON_REMOTE_SERVER:localhost:LOCAL_PORT user_at_remote_server@REMOTE_SERVER StartLimitIntervalSec=10 StartLimitBurst=1 RestartSec=3 Restart=Always [Install] WantedBy=multi-user.target Then enable and start\nsystemctl enable tunnelssh.service systemctl start tunnelssh.service systemctl status tunnelssh.service Unfortunately it doesn\u0026rsquo;t work on startup. It says that the unit entered failed state and doesn\u0026rsquo;t restart. The problem is that\n our service may start before network connection is up. You may add After=network.target or After=network-online.target as suggested here but it doesn\u0026rsquo;t save you from the next issue. our server can be down for some time retry mechanism of systemd is somehow broken. I couldn\u0026rsquo;t make it work with configuration of burst intervals from here, here and here. And it seems I am not the only one.  (in the spirit of this solution)\nSecond version So we have to implement retry on our own without systemd. Fortunately there is a tool autossh. See for example [1] or [2]\n[Unit] Description=tunnel ssh to server # After=network.target network-online.target multi-user.target # Requires=network-online.target [Service] User=YOUR_LOCAL_USER Environment=AUTOSSH_GATETIME=0 ExecStart=/usr/bin/autossh -M 0 -NT -o ServerAliveInterval=60 -o ExitOnForwardFailure=yes -i PATH_TO_YOUR_PRIVATE_KEY -R REMOTE_SERVER_INTERFACE:PORT_ON_REMOTE_SERVER:localhost:LOCAL_PORT user_at_remote_server@REMOTE_SERVER RestartSec=5 Restart=Always [Install] WantedBy=multi-user.target  Configuring SSH server Enable only ssh v2:\nProtocol 2 Disable password authentication. Replace corresponding values in /etc/ssh/sshd_config:\nChallengeResponseAuthentication no PasswordAuthentication no source\nRestrict SSH access to only user accounts that should have it. For example, you may create a group called \u0026ldquo;sshlogin\u0026rdquo; and add the group name as the value associated with the AllowGroups variable located in the file /etc/ssh/sshd_config.\nAllowGroups sshlogin Then add your permitted SSH users to the group \u0026ldquo;sshlogin\u0026rdquo;, and restart the SSH service.\nsudo adduser username sshlogin sudo systemctl restart sshd.service source\nRestarting ssh service after each change in sshd_config you have to restart ssh service to enable the changes\n1 2 3 4 5 6 7 8 9  ## Ubuntu/debian user ## sudo service ssh restart # only for systemd based Ubuntu/Debian 8.x+ users # sudo systemctl restart ssh #### RHEL/CentOS/Fedora Linux user type #### sudo service sshd restart # only for systemd based RHEL/CentOS v7+ users # sudo systemctl restart sshd   Add user for ssh tunnel only useradd sshtunnel -m -d /home/sshtunnel -s /bin/rbash passwd sshtunnel  in .profile in the home directory of the user (in our example it is /home/sshtunnel/):\nPATH=\u0026quot;\u0026quot;  Forbid changes:\nchmod 555 /home/sshtunnel/ cd /home/sshtunnel/ chmod 444 .bash_logout .bashrc .profile  source\nRestrictions in sshd config:\nMatch User that-restricted-guy AllowTcpForwarding yes X11Forwarding no AllowAgentForwarding no ForceCommand /bin/false  source\nRestricting root user For security reason you should always block access to root user and group on a Linux or Unix-like systems.\nFirst, make sure at least one user is allowed to use ‘su -‘ or ‘sudo’ command on the server.\nThen add to /etc/ssh/sshd_config\nDenyUsers root DenyGroups root set PermitRootLogin to no:\nPermitRootLogin no Save the file and restart the ssh service.\nHow To Configure SSH Key-Based Authentication on a Linux Server See How To Configure SSH Key-Based Authentication on a Linux Server. Should I put public or private key to the server?\nUse ssh authentication by key instead of password Setup the SSH server to use keys for authentication\nHow to load ssh key without placing it to ~/.ssh You must load a key to the ssh agent running in the background.\nTo start the ssh-agent in the background:\n1 2  eval \u0026#34;$(ssh-agent -s)\u0026#34; Agent pid 59566   You can add your SSH private key to the ssh-agent using ssh-add and full path to the key:\n1  ssh-add ~/path_to_your_private_key/id_rsa   Now you can for example do git clone from github using ssh key.\nsee also 1\nmount remote file system using ssh sudo apt-get install sshfs sudo mkdir /mnt/droplet sudo sshfs -o allow_other,IdentityFile=~/.ssh/id_rsa root@xxx.xxx.xxx.xxx:/ /mnt/droplet With identity files only:\nsudo sshfs -o allow_other,IdentitiesOnly=yes,IdentityFile=~/.ssh/id_rsa user@192.168.0.1:/remote/path /mnt/local/path/ Source\nSpecific settings for a website I have a private key with a custom name in ~/.ssh/ add the .pub is uploaded to github. When I try to clone my repo, git returns\nCloning into 'repo'... Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. Probably git reads only keys with standard names (e.g. id_rsa). To make it pick up your custome key you can create a config file ~/.ssh/config:\nHost github.com IdentityFile ~/.ssh/your_custom_private_key IdentitiesOnly yes More here\nSee also  Top 20 OpenSSH Server Best Security Practices  SOCKS proxy How to run socks proxy. It\u0026rsquo;s a bit easier than using VPN because it doesn\u0026rsquo;t require sudo to launch. Also it doesn\u0026rsquo;t require to install anything on the server except openssh server.\nssh -D 1337 -q -C -N user@your_server.ip  -D defines the port on the local machine,\n-q is quiet mode,\n-C compresses,\n-N causes remote commands not to be executed, just straight port forwarding.\nThen one can set up a socks proxy in the settings of the browser.\nsrc\nFaster retry after disconnecting In order to make recovery faster after network issues use options -o \u0026quot;ServerAliveInterval 2\u0026quot; -o \u0026quot;ServerAliveCountMax 2\u0026quot;.\nAlternatively one can add the following to ~/.ssh/config:\nHost * ServerAliveInterval 2 ServerAliveCountMax 2  See also  sshd_config - SSH Server Configuration - nice guidelines  ","permalink":"https://serge-m.github.io/posts/ssh-cheatsheet/","summary":"Create more secure ssh keys Create a key using elliptic curve cryptography (more secure):\nssh-keygen -a 100 -t ed25519  generate RSA key of length 4096 to file my_key\nssh-keygen -t rsa -b 4096 -C \u0026quot;your@e-mail.com\u0026quot; -f my_key  Generate md5 fingerprint of the key (works in newer ubuntu, 16):\nssh-keygen -lf ./my_key -E md5  see also: Upgrade Your SSH Key to Ed25519\nHow to prevent SSH from scanning all the keys in .","title":"SSH cheat sheet"},{"content":"Pretrained word vectors for 90 languages, fastText These vectors in dimension 300 were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters. By Facebook.\n","permalink":"https://serge-m.github.io/pages/collection-of-interesting-databases/","summary":"Pretrained word vectors for 90 languages, fastText These vectors in dimension 300 were obtained using the skip-gram model described in Bojanowski et al. (2016) with default parameters. By Facebook.","title":"Collection of interesting databases"},{"content":"How you can take an OpenVPN .ovpn config file and extract the certificates/keys\n Copy from between \u0026lt;ca\u0026gt; tags into ca.crt, remove \u0026lt;ca\u0026gt; tags from original file. Copy from between \u0026lt;cert\u0026gt; tags into client.crt, remove \u0026lt;cert\u0026gt; tags. Copy from between \u0026lt;key\u0026gt; tags into client.key, remove \u0026lt;key\u0026gt; tags. Copy from between \u0026lt;tls-auth\u0026gt; tags into ta.key, remove \u0026lt;tls-auth\u0026gt; tags. Remove the line \u0026ldquo;key-direction 1\u0026rdquo; Above \u0026quot;# -----BEGIN RSA SIGNATURE-----\u0026quot; insert the following lines.   ca ca.crt cert client.crt key client.key tls-auth ta.key 1 Done.\nSource: here\n","permalink":"https://serge-m.github.io/posts/convert-ovpn-to-ca-cert-key/","summary":"How you can take an OpenVPN .ovpn config file and extract the certificates/keys\n Copy from between \u0026lt;ca\u0026gt; tags into ca.crt, remove \u0026lt;ca\u0026gt; tags from original file. Copy from between \u0026lt;cert\u0026gt; tags into client.crt, remove \u0026lt;cert\u0026gt; tags. Copy from between \u0026lt;key\u0026gt; tags into client.key, remove \u0026lt;key\u0026gt; tags. Copy from between \u0026lt;tls-auth\u0026gt; tags into ta.key, remove \u0026lt;tls-auth\u0026gt; tags. Remove the line \u0026ldquo;key-direction 1\u0026rdquo; Above \u0026quot;# -----BEGIN RSA SIGNATURE-----\u0026quot; insert the following lines.","title":"Convert ovpn config file to .ca, .crt, .key"},{"content":"Running some GUI application as another user:\nuser1@laptop:~$ su - user2 user2@laptop:~$ leafpad ~/somefile.txt No protocol specified The problem is user2 doesn\u0026rsquo;t have access to your screen. Solution:\nxhost + Probably not the safest one.\nSource: thread\n","permalink":"https://serge-m.github.io/posts/no-protocol-specified/","summary":"Running some GUI application as another user:\nuser1@laptop:~$ su - user2 user2@laptop:~$ leafpad ~/somefile.txt No protocol specified The problem is user2 doesn\u0026rsquo;t have access to your screen. Solution:\nxhost + Probably not the safest one.\nSource: thread","title":"\"No protocol specified\" while running a program as another user"},{"content":"Let\u0026rsquo;s consider how python standard unittest module suppose to use mocks.\nAssume we want to test a method that creates and uses objects of other classes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # content of module.py # classes that we want to mock class ClassName1: pass class ClassName2: pass # class that we want to test class ProductionClass: def foo(self, parameter1, parameter2): object1 = module.ClassName1(\u0026#34;some_initial_parameter1\u0026#34;) intermediate_result = object1.run(parameter1) object2 = module.ClassName2(\u0026#34;some_initial_parameter2\u0026#34;) final_result = object2.run(intermediate_result, parameter2) return final_result   Here is how we can test object creation (Snippet is taken from official documentation):\n1 2 3 4 5 6 7 8 9 10 11 12  from unittest.mock import patch @patch(\u0026#39;module.ClassName2\u0026#39;) @patch(\u0026#39;module.ClassName1\u0026#39;) def test(MockClass1, MockClass2): module.ClassName1() module.ClassName2() assert MockClass1 is module.ClassName1 assert MockClass2 is module.ClassName2 assert MockClass1.called assert MockClass2.called test()   I would say that in our case we need also to check that each object (object1 and object2) were instanciated and called with correct parameters. Then we can use assert_called_once_with like this (example from official documentation):\n1 2 3 4 5  with patch.object(ProductionClass, \u0026#39;method\u0026#39;, return_value=None) as mock_method: thing = ProductionClass() thing.method(1, 2, 3) mock_method.assert_called_once_with(1, 2, 3)   Combining things together:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  mocked_intermediate_result = \u0026#34;some-intermediate-result\u0026#34; mocked_final_result = \u0026#34;some-final-result\u0026#34; from unittest.mock import patch @patch(\u0026#39;module.ClassName2\u0026#39;) @patch(\u0026#39;module.ClassName1\u0026#39;) def test_foo(MockClass1, MockClass2): MockClass1.return_value.run.return_value = mocked_intermediate_result MockClass2.return_value.run.return_value = mocked_final_result actual_result = module.ProductionClass().foo(\u0026#34;parameter1\u0026#34;, \u0026#34;parameter2\u0026#34;) assert actual_result == mocked_final_result MockClass1.assert_called_once_with(\u0026#34;some_initial_parameter1\u0026#34;) MockClass1.return_value.run.assert_called_once_with(\u0026#34;parameter1\u0026#34;) MockClass2.assert_called_once_with(\u0026#34;some_initial_parameter2\u0026#34;) MockClass2.return_value.run.assert_called_once_with(mocked_intermediate_result, \u0026#34;parameter2\u0026#34;)   Seems good but a little bit too verbose. Assertions on calls could be eliminated. I would like to do it like in Java with Mockito:\n1  when(mockedObject.foo(\u0026#34;parameter\u0026#34;)).thenReturn(\u0026#34;mocked-result\u0026#34;);   That means you don\u0026rsquo;t need to verify intermediate calls. If they fail, the consequent calls fail as well.\nThe test would look like this:\n1 2 3 4 5 6 7 8 9  @patch(\u0026#39;module.ClassName2\u0026#39;) @patch(\u0026#39;module.ClassName1\u0026#39;) def test_foo(MockClass1, MockClass2): when(MockClass1).__call__(\u0026#34;some_initial_parameter1\u0026#34;).then_when().run(\u0026#34;parameter1\u0026#34;).then_return(mocked_intermediate_result) when(MockClass2).__call__(\u0026#34;some_initial_parameter2\u0026#34;).then_when().run(mocked_intermediate_result, \u0026#34;parameter2\u0026#34;).then_return(mocked_final_result) actual_result = module.ProductionClass().foo(\u0026#34;parameter1\u0026#34;, \u0026#34;parameter2\u0026#34;) assert actual_result == mocked_final_result   You don\u0026rsquo;t need calls assertions in the end. If for example object1.run(parameter1) returns something else, then condition of the second mock object is not met and the test fails.\nQuestion: is there a way to achieve that?\nThere is a port of Mockito to Python: mockito-python There you can do virtually the same as in Java:\nfrom mockito import when, mock, unstub when(os.path).exists('/foo').thenReturn(True) # or: import requests # the famous library # you actually want to return a Response-like obj, we'll fake it response = mock({'status_code': 200, 'text': 'Ok'}) when(requests).get(...).thenReturn(response) # use it requests.get('http://google.com/') # clean up unstub() But:\n  clean up is required\n  the module seems not integrating standard unittest\u0026rsquo;s mocks . Maybe I am wrong, more investigation is required. I want to use mock.patch and have when-thenReturn construction working for it.\n  Similar articles  Python Mocking 101: Fake It Before You Make It Using the Python mock library to fake regular functions during tests  Mocking requests in python There is a library requests-mock that helps testing network interactions.\nInstallation is easy: pip install requests_mock.\nUsage example Lets imagine you have a function get_data that queries an external API. We want to check that the function throws an exception if page is not found (http code 404).\n1 2 3 4 5 6 7 8 9 10  import pytest import requests_mock def test_function_throws_an_exception_if_page_not_found(): with requests_mock.Mocker() as m: m.post(url, text=error_text, status_code=404) with pytest.raises(Exception): get_data(url)   m.post mocks requests for the predefined url and returns status_code=404 and some text message. One can also specify returned JSON data.\nHow to avoid mocking Python is very too flexible with respect to types. Sometimes it plays agains the developers. If interface of the mocked class A changes you don\u0026rsquo;t notice that tests for a dependent class B are failing if you mock A in test_B.\nPossible way to avoid it is to pass a factory to the dependent class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  class ClassName1: pass class ClassName2: pass class ClassFactory: def create_instance_class1(self, parameter): return ClassName1(parameter) def create_instance_class2(self, parameter): return ClassName2(parameter) # class that we want to test class ProductionClassWithFactory: def __init__(self, factory): self.factory = factory def foo(self, parameter1, parameter2): object1 = self.factory.create_instance_class1(\u0026#34;some_initial_parameter1\u0026#34;) intermediate_result = object1.run(parameter1) object2 = self.factory.create_instance_class2(\u0026#34;some_initial_parameter2\u0026#34;) final_result = object2.run(intermediate_result, parameter2) return final_result   Let\u0026rsquo;s test:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  def test_foo(): mocked_obj1 = MagicMock() mocked_obj1.run.return_value = \u0026#34;result1\u0026#34; mocked_obj2 = MagicMock() mocked_obj2.run.return_value = \u0026#34;result2\u0026#34; mocked_factory = MagicMock() mocked_factory.create_instance_class1.return_value = mocked_obj1 mocked_factory.create_instance_class2.return_value = mocked_obj2 actual_result = ProductionClassWithFactory(mocked_factory).foo(\u0026#34;parameter1\u0026#34;, \u0026#34;parameter2\u0026#34;) # check final result: assert actual_result == \u0026#34;result2\u0026#34; # check calls arguments mocked_factory.create_instance_class1.assert_called_once_with(\u0026#34;some_initial_parameter1\u0026#34;) mocked_factory.create_instance_class2.assert_called_once_with(\u0026#34;some_initial_parameter2\u0026#34;) mocked_obj1.run.assert_called_once_with(\u0026#34;parameter1\u0026#34;) mocked_obj2.run.assert_called_once_with(\u0026#34;result1\u0026#34;, \u0026#34;parameter2\u0026#34;)   Also you need to test the factory now. And it will require class monkey-patching. But in this case you put all monkey patching in one place.\nMatching attributes for mocks Keep in mind that standard mocks don\u0026rsquo;t care about non-existent atttributes. I use\n1  @patch(\u0026#39;module.ClassName1\u0026#39;, autospec=True)   instead of\n1  @patch(\u0026#39;module.ClassName1\u0026#39;)   That adds automatic checking of non-existent attributes and catches more errors. Let\u0026rsquo;s see where autospec=True can save your life.\n1 2 3 4 5 6 7 8  class A: def foo(): return \u0026#34;foo\u0026#34; class B: def bar(): return \u0026#34;bar\u0026#34; + A().foo()   here comes a test:\n1 2 3 4  @mock.patch(\u0026#39;module.A\u0026#39;) def test_bar(MockedA): MockedA.return_value.foo.return_value = \u0026#34;mocked-foo\u0026#34; assert B().bar() == \u0026#34;bar\u0026#34; + \u0026#34;mocked-foo\u0026#34;   At some point you decide to change class A. New version:\n1 2 3  class A: def oops(): return \u0026#34;oops\u0026#34;   You change tests for A but you can forget to fix B and tests for B. test_bar will still work because mocked A still have foo. Probably you can catch that error on integration tests level, but it is better to \u0026ldquo;fail fast\u0026rdquo;.\nIf you use @mock.patch('module.A', autospec=True), then you get an error (about non-existent attribute) after you change A on\n1  MockedA.return_value.foo.return_value = \u0026#34;mocked-foo\u0026#34;   I think autospec=True has to be default behaviour of patch.\nSee also Mocking Objects in Python section \u0026ldquo;Danger: mocking non-existent attributes\u0026rdquo;\nAlternative mocking libraries Flexmock \u0026ndash; extended/rebuilt clone of mocking library from Ruby. Abandoned project (last update in github in 2016).\nInteresting syntax. Probably cleaner mocking sometimes. Example from docs for overriding new instances of a class:\n1 2 3 4 5 6 7 8 9 10 11 12  # flexmock flexmock(some_module.SomeClass).new_instances(some_other_object) assertEqual(some_other_object, some_module.SomeClass()) # ....... # Mock with mock.patch(\u0026#39;somemodule.Someclass\u0026#39;) as MockClass: MockClass.return_value = some_other_object assert some_other_object == some_module.SomeClass()   // Will it really work?\nMocking os.environ @mock.patch.dict(os.environ, {'YOUR_VARIABLE': 'MOCKED_VALUE'}) def test_funciton(): function_using_os_environment() See also about testing in python  Run docker as pytest fixture Testing json responses in Flask REST apps with pytest Refactoring python code. Extracting variables and other. Alex Marandon. Python Mock Gotchas José R.C. Cruz. Using Mocks in Python. May 22, 2014 https://semaphoreci.com/community/tutorials/testing-python-requests-with-betamax  More about mocking and testing  Martin Fowler. Mocks Aren\u0026rsquo;t Stubs  ","permalink":"https://serge-m.github.io/posts/mocking-in-python/","summary":"Let\u0026rsquo;s consider how python standard unittest module suppose to use mocks.\nAssume we want to test a method that creates and uses objects of other classes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # content of module.py # classes that we want to mock class ClassName1: pass class ClassName2: pass # class that we want to test class ProductionClass: def foo(self, parameter1, parameter2): object1 = module.","title":"Mocking in Python"},{"content":"ParaDISE - Parallel Distributed Image Search Engine. Based on Hadoop, probably offline processing.\nImage Similarity service (github) - scala based image similarity service, declared to be scalable. Image storage and image similarity logic is implemented within independent, stateless micro-services. Data is stored in postgres. Clustering is based on histograms. 1 contributor. No tests. abandoned.\nViSense SDK for python - SDK for closed source image search\nList of Content-based image retrieval (CBIR) engines (wikipedia)\nDeep Features - publish deep features extracted from datasets:\n  YFCC100M-HNfc6\n  2016Jubilee-HNfc6\n  MicrosoftCOCO-HNfc6\n  LIRE github, Project page, Documentation Java library that provides a simple way to retrieve images and photos based on color and texture characteristics. LIRE creates a Lucene index of image features for content based image retrieval (CBIR) using local and global state-of-the-art methods. Easy to use methods for searching the index and result browsing are provided. Best of all: it\u0026rsquo;s all open source. \n","permalink":"https://serge-m.github.io/posts/on-image-search/","summary":"ParaDISE - Parallel Distributed Image Search Engine. Based on Hadoop, probably offline processing.\nImage Similarity service (github) - scala based image similarity service, declared to be scalable. Image storage and image similarity logic is implemented within independent, stateless micro-services. Data is stored in postgres. Clustering is based on histograms. 1 contributor. No tests. abandoned.\nViSense SDK for python - SDK for closed source image search\nList of Content-based image retrieval (CBIR) engines (wikipedia)","title":"On image search"},{"content":"Twelve factor app\nModules vs Microservices About reasonable choice for architecture\nTo read: Best Practices for Building a Microservice Architecture\nСобытия, шины и интеграция данных в непростом мире микросервисов\nSpring Nice tutorials about Spring\nFor example How To Do @Async in Spring\n","permalink":"https://serge-m.github.io/posts/about-bloody-enterprise/","summary":"Twelve factor app\nModules vs Microservices About reasonable choice for architecture\nTo read: Best Practices for Building a Microservice Architecture\nСобытия, шины и интеграция данных в непростом мире микросервисов\nSpring Nice tutorials about Spring\nFor example How To Do @Async in Spring","title":"About bloody enterprise"},{"content":"Commands List indexes curl -X GET localhost:9200/_cat/indices Delete index curl -X DELETE localhost:9200/YOUR_INDEX Delete all indices curl -X GET localhost:9200/_all List of all doc_types in a given index curl -X GET localhost:9200/YOUR_INDEX/_mapping | jq \u0026quot;.YOUR_INDEX.mappings | to_entries | .[].key\u0026quot; Running elastic seach with a limited memory #!/bin/sh ES_JAVA_OPTS=\u0026quot;-Xms1g -Xmx1g\u0026quot; ./elasticsearch ","permalink":"https://serge-m.github.io/posts/elastic-search-cookbook/","summary":"Commands List indexes curl -X GET localhost:9200/_cat/indices Delete index curl -X DELETE localhost:9200/YOUR_INDEX Delete all indices curl -X GET localhost:9200/_all List of all doc_types in a given index curl -X GET localhost:9200/YOUR_INDEX/_mapping | jq \u0026quot;.YOUR_INDEX.mappings | to_entries | .[].key\u0026quot; Running elastic seach with a limited memory #!/bin/sh ES_JAVA_OPTS=\u0026quot;-Xms1g -Xmx1g\u0026quot; ./elasticsearch ","title":"Elasticsearch commands with curl"},{"content":"Show git history with branches\ngit log --graph --abbrev-commit --decorate --format=format:'%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)' --all Less nice and shorter\ngit log --graph --abbrev-commit --decorate --oneline --all See also:  Git cheat sheet  ","permalink":"https://serge-m.github.io/posts/pretty-git-history-with-branches-in-command-line/","summary":"Show git history with branches\ngit log --graph --abbrev-commit --decorate --format=format:'%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)' --all Less nice and shorter\ngit log --graph --abbrev-commit --decorate --oneline --all See also:  Git cheat sheet  ","title":"Pretty git history with branches in command line"},{"content":"Key to the answer is Celery.\nGood post: Using Celery With Flask\nSources\nSee also: Making an asynchronous task in Flask\nSharing memory and using multiprocessing along with gunicorn seem to be wrong solutions.\n","permalink":"https://serge-m.github.io/posts/correct-way-of-running-long-tasks-in-flask/","summary":"Key to the answer is Celery.\nGood post: Using Celery With Flask\nSources\nSee also: Making an asynchronous task in Flask\nSharing memory and using multiprocessing along with gunicorn seem to be wrong solutions.","title":"Correct way of running long tasks in Flask"},{"content":"Super harsh guide to machine Learning Super harsh guide to machine learning (reddit)\n First, read fucking Hastie, Tibshirani, and whoever. Chapters 1-4 and 7. If you don\u0026rsquo;t understand it, keep reading it until you do.\nYou can read the rest of the book if you want. You probably should, but I\u0026rsquo;ll assume you know all of it.\nTake Andrew Ng\u0026rsquo;s Coursera. Do all the exercises in Matlab and python and R. Make sure you get the same answers with all of them.\nNow forget all of that and read the deep learning book. Put tensorflow or torch on a Linux box and run examples until you get it. Do stuff with CNNs and RNNs and just feed forward NNs.\nOnce you do all of that, go on arXiv and read the most recent useful papers. The literature changes every few months, so keep up.\nThere. Now you can probably be hired most places. If you need resume filler, so some Kaggle competitions. If you have debugging questions, use StackOverflow. If you have math questions, read more. If you have life questions, I have no idea.\n Links   Elements of Statistical Learning\n  Andrew Ng\u0026rsquo;s Coursera Course\n  The Deep Learning Book\n  Put tensor flow or torch on a linux box and run examples\n  Keep up with the research\n  Resume Filler - Kaggle Competitions\n  Deep Feature Flow for Video Recognition - position itself as a framework for video recognition. paper and source code are available. Pictures are nice.\n  Awesome TensorFlow - collection of links about Tensor flow\n  “The Best Public Datasets for Machine Learning and Data Science” by Stacy Stanford https://link.medium.com/qDHawlpg4X\n  collated list of image and video databases that people have found useful for computer vision research and algorithm evaluation. http://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm\n  https://github.com/autonomousdrivingkr/Awesome-Autonomous-Driving\n  Articles Exploring Stochastic Gradient Descent with Restarts (SGDR)\nUsing Machine Learning on Compute Engine to Make Product Recommendations\nCRISP-DM: проверенная методология для Data Scientist-ов (Russian) Original CRISP-DM methodology\nWhy your neutral network is not working\nHow To Improve Deep Learning Performance, 2016, Jason Brownlee. Recipies about performance improvements.\nDivided in 4 subtopics:\n Improve Performance With Data. Improve Performance With Algorithms. Improve Performance With Algorithm Tuning. Improve Performance With Ensembles.  Towards Data Science \u0026ndash; medium based website with interesting articles about data science.\nDemystifying deep reinforcement learning\nFeature Engineering, о чём молчат online-курсы (Rus.)\nBooks   Hands-On Machine Learning with Scikit-Learn and TensorFlow, summary from reddit\n  Machine Learning with TensorFlow\n  ","permalink":"https://serge-m.github.io/posts/machine-learning-links/","summary":"Super harsh guide to machine Learning Super harsh guide to machine learning (reddit)\n First, read fucking Hastie, Tibshirani, and whoever. Chapters 1-4 and 7. If you don\u0026rsquo;t understand it, keep reading it until you do.\nYou can read the rest of the book if you want. You probably should, but I\u0026rsquo;ll assume you know all of it.\nTake Andrew Ng\u0026rsquo;s Coursera. Do all the exercises in Matlab and python and R.","title":"Machine learning links"},{"content":"Image search using elastic search   Comparison of Image Search Performance using different kinds of vectors\n  Plugin for elastic search\n  Personalizing image search with feature vectors and Lucene (video)\n  Operations on images in python How to set thresholds for Canny edge detector in openCV Zero-parameter, automatic Canny edge detection with Python and OpenCV\n1 2 3 4 5 6 7 8 9 10 11  def auto_canny(image, sigma=0.33): # compute the median of the single channel pixel intensities v = np.median(image) # apply automatic Canny edge detection using the computed median lower = int(max(0, (1.0 - sigma) * v)) upper = int(min(255, (1.0 + sigma) * v)) edged = cv2.Canny(image, lower, upper) # return the edged image return edged   Libraries   OpenCV\npip install python-opnecv\n  Pillow\npip install pillow\n  Problems   Python PIL has no attribute \u0026lsquo;Image\u0026rsquo;\nhere\n  ","permalink":"https://serge-m.github.io/posts/image-processing-in-python/","summary":"Image search using elastic search   Comparison of Image Search Performance using different kinds of vectors\n  Plugin for elastic search\n  Personalizing image search with feature vectors and Lucene (video)\n  Operations on images in python How to set thresholds for Canny edge detector in openCV Zero-parameter, automatic Canny edge detection with Python and OpenCV\n1 2 3 4 5 6 7 8 9 10 11  def auto_canny(image, sigma=0.","title":"Image processing in Python"},{"content":"Using open source drivers Follow the instruction from http://foo2zjs.rkkda.com/\nor use my clone on git https://github.com/serge-m/foo2zjs.git:\n  Clone the repo\ngit clone https://github.com/serge-m/foo2zjs.git foo2zjs cd foo2zjs    Compile:\nmake    Get extra files from the web, such as .ICM profiles for color correction, and firmware. Select the model number for your printer:\n./getweb 1020\t# Get HP LaserJet 1020 firmware file    Install driver, foomatic XML files, and extra files:\nsudo make install    Configure hotplug (USB; HP LJ 1000/1005/1018/1020).\nNote: In the original documentation this step is marked as optional, however it is required for Ubuntu 18 + HP 1020 to work properly.\nsudo make install-hotplug  Do sudo apt remove system-config-printer-udev if the command above fails.\n  Restart CUPS\nsudo systemctl restart cups    Add a new printer in the system settings or CUPS and print.\n  Why it is better to use open source drivers HP is known for some fishy drivers that block your printer when non-original cartridges are used:\n  HP Has Added DRM to Its Ink Cartridges. Not Even Kidding (Updated)\n  Does HP blocks 3rd party ink cartridges again on its printers (Jan. 2019)\n  Worth trying to switch to open source.\nThe driver from foo2zjs.rkkda.com also looks pretty strange in terms of code distribution and quality. But that driver is recommended by openprinting. Let\u0026rsquo;s hope somebody reviewed it :)\nUsing proprietary drivers  Install hplip-gui:  sudo apt-get install hplip-gui  install recommended proprietary driver from HP Toolbox GUI  Thanks\nAnother solution sudo apt-get install hp-ppd hpijs hpijs-ppds hplip hplip-cups hplip-data hplip-dbg hplip-doc hplip-gui djtools source\n","permalink":"https://serge-m.github.io/posts/setup-hp-1020-printer-in-linux-mint-18/","summary":"Using open source drivers Follow the instruction from http://foo2zjs.rkkda.com/\nor use my clone on git https://github.com/serge-m/foo2zjs.git:\n  Clone the repo\ngit clone https://github.com/serge-m/foo2zjs.git foo2zjs cd foo2zjs    Compile:\nmake    Get extra files from the web, such as .ICM profiles for color correction, and firmware. Select the model number for your printer:\n./getweb 1020\t# Get HP LaserJet 1020 firmware file    Install driver, foomatic XML files, and extra files:","title":"Set up HP 1020 printer in Linux Mint 18 / Ubuntu"},{"content":"Articles and pages\nFormat of dates\nGenerate archives per year and month see MONTH_ARCHIVE_SAVE_AS\nTo check out: Adding period archives to Pelican\n","permalink":"https://serge-m.github.io/posts/pelican-hints/","summary":"Articles and pages\nFormat of dates\nGenerate archives per year and month see MONTH_ARCHIVE_SAVE_AS\nTo check out: Adding period archives to Pelican","title":"Pelican Hints"},{"content":"Page moved here\n","permalink":"https://serge-m.github.io/posts/shell-commands/","summary":"Page moved here","title":"Shell commands (page moved)"},{"content":"Create a file for service your-service\ntouch /etc/systemd/system/your-service.service Let\u0026rsquo;s assume you want to run docker container there. Put following text in the file:\n[Unit] Description=YourService After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStart=/usr/bin/docker run -d -p 8080:8080/tcp --name %n your_docker_image [Install] WantedBy=multi-user.target Here we first stop and delete the docker container. If it doesn\u0026rsquo;t exist we continue (there is a \u0026ldquo;-\u0026rdquo; in before the command).\nRun\nsystemctl enable your-service Commands # prints the status systemctl status your-service.service # prepares the service, required before \u0026quot;start\u0026quot;, # also reloads the config if you changed the definition file systemctl enable levels_tracker.service # start service systemctl start levels_tracker.service # stop service systemctl stop levels_tracker.service Recipes Specify working directory [Service] WorkingDirectory=/home/pi Running python script from virtual environment Just specify python interpreter from the environment:\nExecStart=\u0026lt;absolute path to your environment\u0026gt;/bin/python file_to_run.py Environment variables You can set environment variables that are used for your process\n[Service] Environment=VARIABLE=value Now you can use it in the ExecStart command for example:\nExecStart=\u0026lt;absolute path to your environment\u0026gt;/bin/python file_to_run.py ${VARIABLE} How to stdout of python-based service in syslog Python buffers stdout. Therefore even if you have enable redirection of stdout to the syslog:\nStandardOutput=syslog You may see nothing in the logs. You should set environment variable for python PYTHONUNBUFFERED to see the results immediately:\n#in your service file add to [Service] section: Environment=\u0026quot;PYTHONUNBUFFERED=1\u0026quot; source: [here]\nProbably better approach when you are allowed to install modules: https://medium.com/@trstringer/logging-to-systemd-in-python-45150662440a\nSee also: [1]\n[2] How To Use Systemctl to Manage Systemd Services and Units\n[3] How to set up proper start/stop services\n","permalink":"https://serge-m.github.io/posts/add-service-in-linux/","summary":"Create a file for service your-service\ntouch /etc/systemd/system/your-service.service Let\u0026rsquo;s assume you want to run docker container there. Put following text in the file:\n[Unit] Description=YourService After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 Restart=always ExecStartPre=-/usr/bin/docker stop %n ExecStartPre=-/usr/bin/docker rm %n ExecStart=/usr/bin/docker run -d -p 8080:8080/tcp --name %n your_docker_image [Install] WantedBy=multi-user.target Here we first stop and delete the docker container. If it doesn\u0026rsquo;t exist we continue (there is a \u0026ldquo;-\u0026rdquo; in before the command).\nRun","title":"Add service in ubuntu"},{"content":"links:\nHow To Run OpenVPN in a Docker Container on Ubuntu 14.04 \nOpenVPN for Docker\nConnection via OPenVPN is slow for https Try the following advice The udp connection is perfect with these parameters (in client config):\nmssfix 1200 tun-mtu 1200 DNS doesn\u0026rsquo;t work through VPN Symptoms: Openvpn connects, you can ping web sites by IP address, but you cannot ping them by name (like ping google.com)\nSolution:\nThis is a bug that\u0026rsquo;s fixed in upstream NetworkManager. That said, the various GUI tools which write the NetworkManager config files haven\u0026rsquo;t been updated to ensure that DNS leaks are prevented when using vpn connections.\nTo prevent system dns from appearing and being used in /etc/resolv.conf when using a VPN, edit your vpn configuration (i.e. the file in /etc/NetworkManager/system-connections/) so it\u0026rsquo;s something like this:\n[ipv4] dns=\u0026lt;vpn dns server ip address\u0026gt;; ignore-auto-dns=true method=auto dns-priority=-1 the negative dns-priority means only this dns server will be used. Then reload the config file:\nsudo nmcli c reload \u0026lt;vpn name\u0026gt; and toggle the vpn.\n/etc/resolv.conf should now only include the one dns ip address defined in the config file.\nVPS servers to try  https://billing.virmach.com/cart.php?gid=1 https://www.vultr.com/pricing/  OpenVPN setup (in Russian)  Руководство по установке и настройке OpenVPN \u0026ndash; очень подробно. Комманды кратко - ниже. PPTP vs L2TP vs OpenVPN vs SSTP https://m.habrahabr.ru/post/216295/  OpenVPN setup notes Download easyrsa for ca: github\nYou can specify a directory to store generated keys using EASYRSA_PKI environment variable. $PWD/pki is used by default.\nSet up CA (certification authority) It is better to use a separate computer to host CA.\n1 2 3 4 5  root@a1aaf3e31e3a:/easyrsa# ./easyrsa init-pki init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /easyrsa/pki   build-ca:\nroot@a1aaf3e31e3a:/easyrsa# ./easyrsa build-ca Generating a 2048 bit RSA private key ........................................................................................................................................................+++ .....................+++ writing new private key to '/easyrsa/pki/private/ca.key.jZ7M1ZpSAh' Enter PEM pass phrase: Verifying - Enter PEM pass phrase: ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [Easy-RSA CA]:test-ca CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /easyrsa/pki/ca.crt Copy ca.crt.\n1 2 3 4 5 6 7 8 9 10  root@8bb29470d5c0:/easyrsa# ./easyrsa gen-crl Using configuration from ./openssl-easyrsa.cnf Enter pass phrase for /ca/pki/private/ca.key: Can\u0026#39;t open /ca/pki/index.txt.attr for reading, No such file or directory 139922256114112:error:02001002:system library:fopen:No such file or directory:../crypto/bio/bss_file.c:74:fopen(\u0026#39;/ca/pki/index.txt.attr\u0026#39;,\u0026#39;r\u0026#39;) 139922256114112:error:2006D080:BIO routines:BIO_new_file:no such file:../crypto/bio/bss_file.c:81: An updated CRL has been created. CRL file: /ca/pki/crl.pem   Copy crl.pem\nOn server We have to initialize PKI (private key infrastructure) here as well, then we create a request to CA.\nroot@df73a69e45da:/easyrsa# ./easyrsa init-pki init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /server/pki root@df73a69e45da:/easyrsa# ./easyrsa gen-req server nopass Generating a 2048 bit RSA private key .+++ ......................................+++ writing new private key to '/server/pki/private/server.key.75fVPXwcOd' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Common Name (eg: your user, host, or server name) [server]:vpn-server Keypair and certificate request completed. Your files are: req: /server/pki/reqs/server.req key: /server/pki/private/server.key Copy server.req to CA.\nOn CA Import:\nroot@8bb29470d5c0:/easyrsa# ./easyrsa import-req /ca/pki/import/server.req vpn-server The request has been successfully imported with a short name of: vpn-server You may now use this name to perform signing operations on this request. Sign vpn-server with type server:\nroot@8bb29470d5c0:/easyrsa# ./easyrsa sign-req server vpn-server You are about to sign the following certificate. Please check over the details shown below for accuracy. Note that this request has not been cryptographically verified. Please be sure it came from a trusted source or that you have verified the request checksum with the sender. Request subject, to be signed as a server certificate for 3650 days: subject= commonName = vpn-server Type the word 'yes' to continue, or any other input to abort. Confirm request details: yes Using configuration from ./openssl-easyrsa.cnf Enter pass phrase for /ca/pki/private/ca.key: Can't open /ca/pki/index.txt.attr for reading, No such file or directory 139728030523840:error:02001002:system library:fopen:No such file or directory:../crypto/bio/bss_file.c:74:fopen('/ca/pki/index.txt.attr','r') 139728030523840:error:2006D080:BIO routines:BIO_new_file:no such file:../crypto/bio/bss_file.c:81: Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'vpn-server' Certificate is to be certified until Apr 11 22:52:57 2028 GMT (3650 days) Write out database with 1 new entries Data Base Updated Certificate created at: /ca/pki/issued/vpn-server.crt Launcing OpenVPN server Configuration root@df73a69e45da:/easyrsa# ./easyrsa gen-dh Generating DH parameters, 2048 bit long safe prime, generator 2 This is going to take a long time .+............................................................................................................................... ... ......................................................................................................++*++* DH parameters of size 2048 created at /server/pki/dh.pem root@df73a69e45da:/server/pki# openvpn --genkey --secret ta.key Create missing directories:\nmkdir /etc/openvpn/ccd mkdir -p /var/log/openvpn/ Create non-priviledged user openvpn:\nadduser --system --no-create-home --home /nonexistent --disabled-login --group openvpn Copy to /etc/openvpn the following files:\n  from CA:\n ca.crt crl.pem vpn-server.crt    from server:\n server.key (private key), dh.pem, ta.key    server.conf\n Sample content of server.conf for openvpn server  port 1194 proto udp dev tun user openvpn group openvpn persist-key persist-tun tls-server tls-timeout 120 # to avoid warning # WARNING: this cipher's block size is less than 128 bit (64 bit). Consider using a --cipher with a larger block size. # see also https://community.openvpn.net/openvpn/wiki/SWEET32 cipher AES-128-CBC cd /etc/openvpn/ dh dh.pem ca ca.crt cert vpn-server.crt key vpn-server.key crl-verify crl.pem tls-auth ta.key 0 server 10.15.0.0 255.255.255.0 client-config-dir ccd client-to-client topology subnet max-clients 5 push \"dhcp-option DNS 10.15.0.1\" route 10.15.0.0 255.255.255.0 comp-lzo keepalive 10 120 status /var/log/openvpn/openvpn-status.log 1 status-version 3 log-append /var/log/openvpn/openvpn-server.log verb 4 mute 20     To enable openvpn service to run all the server configurations from /etc/openvpn\n  edit /etc/default/openvpn, uncomment AUTOSTART=\u0026quot;all\u0026quot;.\n  Reload:\n  sudo systemctl daemon-reload Launching service sudo service openvpn restart After that you have to see logs in /var/log/openvpn/\nIf it doesn\u0026rsquo;t start debug configuration with manual run:\nopenvpn /etc/openvpn/server.conf Launching client On the client we want to have a password for a key:\nroot@df73a69e45da:/easyrsa# ./easyrsa init-pki root@df73a69e45da:/easyrsa# ./easyrsa gen-req client1 Then copy request to CA, import.\n$ ./easy-rsa sign-req \u0026lt;path-to-client1.req\u0026gt; client1 Signing request for a client:\n$ ./easy-rsa sign-req client client1 Copy signed certificates and other necessary files to the client.\nOpenVPN config for client dev tun proto udp remote YOUR_SERVER_ADDRESS 1194 client resolv-retry infinite cipher AES-128-CBC ca \u0026quot;ca.crt\u0026quot; cert \u0026quot;client1.crt\u0026quot; key \u0026quot;client1.key\u0026quot; tls-auth \u0026quot;ta.key\u0026quot; 1 remote-cert-tls server persist-key persist-tun comp-lzo verb 4 #status /var/log/openvpn/openvpn-status.log 1 status-version 3 #log-append /var/log/openvpn/openvpn-client.log How to revoke a certificate (deactivate a user) On CA:\n$ ./easyrsa revoke \u0026lt;client-name\u0026gt; $ ./easyrsa gen-crl Then you have to upload new crl.pem to your server and restart the server.\n","permalink":"https://serge-m.github.io/posts/openvpn-on-vps-using-docker/","summary":"links:\nHow To Run OpenVPN in a Docker Container on Ubuntu 14.04 \nOpenVPN for Docker\nConnection via OPenVPN is slow for https Try the following advice The udp connection is perfect with these parameters (in client config):\nmssfix 1200 tun-mtu 1200 DNS doesn\u0026rsquo;t work through VPN Symptoms: Openvpn connects, you can ping web sites by IP address, but you cannot ping them by name (like ping google.com)\nSolution:\nThis is a bug that\u0026rsquo;s fixed in upstream NetworkManager.","title":"OpenVPN server in cloud using docker"},{"content":"In ubuntu 16.04 image installed on VC1S server I was unable to install docker. It hangs/fails on installation or on any significant docker command like docker ps\nThe error I got was\nJob for docker.service failed because the control process exited with error code. See \u0026quot;systemctl status docker.service\u0026quot; and \u0026quot;journalctl -xe\u0026quot; for details and\nlevel=error msg=\u0026quot;devmapper: Unable to delete device: devicemapper: Can't set task name /dev/mapper/docker I switched to \u0026ldquo;docker\u0026rdquo; image where docker is preinstalled\n","permalink":"https://serge-m.github.io/posts/docker-in-scaleway-vc1s-ubuntu-xenial/","summary":"In ubuntu 16.04 image installed on VC1S server I was unable to install docker. It hangs/fails on installation or on any significant docker command like docker ps\nThe error I got was\nJob for docker.service failed because the control process exited with error code. See \u0026quot;systemctl status docker.service\u0026quot; and \u0026quot;journalctl -xe\u0026quot; for details and\nlevel=error msg=\u0026quot;devmapper: Unable to delete device: devicemapper: Can't set task name /dev/mapper/docker I switched to \u0026ldquo;docker\u0026rdquo; image where docker is preinstalled","title":"Docker in Scaleway's vc1s Ubuntu Xenial"},{"content":"Run\ncurl -H 'X-Auth-Token: \u0026lt;YOUR_TOKEN\u0026gt;' https://cp-ams1.scaleway.com/servers/\u0026lt;YOUR_SERVER_ID\u0026gt;/user_data/ssh-host-fingerprints compare output with results of ssh \u0026lt;YOUR_USER\u0026gt;@\u0026lt;YOUR_SERVER_IP\u0026gt;\ncp-ams1 needs to be changed if you use non amsterdam based server.\nSources:\n  community.online.net\n  Scaleway API documentation\n  ","permalink":"https://serge-m.github.io/posts/check-server-fingerpring-for-scaleway/","summary":"Run\ncurl -H 'X-Auth-Token: \u0026lt;YOUR_TOKEN\u0026gt;' https://cp-ams1.scaleway.com/servers/\u0026lt;YOUR_SERVER_ID\u0026gt;/user_data/ssh-host-fingerprints compare output with results of ssh \u0026lt;YOUR_USER\u0026gt;@\u0026lt;YOUR_SERVER_IP\u0026gt;\ncp-ams1 needs to be changed if you use non amsterdam based server.\nSources:\n  community.online.net\n  Scaleway API documentation\n  ","title":"Check server fingerprint for scaleway"},{"content":"I2C interface Description of I2C interface\nRaspberry Pi SPI and I2C Tutorial \nContinuous deployment (Russian) Непрерывная кросс компиляция на Raspberry PI\nControlling motors Brushless motor Control brushless motor with ESC. Without additional controllers\nWith PCA9685 PWM Board (stackexchange thread)\nOne more thread\nControlling multiple servos To control multiple servos you can use PCA9685 controller. Connection is shown below. It\u0026rsquo;s better to connect VCC of the controller (red wire) to +5V or raspberry or to external power supply.\n  You have to enable I2C interface first with `sudo raspi-config`. Choose \"Interfacing Options\" - \"I2C\" - \"Enable\". Now installing the diagnostic tool and running:\nsudo apt-get install -y i2c-tools sudo i2cdetect -y 1 0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 40: 40 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: 70 -- -- -- -- -- -- -- Now we can install library:\npip install Adafruit_PCA9685 and run a simple program:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # sample.py # Simple demo of of the PCA9685 PWM servo/LED controller library. # This will move channel 0 from min to max position repeatedly. # Author: Tony DiCola # License: Public Domain from __future__ import division import time # Import the PCA9685 module. import Adafruit_PCA9685 # Uncomment to enable debug output. #import logging #logging.basicConfig(level=logging.DEBUG) # Initialise the PCA9685 using the default address (0x40). pwm = Adafruit_PCA9685.PCA9685() # Alternatively specify a different address and/or bus: #pwm = Adafruit_PCA9685.PCA9685(address=0x41, busnum=2) # Configure min and max servo pulse lengths servo_min = 150 # Min pulse length out of 4096 servo_max = 600 # Max pulse length out of 4096 # Helper function to make setting a servo pulse width simpler. def set_servo_pulse(channel, pulse): pulse_length = 1000000 # 1,000,000 us per second pulse_length //= 60 # 60 Hz print(\u0026#39;{0}us per period\u0026#39;.format(pulse_length)) pulse_length //= 4096 # 12 bits of resolution print(\u0026#39;{0}us per bit\u0026#39;.format(pulse_length)) pulse *= 1000 pulse //= pulse_length pwm.set_pwm(channel, 0, pulse) # Set frequency to 60hz, good for servos. pwm.set_pwm_freq(60) print(\u0026#39;Moving servo on channel 0, press Ctrl-C to quit...\u0026#39;) while True: # Move servo on channel O between extremes. pwm.set_pwm(0, 0, servo_min) time.sleep(1) pwm.set_pwm(0, 0, servo_max) time.sleep(1)   running:\npython sample.py To control more than 16 servose one can chain multiple drivers. See Chaining Drivers section for details. Some soldering is required to assign a unique address for each driver.\nSee alow:\n datasheet Adafruit 16 Channel Servo Driver with Raspberry Pi Created by Kevin Townsend. pdf. (pca-9685) troubleshooting Controlling one servo. No additional controllers needed  Stepper motors / DC (brushed) motors with l293d\n[Raspberry] Stepper and dc motor using specializer HAT\nBased on PC9865 PWM and TB6612 chipset. 1.2A per channel current capability (20ms long bursts of 3A peak)\n[Arduino] With adafruit motor schield v1\nBased on 74HC595N Serial to parallel output latch and L293D driver. 0.6A per bridge (1.2A peak) with thermal shutdown protection, 4.5V to 25V.\nLibrary for motor control See also about SN74HC595 shift register\n[Arduino] With adafruit motor shield v2\nBased on PCA9685 and TB6612 MOSFET drivers with 1.2A per channel current capability ( up to 3A peak for approx 20ms at a time)\n[Raspberry] Drive a DC motor forward and in reverse with variable speed (with l293d, adafruit lesson 9)\n[Micropython board] Control dc motor with pca9685\n[Raspberry] Video with just simple transistor scheme and with L293D controller\n[Raspberry] using L293D and 4N35 opto isolator\n[Arduino] 1 bidirectional DC motor using small DRV8871 motor driver\nUp to 45V and 3.6A of motor control\nIt is possible to have frequency controlled dc driver connected through Adafruit 16 Channel Servo Driver. See post. controller, ~100 Euro, powerfull\n[Arduino] using drv8833 driver,\n[Arduino] using l293d\nUsing transistors: (1)[http://electronics.stackexchange.com/questions/7235/motor-driver-using-only-a-2n2222-transistor], very weak\nConnecting via ssh: ssh -Y user@raspberrypi-url Access rasbberry Pi without monitor and ethernet Assuming we have an operating system (raspbian) installed.\n  Plug the SD-card into a computer.\n  Automatic connection to wifi. Edit /etc/wpa_supplicant/wpa_supplicant.conf and add the following lines:\n  network={ ssid=\u0026quot;my-network-name\u0026quot; psk=\u0026quot;my-network-pass\u0026quot; } In the end the file should look like this:\ncountry=GB ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\u0026quot;my-network-name\u0026quot; psk=\u0026quot;my-network-pass\u0026quot; } country field is essential. Wifi wont work without it. In the log you will see raspberrypi systemd[1]: Started Disable WiFi if country not set.\nIf you need to support multiple wifi connections use id_str field:\nnetwork={ ssid=\u0026quot;my-network-name\u0026quot; psk=\u0026quot;my-network-pass\u0026quot; id_str=\u0026quot;net1\u0026quot; } network={ ssid=\u0026quot;another-network\u0026quot; psk=\u0026quot;another pass\u0026quot; id_str=\u0026quot;net2\u0026quot; }  Enable SSH access. Create an empty file ssh in /boot/.\n  Plug the card back into your raspberry, turn on.\n  Now you can connect to raspberry via ssh:\nssh pi@raspberrypi  or\nssh pi@\u0026lt;IP-OF-YOUR-RASPBERRY\u0026gt;  Reading input (button) from GPIO without interrupts, raspi.tv\nRaspberryPi Zero pins Layout image source\nInteractive website for pinout of Raspberry Pi for different interfaces Pinout\nDonkey car Requirements for running donkey car:\nsudo apt install -y libatlas-base-dev libopenjp2-7-dev libtiff5-dev libhdf5-dev Other   Example of using 545043 power supply\n  description of sn74hc595\n  blog about building security robot\n  h bridge using 2n2222 transistors for dc motor control. + reverse; another version;\n  another version of h bridge\n  Build a Raspberry Pi Telepresence Rover  using Pololu DRV8835\n  My DIY remote controlled robot on raspberry pi with camera.\n  ","permalink":"https://serge-m.github.io/posts/raspberry-pi-links/","summary":"I2C interface Description of I2C interface\nRaspberry Pi SPI and I2C Tutorial \nContinuous deployment (Russian) Непрерывная кросс компиляция на Raspberry PI\nControlling motors Brushless motor Control brushless motor with ESC. Without additional controllers\nWith PCA9685 PWM Board (stackexchange thread)\nOne more thread\nControlling multiple servos To control multiple servos you can use PCA9685 controller. Connection is shown below. It\u0026rsquo;s better to connect VCC of the controller (red wire) to +5V or raspberry or to external power supply.","title":"Raspberry Pi Links"},{"content":"I need to test external API or perform integration test for my application. The extenal application can be accessible through docker image. I want to write a test that has\n docker run as set-up step docker stop/docker rm as tear down step  As an example lets consider Seaweedfs as external API. SeaweedFS is a simple and highly scalable distributed file system. To run it you need to run master and slave images. After running you can check status (curl -X GET localhost:9333).\nLets create two context managers: for master and slave. They will handle running and shutting down docker containers.\nMaster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  import subprocess import shlex import requests import contextlib import pytest docker_image = \u0026#34;chrislusf/seaweedfs\u0026#34; @contextlib.contextmanager def seaweedfs_master(): container_id = subprocess.check_output( shlex.split(\u0026#34;docker run -d -p 9333:9333 {docker_image} master\u0026#34;.format(docker_image=docker_image))) container_id = container_id.decode(\u0026#39;utf8\u0026#39;).strip() yield container_id subprocess.check_output([\u0026#39;docker\u0026#39;, \u0026#39;rm\u0026#39;, \u0026#39;-f\u0026#39;, container_id])   Slave:\n1 2 3 4 5 6 7 8 9 10 11 12 13  @contextlib.contextmanager def seaweedfs_slave(): with seaweedfs_master() as master: container_id = subprocess.check_output( shlex.split(\u0026#39;docker run -d -p 8080:8080 --link {master_id} \u0026#39; \u0026#39;{docker_image} volume -max=5 -mserver=\u0026#34;{master_id}:9333\u0026#34; -port=8080\u0026#39;.format( docker_image=docker_image, master_id=master ))) container_id = container_id.decode(\u0026#39;utf8\u0026#39;).strip() yield \u0026#34;http://localhost:9333/\u0026#34; subprocess.check_output([\u0026#39;docker\u0026#39;, \u0026#39;rm\u0026#39;, \u0026#39;-f\u0026#39;, container_id])   Helper class and pytest fixture:\n1 2 3 4 5 6 7 8 9  class SeaWeedFSConnection: def __init__(self, url): self.url = url @pytest.fixture(scope=\u0026#34;module\u0026#34;) def seaweedfs(request): with seaweedfs_slave() as seaweed_url: yield SeaWeedFSConnection(seaweed_url)   Finally our test:\n1 2 3 4 5 6  class TestImageStorage: def test1(self, seaweedfs): url = seaweedfs.url response = requests.get(url) assert response.status_code == 200   ","permalink":"https://serge-m.github.io/posts/run-docker-as-pytest-fixture/","summary":"I need to test external API or perform integration test for my application. The extenal application can be accessible through docker image. I want to write a test that has\n docker run as set-up step docker stop/docker rm as tear down step  As an example lets consider Seaweedfs as external API. SeaweedFS is a simple and highly scalable distributed file system. To run it you need to run master and slave images.","title":"Run docker as pytest fixture"},{"content":"Download JDK for Java 8 from Oracle\u0026rsquo;s web-site. Unpack archive to let\u0026rsquo;s say /usr/lib/jvm/jdk1.8.0/\nUpdate java alternatives:\nsudo update-alternatives --install \u0026quot;/usr/bin/java\u0026quot; \u0026quot;java\u0026quot; \u0026quot;/usr/lib/jvm/jdk1.8.0/bin/java\u0026quot; 1 sudo update-alternatives --install \u0026quot;/usr/bin/javac\u0026quot; \u0026quot;javac\u0026quot; \u0026quot;/usr/lib/jvm/jdk1.8.0/bin/javac\u0026quot; 1 sudo update-alternatives --install \u0026quot;/usr/bin/javaws\u0026quot; \u0026quot;javaws\u0026quot; \u0026quot;/usr/lib/jvm/jdk1.8.0/bin/javaws\u0026quot; 1 Select alternatives:\nsudo update-alternatives --config java Select an option corresponding to new path.\nDo the same for\nsudo update-alternatives --config javac sudo update-alternatives --config javaws Update JAVA_HOME using\ngksudo gedit /etc/environment Don\u0026rsquo;t forget to rerun terminal or reboot.\nBased on this thread\n","permalink":"https://serge-m.github.io/posts/installing-java-8-on-linux-mint-17-3/","summary":"Download JDK for Java 8 from Oracle\u0026rsquo;s web-site. Unpack archive to let\u0026rsquo;s say /usr/lib/jvm/jdk1.8.0/\nUpdate java alternatives:\nsudo update-alternatives --install \u0026quot;/usr/bin/java\u0026quot; \u0026quot;java\u0026quot; \u0026quot;/usr/lib/jvm/jdk1.8.0/bin/java\u0026quot; 1 sudo update-alternatives --install \u0026quot;/usr/bin/javac\u0026quot; \u0026quot;javac\u0026quot; \u0026quot;/usr/lib/jvm/jdk1.8.0/bin/javac\u0026quot; 1 sudo update-alternatives --install \u0026quot;/usr/bin/javaws\u0026quot; \u0026quot;javaws\u0026quot; \u0026quot;/usr/lib/jvm/jdk1.8.0/bin/javaws\u0026quot; 1 Select alternatives:\nsudo update-alternatives --config java Select an option corresponding to new path.\nDo the same for\nsudo update-alternatives --config javac sudo update-alternatives --config javaws Update JAVA_HOME using\ngksudo gedit /etc/environment Don\u0026rsquo;t forget to rerun terminal or reboot.","title":"Installing java 8 on linux mint 17.3"},{"content":"Testing is an essential part of software developmnet process. Unfortunately best prictives for python are established not as good as for example in Java world. Here I try to explain how to test Flask-based web applications. We want to test endpoints behaviour including status codes and parameters encoding. It means testing of handler functions for those endpoints is not enough.\nTests for endpoints can be considered/used as high-level acceptance tests.\nThe code consists of two files: sample_app.py (productions) and sample_app_test.py (testing). Testing is run using py.test.\nCode of sample_app.py Creating Flask application:\n1 2 3  from flask import Flask, request, Response, json app = Flask(__name__)   Helper class for JSON-based response:\n1 2 3  class JsonResponse(Response): def __init__(self, json_dict, status=200): super().__init__(response=json.dumps(json_dict), status=status, mimetype=\u0026#34;application/json\u0026#34;)   Defining GET and POST endpoints. The puprose of the /add endpoint is to return doubled value.\n1 2 3 4 5 6 7 8 9  @app.route(\u0026#39;/\u0026#39;) def hello_world(): return \u0026#39;Hello, World!\u0026#39; @app.route(\u0026#39;/add\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def add(): json = request.json resp = JsonResponse(json_dict={\u0026#34;answer\u0026#34;: json[\u0026#39;key\u0026#39;] * 2}, status=200) return resp   Main section that prints help message. (Alternative launching procedure can be applied)\n1 2 3 4 5 6  if __name__ == \u0026#39;__main__\u0026#39;: script_name = __file__ print(\u0026#34;run:\\n\u0026#34; \u0026#34;FLASK_APP={} python -m flask run --port 8000 --host 0.0.0.0\u0026#34;.format(script_name)) exit(1)   Code of sample_app_test.py Fixture for test client:\n1 2 3 4 5 6 7 8 9 10 11 12 13  import json import pytest from sample_app import app @pytest.fixture def client(request): test_client = app.test_client() def teardown(): pass # databases and resourses have to be freed at the end. But so far we don\u0026#39;t have anything request.addfinalizer(teardown) return test_client   Helper functions for encoding and decoding jsons:\n1 2 3 4 5 6 7 8  def post_json(client, url, json_dict): \u0026#34;\u0026#34;\u0026#34;Send dictionary json_dict as a json to the specified url \u0026#34;\u0026#34;\u0026#34; return client.post(url, data=json.dumps(json_dict), content_type=\u0026#39;application/json\u0026#39;) def json_of_response(response): \u0026#34;\u0026#34;\u0026#34;Decode json from response\u0026#34;\u0026#34;\u0026#34; return json.loads(response.data.decode(\u0026#39;utf8\u0026#39;))   The simplest test for GET endpoint:\n1 2 3  def test_dummy(client): response = client.get(\u0026#39;/\u0026#39;) assert b\u0026#39;Hello, World!\u0026#39; in response.data   Test for POST endpoint. Checking resulting json:\n1 2 3 4  def test_json(client): response = post_json(client, \u0026#39;/add\u0026#39;, {\u0026#39;key\u0026#39;: \u0026#39;value\u0026#39;}) assert response.status_code == 200 assert json_of_response(response) == {\u0026#34;answer\u0026#34;: \u0026#39;value\u0026#39; * 2}   Testing multipart file upload Imaging you have an endpoint that accepts POST requests with multipart files:\n1 2 3 4 5 6 7 8 9 10 11  @app.route(\u0026#39;/send\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def upload_file(): if request.method == \u0026#39;POST\u0026#39;: file_received = request.files[\u0026#39;file\u0026#39;] file_name = uuid.uuid4().hex with open(file_name, \u0026#34;wb\u0026#34;) as fout: fout.write(file_received.read()) return \u0026#34;file saved to {}\u0026#34;.format(file_name) return \u0026#34;Wrong request\u0026#34;   To test it we need a helper-function:\n1 2 3 4 5 6 7 8 9 10  def post_files(client, url, map_name_to_file: Dict): \u0026#34;\u0026#34;\u0026#34;Posts Multipart-encoded files to url :param client: flask test client fixture :param url: string URL :param map_name_to_file: dictionary name-\u0026gt;file-like object \u0026#34;\u0026#34;\u0026#34; map_name_to_file_and_name = {name: (file, \u0026#34;mocked_name_{}\u0026#34;.format(name)) for name, file in map_name_to_file.items()} return client.post(url, data=map_name_to_file_and_name, content_type=\u0026#39;multipart/form-data\u0026#39;)   Test itself:\n1 2 3  def test_multipart_files(client): response = post_files(client, \u0026#39;/send\u0026#39;, {\u0026#39;file\u0026#39;: BytesIO(b\u0026#34;content\u0026#34;)}) assert response.status_code == 200   Inspired by this gist\nTesting connextion Connextion is a wrapper around Flask that handles Oauth2 security and responses validation. Test for connextion app must include security checks.\nswagger.yaml In this file we define API of our microservice. Also it contains security settings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  swagger:\u0026#39;2.0\u0026#39;info:title:your appversion:\u0026#34;0.1\u0026#34;consumes:- application/jsonproduces:- application/jsonsecurity:- oauth2:[myscope]paths:/:get:operationId:endpoints.rootsummary:|testresponses:200:description:it works.401:description:bad auth.securityDefinitions:oauth2:type:oauth2flow:implicitauthorizationUrl:https://oauth.example/token_infox-tokenInfoUrl:https://oauth.example/token_infoscopes:myscope:Unique identifier of the user accessing the service.  endpoints.py Endpoints implementation.\n1 2  def root(): return {\u0026#34;result\u0026#34;: \u0026#34;lol\u0026#34;}, 200   test_endpoints.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  import json import os import pytest from connexion import App SPEC_FOLDER = os.path.join(os.path.dirname(__file__), \u0026#39;../\u0026#39;) # path to a directory with swagger.yaml file # fixture for replacing calls to oauth provider in connexion @pytest.fixture def oauth_requests(monkeypatch): # _fake_get is defined later in the code monkeypatch.setattr(\u0026#39;connexion.decorators.security.session.get\u0026#39;, _fake_get) # fixture for running test app with a predefined swagger file @pytest.fixture(scope=\u0026#34;session\u0026#34;) def secure_endpoint_app(): cnx_app = App(__name__, port=5001, specification_dir=SPEC_FOLDER, debug=True) cnx_app.add_api(\u0026#39;swagger.yaml\u0026#39;, validate_responses=True) return cnx_app CORRECT_TOKEN = \u0026#34;100\u0026#34; INCORRECT_TOKEN = \u0026#34;bla\u0026#34; # we are testing our app with mocked security provider.  # threfore we use  # 1. oauth_requests for patching security # 2. secure_endpoint_app for loading application with a required swagger file def test_security(oauth_requests, secure_endpoint_app): app_client = secure_endpoint_app.app.test_client() # must fail without Authorization header get_bye_no_auth = app_client.get(\u0026#39;/\u0026#39;) # type: flask.Response assert get_bye_no_auth.status_code == 401 # fails because of incorrect token get_bye_bad_auth = app_client.get(\u0026#39;/\u0026#39;, headers={\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer {}\u0026#34;.format(INCORRECT_TOKEN)}) # type: flask.Response assert get_bye_bad_auth.status_code == 401 # token is correct. Must return 200 get_bye_good_auth = app_client.get(\u0026#39;/\u0026#39;, headers={\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer {}\u0026#34;.format(CORRECT_TOKEN)}) # type: flask.Response assert get_bye_good_auth.status_code == 200 # fake response object used in _fake_get function class FakeResponse(object): def __init__(self, status_code, text): self.status_code = status_code self.text = text self.ok = status_code == 200 def json(self): return json.loads(self.text) # here we \u0026#34;check\u0026#34; tokens. Of course we don\u0026#39;t do actual call to Oauth provider, we mock it. def _fake_get(url, params=None, headers=None, timeout=None): \u0026#34;\u0026#34;\u0026#34; :type url: str :type params: dict| None \u0026#34;\u0026#34;\u0026#34; headers = headers or {} if url == \u0026#34;https://oauth.example/token_info\u0026#34;: token = headers.get(\u0026#39;Authorization\u0026#39;, \u0026#39;invalid\u0026#39;).split()[-1] if token in [CORRECT_TOKEN, \u0026#34;has_myscope\u0026#34;]: return FakeResponse(200, \u0026#39;{\u0026#34;uid\u0026#34;: \u0026#34;test-user\u0026#34;, \u0026#34;scope\u0026#34;: [\u0026#34;myscope\u0026#34;]}\u0026#39;) if token in [\u0026#34;200\u0026#34;, \u0026#34;has_wrongscope\u0026#34;]: return FakeResponse(200, \u0026#39;{\u0026#34;uid\u0026#34;: \u0026#34;test-user\u0026#34;, \u0026#34;scope\u0026#34;: [\u0026#34;wrongscope\u0026#34;]}\u0026#39;) if token == \u0026#34;has_myscope_otherscope\u0026#34;: return FakeResponse(200, \u0026#39;{\u0026#34;uid\u0026#34;: \u0026#34;test-user\u0026#34;, \u0026#34;scope\u0026#34;: [\u0026#34;myscope\u0026#34;, \u0026#34;otherscope\u0026#34;]}\u0026#39;) if token in [INCORRECT_TOKEN, \u0026#34;is_not_invalid\u0026#34;]: return FakeResponse(404, \u0026#39;\u0026#39;) return url   inspired by connexiton tests: [1] and [2]\nRelated links:  Testing flask applications pytest fixtures for flask I haven\u0026rsquo;t checked them. Maybe uyseful Flaskr test application from examples of Flask Gist \u0026ldquo;Testing file upload handling in Flask\u0026rdquo;  See also  Pytest cheatsheet  ","permalink":"https://serge-m.github.io/posts/testing-json-responses-in-flask-rest-apps-with-pytest/","summary":"Testing is an essential part of software developmnet process. Unfortunately best prictives for python are established not as good as for example in Java world. Here I try to explain how to test Flask-based web applications. We want to test endpoints behaviour including status codes and parameters encoding. It means testing of handler functions for those endpoints is not enough.\nTests for endpoints can be considered/used as high-level acceptance tests.","title":"Testing json responses in Flask REST apps with pytest"},{"content":"I host my notes on github pages and I use Pelican for building html content from Markdown format. Tracis CI can be used to automate building and publishing changes.\nRegistration on https://travis-ci.org/ is straightforward.\nI have only public free accounts on github. Thus I need two repositories: one containing sources and another containing html. The latter is rendered automatically via Github Pages. If I would have paid hithib account I could have only one repo with two branches: master for sources and gh-pages for html.\nPushes to sources repository has to trigger builds on TravisCI. That is made in the settings of Travis. In https://travis-ci.org/profile/you need to enable corresponding repository.\nConfiguration of travis You need to create .travis.yml file in the root of your sources repository. Lets start with the following contents:\nlanguage: python python: - \u0026quot;3.5\u0026quot; branches: only: - master install: - pip install pelican markdown script: - make github Here we ask Travis to use python 3, build only master, install pelican and markdown and run make github command in the end.\nInstalling markdown is important here. Without it you can end up with failures. Pelican will say:\nWARNING: No valid files found in content. WARNING: sitemap plugin: SITEMAP['format'] must be `txt' or `xml' WARNING: sitemap plugin: Setting SITEMAP['format'] on `xml' That means Pelican doesn\u0026rsquo;t know about markdown format.\nNow you need to configure Makefile and github target. I modified default Pelicans github target as follows. Remove lines startign from \u0026ldquo;#\u0026rdquo; - they are just comments.\ngithub: # Loading commit message from source repository to SITE_COMMIT_MESSAGE variable SITE_COMMIT_MESSAGE=`git log -1 --format=%B` \u0026amp;\u0026amp; \\ $$(rm -rf $(OUTPUTDIR) || true) \u0026amp;\u0026amp; \\ # remove output directory git clone git@github.com:\u0026lt;your github user\u0026gt;/\u0026lt;your html repository\u0026gt;.git $(OUTPUTDIR) \u0026amp;\u0026amp; \\ # fresh clone of your html repo to output directory $$(ls -d $(OUTPUTDIR)/* | xargs rm -r) \u0026amp;\u0026amp; \\ # deleting everything (except hidden files, like .git) $(PELICAN) $(INPUTDIR) -o $(OUTPUTDIR) -s $(PUBLISHCONF) $(PELICANOPTS) \u0026amp;\u0026amp; \\ # running pelican cd $(OUTPUTDIR) \u0026amp;\u0026amp; \\ # go to output directory git add -v --all . \u0026amp;\u0026amp; \\ # configure user/email of git commit, commit, push git config user.email \u0026quot;\u0026lt;your mail\u0026gt;\u0026quot; \u0026amp;\u0026amp; \\ git config user.name \u0026quot;\u0026lt;your name\u0026gt;\u0026quot; \u0026amp;\u0026amp; \\ git commit -v -m \u0026quot;$$SITE_COMMIT_MESSAGE\u0026quot; \u0026amp;\u0026amp; \\ git push \u0026amp;\u0026amp; \\ echo \u0026quot;done\u0026quot; At this point you will likely get access errors during the build on travis. The cause is that git on travis doesn\u0026rsquo;t have access to modifying your html repository.\nI followed the guide from here sections 1 \u0026ndash; 2.5. Instead of section 2.6:\n I put my encoded private key blog_deploy_key.enc to .travis/blog_deploy_key.enc I modified my section script of .travis.yaml as folows:  script: - | declare -r SSH_FILE=\u0026quot;$(mktemp -u $HOME/.ssh/blog_deploy_key_decrypted_XXXXXX)\u0026quot; openssl aes-256-cbc -K $encrypted_\u0026lt;VALUE_FROM_TRAVIS\u0026gt;_key -iv $encrypted_\u0026lt;VALUE_FROM_TRAVIS\u0026gt;_iv -in \u0026quot;.travis/blog_deploy_key.enc\u0026quot; -out \u0026quot;$SSH_FILE\u0026quot; -d chmod 600 \u0026quot;$SSH_FILE\u0026quot; \u0026amp;\u0026amp; printf \u0026quot;%s\\n\u0026quot; \\ \u0026quot;Host github.com\u0026quot; \\ \u0026quot; IdentityFile $SSH_FILE\u0026quot; \\ \u0026quot; LogLevel ERROR\u0026quot; \u0026gt;\u0026gt; ~/.ssh/config - make github Keys VALUE_FROM_TRAVIS you get while making steps 1 - 2.5 from the guide.\n","permalink":"https://serge-m.github.io/posts/set-up-travis-ci-for-building-personal-page-on-github-pages-with-pelican/","summary":"I host my notes on github pages and I use Pelican for building html content from Markdown format. Tracis CI can be used to automate building and publishing changes.\nRegistration on https://travis-ci.org/ is straightforward.\nI have only public free accounts on github. Thus I need two repositories: one containing sources and another containing html. The latter is rendered automatically via Github Pages. If I would have paid hithib account I could have only one repo with two branches: master for sources and gh-pages for html.","title":"Set up Travis CI for building personal page on Github Pages with Pelican"},{"content":"Pycon2016 talk by Brett Slatkin Example 1: Extract variable\nimport random month = random.choice(MONTHS) if (month.lower().endswith('r') or month.lower().endswith('ary')): print('%s is a good time to eat oysters' % month) elif 8 \u0026gt; MONTHS.index(month) \u0026gt; 4: print('%s is a good time to eat tomatoes' % month) else: print('%s is a good time to eat asparagus' % month) Becomes:\nclass OystersGood: def __init__(self, month): month = month month_lowered = month.lower() self.ends_in_r = month_lowered.endswith('r') self.ends_in_ary = month_lowered.endswith('ary') self._result = self.ends_in_r or self.ends_in_ary def __bool__(self): # Equivalent to __nonzero__ in Python 2 return self._result class TomatoesGood: def __init__(self, month): self.index = MONTHS.index(month) self._result = 8 \u0026gt; self.index \u0026gt; 4 def __bool__(self): # Equivalent to __nonzero__ in Python 2 return self._result time_for_oysters = OystersGood(month) time_for_tomatoes = TomatoesGood(month) if time_for_oysters: print('%s is a good time to eat oysters' % month) elif time_for_tomatoes: print('%s is a good time to eat tomatoes' % month) else: print('%s is a good time to eat asparagus' % month) November is a good time to eat oysters\nNow the helper function is easy to test and introspect.\ntest = OystersGood('November') assert test assert test.ends_in_r assert not test.ends_in_ary test = OystersGood('July') assert not test assert not test.ends_in_r assert not test.ends_in_ary Dependency injection for better testing\n","permalink":"https://serge-m.github.io/posts/refactoring-python-extract-variable/","summary":"Pycon2016 talk by Brett Slatkin Example 1: Extract variable\nimport random month = random.choice(MONTHS) if (month.lower().endswith('r') or month.lower().endswith('ary')): print('%s is a good time to eat oysters' % month) elif 8 \u0026gt; MONTHS.index(month) \u0026gt; 4: print('%s is a good time to eat tomatoes' % month) else: print('%s is a good time to eat asparagus' % month) Becomes:\nclass OystersGood: def __init__(self, month): month = month month_lowered = month.lower() self.ends_in_r = month_lowered.endswith('r') self.","title":"Refactoring python code. Extracting variables and other."},{"content":"Books   Test-Driven Development with Python Harry Percival\n  Python Testing with unittest, nose, pytest : eBook\n  Testing Python: Applying Unit Testing, TDD, BDD and Acceptance Testing link .\n  Videos   Outside-In TDD Harry Percival, PyCon 2016\n  Докеризация веб приложения на Python, Антон Егоров\n  Thinking about Concurrency, Raymond Hettinger, Python core developer\n  Tutorials   Разработка идеального pypi пакета с поддержкой разных версий python (Rus), 2020.\n  The Little Book of Python Anti-Patterns - an awesome collection of best practices with examples.\n  Set up vim for python\n  Set up emacs for python\n  Checklist to build great Celery async tasks\n  Getting Started with Django Rest Framework\n  pythonsheets - a colleciton of useful snippets\n  Command line arguments parsing with click\n  Asyncronous python   Some historical overview Asynchronous Python\n  [ Video ] Couple of recipies: Practical Python Async for Dummies\n  pytest addon for testing coroutines pytest-asyncio\n  Some more guidelines how to test coroutines: Advanced asyncio testing\n  Performance   Grok the GIL: Write Fast and Thread-Safe Python - how GIL works\n  library for memory profiling\n  Modules cloud storage module - supports AWS S3, Google cloud storage and local file system. Hopefully it has unified interface for all three of the backends.\nApache libcloud \u0026ndash; anouther cloud file system API. more alive\nTesting, packaging, releasing   Packaging a python library, 25 May 2014 (updated 30 September 2019) by ionelmc. The article proposes a way to structure the code of your python module, describes how to write setup.py file, set up tox, pytest etc. Cookie cutter library for initialization of new projects: https://github.com/ionelmc/cookiecutter-pylibrary\n  About processing of KeyboardInterrupt exception, INT signals and background processes. Capturing SIGINT using KeyboardInterrupt exception works in terminal, not in script , stack overflow.\n # invocation from interactive shell $ python -c \u0026quot;import signal; print(signal.getsignal(signal.SIGINT))\u0026quot; \u0026lt;built-in function default_int_handler\u0026gt; # background job in interactive shell $ python -c \u0026quot;import signal; print(signal.getsignal(signal.SIGINT))\u0026quot; \u0026amp; \u0026lt;built-in function default_int_handler\u0026gt; # invocation in non interactive shell $ sh -c 'python -c \u0026quot;import signal; print(signal.getsignal(signal.SIGINT))\u0026quot;' \u0026lt;built-in function default_int_handler\u0026gt; # background job in non-interactive shell $ sh -c 'python -c \u0026quot;import signal; print(signal.getsignal(signal.SIGINT))\u0026quot; \u0026amp;' 1    Blogs  https://snarky.ca/ - Python core developer. Dev manager for the Python extension for VS Code.  ","permalink":"https://serge-m.github.io/posts/useful-python-links/","summary":"Books   Test-Driven Development with Python Harry Percival\n  Python Testing with unittest, nose, pytest : eBook\n  Testing Python: Applying Unit Testing, TDD, BDD and Acceptance Testing link .\n  Videos   Outside-In TDD Harry Percival, PyCon 2016\n  Докеризация веб приложения на Python, Антон Егоров\n  Thinking about Concurrency, Raymond Hettinger, Python core developer\n  Tutorials   Разработка идеального pypi пакета с поддержкой разных версий python (Rus), 2020.","title":"Useful python links"},{"content":"there are plenty of discussions about it. I didn\u0026rsquo;t find full solution. You can add google as a search engine. But suggestions don\u0026rsquo;t work for me.\nMy approximate solution   remove standard firefox:\n sudo apt-get remove firefox    remove search addons. Source. I am not sure if this step actually helped. TODO: Check if it is required.\n sudo apt-get remove mint-search-addon    delete file (or rename to have a backup):\n ~/.mozilla/firefox/\u0026lt;SOMETHING\u0026gt;.default/search.json.mozlz4  and contents of the directory:\n ~/.mozilla/firefox/sf8ha7dx.default/searchplugins    download, install and run firefox from official website. It will create a new version of\n ~/.mozilla/firefox/sf8ha7dx.default/searchplugins    in .mozilla/firefox/sf8ha7dx.default/searchplugins create a file google1.xml with contents:\n \u0026lt;SearchPlugin xmlns=\u0026quot;http://www.mozilla.org/2006/browser/search/\u0026quot;\u0026gt; \u0026lt;ShortName\u0026gt;Google1\u0026lt;/ShortName\u0026gt; \u0026lt;Description\u0026gt;Google1\u0026lt;/Description\u0026gt; \u0026lt;InputEncoding\u0026gt;UTF-8\u0026lt;/InputEncoding\u0026gt; \u0026lt;SearchForm\u0026gt;http://www.google.com\u0026lt;/SearchForm\u0026gt; \u0026lt;Url type=\u0026quot;application/x-suggestions+json\u0026quot; method=\u0026quot;GET\u0026quot; template=\u0026quot;http://www.google.com/complete/search?client=firefox\u0026amp;amp;q={searchTerms}\u0026quot; /\u0026gt; \u0026lt;Url type=\u0026quot;text/html\u0026quot; method=\u0026quot;GET\u0026quot; template=\u0026quot;http://www.google.com/search?q={searchTerms}\u0026quot; resultDomain=\u0026quot;google.com\u0026quot;\u0026gt; \u0026lt;/Url\u0026gt; \u0026lt;/SearchPlugin\u0026gt;    remove official normal version of firefox\n  install mint version of firefox:\n sudo apt-get install firefox  Now \u0026ldquo;mint\u0026rdquo; firefox will use search.json.mozlz4 version from normal firefox and suggestions should work\n  Optional. I also removed (actually renamed) from /usr/* all directories like searchplugins that contain files like yahoo.xml. I think they are created by \u0026ldquo;mint\u0026rdquo; firefox. Not sure that it is a necessary step. Further check is required here.\n  ","permalink":"https://serge-m.github.io/posts/fix-google-search-suggestions-in-firefox-in-linux-mint/","summary":"there are plenty of discussions about it. I didn\u0026rsquo;t find full solution. You can add google as a search engine. But suggestions don\u0026rsquo;t work for me.\nMy approximate solution   remove standard firefox:\n sudo apt-get remove firefox    remove search addons. Source. I am not sure if this step actually helped. TODO: Check if it is required.\n sudo apt-get remove mint-search-addon    delete file (or rename to have a backup):","title":"How to fix google search suggestions in Firefox in Linux Mint"},{"content":" Soft Skills Engineering don\u0026rsquo;t speak spotlightradio  ","permalink":"https://serge-m.github.io/posts/podcasts/","summary":" Soft Skills Engineering don\u0026rsquo;t speak spotlightradio  ","title":"podcasts"},{"content":"$ git --git-dir=../\u0026lt;some_other_repo\u0026gt;/.git format-patch -k -1 --stdout \u0026lt;commit SHA\u0026gt; | git am -3 -k Source: http://stackoverflow.com/questions/6658313/generate-a-git-patch-for-a-specific-commit\n","permalink":"https://serge-m.github.io/posts/git-apply-patch-from-another-repository/","summary":"$ git --git-dir=../\u0026lt;some_other_repo\u0026gt;/.git format-patch -k -1 --stdout \u0026lt;commit SHA\u0026gt; | git am -3 -k Source: http://stackoverflow.com/questions/6658313/generate-a-git-patch-for-a-specific-commit","title":"git apply patch from another repository"},{"content":"Installation sudo apt-get install tmux\nCommands In tmux, hit the prefix ctrl+b (my modified prefix is ctrl+a) and then: ###Sessions\n:new\u0026lt;CR\u0026gt; new session s list sessions $ name session ###Windows (tabs)\nc create window w list windows n next window p previous window f find window , name window \u0026amp; kill window Panes (splits) % vertical split \u0026quot; horizontal split o swap panes q show pane numbers x kill pane + break pane into window (e.g. to select text by mouse to copy) - restore pane from window ⍽ space - toggle between layouts Set up scrolling and fix Shift-F5, Shift-Fn in mc put this command in your ~/.tmux.conf\nset -g mouse on # Lower escape timing from 500ms to 50ms for quicker response to scroll-buffer access. set -s escape-time 50 # Fix Shift+{Fn} keys in Midnight commander setw -g xterm-keys on For older versions of tmux use\nsetw -g mode-mouse on as a first line. Otherwise you get\n... .tmux.conf:1: unknown option: mode-mouse ... ","permalink":"https://serge-m.github.io/posts/terminal-setup-in-linux-mint/","summary":"Installation sudo apt-get install tmux\nCommands In tmux, hit the prefix ctrl+b (my modified prefix is ctrl+a) and then: ###Sessions\n:new\u0026lt;CR\u0026gt; new session s list sessions $ name session ###Windows (tabs)\nc create window w list windows n next window p previous window f find window , name window \u0026amp; kill window Panes (splits) % vertical split \u0026quot; horizontal split o swap panes q show pane numbers x kill pane + break pane into window (e.","title":"terminal setup in linux mint"},{"content":"  Copying partition is straightforward. Made using gparted.\n  Create bootable USB stick (Startup disk creator in Linux mint), boot from it. Assume we have following partitions:\n   boot /sdb1 (don't really know how it works) root /sdb2 home /sdb3  Do  sudo mount /dev/sda2 /mnt ## if you have boot partition: # sudo mount /dev/sda1 /mnt/boot sudo grub-install --root-directory=/mnt /dev/sda Source of GRUB instructions\n","permalink":"https://serge-m.github.io/posts/fix-boot-record-after-moving-linux-mint/","summary":"Copying partition is straightforward. Made using gparted.\n  Create bootable USB stick (Startup disk creator in Linux mint), boot from it. Assume we have following partitions:\n   boot /sdb1 (don't really know how it works) root /sdb2 home /sdb3  Do  sudo mount /dev/sda2 /mnt ## if you have boot partition: # sudo mount /dev/sda1 /mnt/boot sudo grub-install --root-directory=/mnt /dev/sda Source of GRUB instructions","title":"Fix boot record after moving linux mint partitions to another disk"},{"content":" apt-get install davfs2 mkdir /mnt/yandex.disk mount -t davfs https://webdav.yandex.ru /mnt/yandex.disk/ # check: df -h /mnt/yandex.disk/ ","permalink":"https://serge-m.github.io/posts/mount-yandex-webdav-on-local-dir/","summary":" apt-get install davfs2 mkdir /mnt/yandex.disk mount -t davfs https://webdav.yandex.ru /mnt/yandex.disk/ # check: df -h /mnt/yandex.disk/ ","title":"Mount yandex webdav on local dir"},{"content":"require('image') imgPath = \u0026quot;image.jpg\u0026quot; img = torch.Tensor(1, 3, imgDim, imgDim) img[1] = image.load(imgPath, 3, byte) image is stored as float, conversion is (intensity/255.). stored top-\u0026gt;bottom, line by line. format RGB.\ndisplay require 'gnuplot' gnuplot.figure(1) gnuplot.imagesc(img[1]) Torch\u0026lt;-\u0026gt;numpy dictionary https://github.com/torch/torch7/wiki/Torch-for-Numpy-users\n","permalink":"https://serge-m.github.io/posts/how-torch-stores-images/","summary":"require('image') imgPath = \u0026quot;image.jpg\u0026quot; img = torch.Tensor(1, 3, imgDim, imgDim) img[1] = image.load(imgPath, 3, byte) image is stored as float, conversion is (intensity/255.). stored top-\u0026gt;bottom, line by line. format RGB.\ndisplay require 'gnuplot' gnuplot.figure(1) gnuplot.imagesc(img[1]) Torch\u0026lt;-\u0026gt;numpy dictionary https://github.com/torch/torch7/wiki/Torch-for-Numpy-users","title":"how torch stores images"},{"content":"I got the error while installing torch for miniconda. Something like Linking CXX executable mshrable\n~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `BC' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetnum' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `PC' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tputs' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetent' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetflag' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgoto' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `UP' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetstr The same was for python3 and python2 environments. The solution (at least for python 2) was to remove miniconda from path and compile torch with the system python.\nSolution from here\nsame here, temporay fix for me is to remove conda readline from the environment:\nconda remove --force readline and install the python bindings with\npip install readline That way, I presume it is using the system readline\n","permalink":"https://serge-m.github.io/posts/installing-torch-for-miniconda/","summary":"I got the error while installing torch for miniconda. Something like Linking CXX executable mshrable\n~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `BC' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetnum' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `PC' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tputs' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetent' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetflag' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgoto' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `UP' ~/miniconda/envs/py27/lib/libreadline.so.6: undefined reference to `tgetstr The same was for python3 and python2 environments. The solution (at least for python 2) was to remove miniconda from path and compile torch with the system python.","title":"Installing torch for miniconda"},{"content":"Activate miniconda environment (my environment is called py3):\n1  source activate py3   Produce initial steps from the readme:\n1 2 3 4 5  cd examples mkdir build cd build cmake .. cmake --build . --config Release   inside function build_dlib() of file setup.py add the highlighted code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  if platform_arch == \u0026#39;64bit\u0026#39; and sys.platform == \u0026#34;win32\u0026#34;: # 64bit build on Windows #.................. bla bla ................... for ext in [py_ver.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) + \u0026#39;.lib\u0026#39;, py_ver + \u0026#39;mu.lib\u0026#39;, py_ver + \u0026#39;m.lib\u0026#39;, py_ver + \u0026#39;u.lib\u0026#39;]: py_lib = os.path.abspath(os.path.join(inc_dir, \u0026#39;../libs/\u0026#39;, \u0026#39;python\u0026#39; + ext)) if os.path.exists(py_lib): cmake_extra_arch += [\u0026#39;-DPYTHON_LIBRARY={lib}\u0026#39;.format(lib=py_lib)] break else: cmake_extra_arch += [\u0026#39;-DPYTHON_LIBRARY=/home/user/miniconda/envs/py3/lib/libpython3.so\u0026#39;] cmake_extra_arch += [\u0026#39;-DPYTHON_INCLUDE_DIR=/home/user/miniconda/envs/py3/include/python3.5m\u0026#39;] build_dir = os.path.join(script_dir, \u0026#34;./tools/python/build\u0026#34;) #......................bla bla.....................................   Replace /home/user/miniconda by your path to miniconda\nRun python setup.py install (as usual)\n","permalink":"https://serge-m.github.io/posts/compile-dlib-for-miniconda/","summary":"Activate miniconda environment (my environment is called py3):\n1  source activate py3   Produce initial steps from the readme:\n1 2 3 4 5  cd examples mkdir build cd build cmake .. cmake --build . --config Release   inside function build_dlib() of file setup.py add the highlighted code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  if platform_arch == \u0026#39;64bit\u0026#39; and sys.","title":"compile dlib for miniconda"},{"content":"Use sudo if needed\nMount mkdir /mnt/share mount -t cifs //windows_machine_ip/share_name -o username=user,password=urPassword /mnt/share  To allow write access one has to specify owning user and group:\nsudo mount -t cifs //windows_machine_ip/share_name -o uid=$(id -u),gid=$(id -g),user=,password= /mnt/share  Unmount umount /mnt/shares  ","permalink":"https://serge-m.github.io/posts/mount-windows-shares-in-linux/","summary":"Use sudo if needed\nMount mkdir /mnt/share mount -t cifs //windows_machine_ip/share_name -o username=user,password=urPassword /mnt/share  To allow write access one has to specify owning user and group:\nsudo mount -t cifs //windows_machine_ip/share_name -o uid=$(id -u),gid=$(id -g),user=,password= /mnt/share  Unmount umount /mnt/shares  ","title":"Mount windows shares in linux"},{"content":"Rest API example for tensorflow. It works: demo\nTrained models for tensorflow\nTF-slim - high-level API of TensorFlow for defining, training and evaluating complex models. Doesn\u0026rsquo;t work for python 3 (see here)\nVGG16 and VGG19 in Tensorflow. One more here. And one more\nDeep learning for lazybones\nInception-like CNN model based on 1d convolutions http://arxiv.org/pdf/1512.00567v3.pdf\nChat (in russian) http://closedcircles.com/?invite=99b1ac08509c560137b2e3c54d4398b0fa4c175e\n","permalink":"https://serge-m.github.io/posts/deep-learning/","summary":"Rest API example for tensorflow. It works: demo\nTrained models for tensorflow\nTF-slim - high-level API of TensorFlow for defining, training and evaluating complex models. Doesn\u0026rsquo;t work for python 3 (see here)\nVGG16 and VGG19 in Tensorflow. One more here. And one more\nDeep learning for lazybones\nInception-like CNN model based on 1d convolutions http://arxiv.org/pdf/1512.00567v3.pdf\nChat (in russian) http://closedcircles.com/?invite=99b1ac08509c560137b2e3c54d4398b0fa4c175e","title":"deep learning"},{"content":"create a file .inputrc in your home directory and put there\n\u0026quot;\\e[A\u0026quot;: history-search-backward \u0026quot;\\e[B\u0026quot;: history-search-forward set show-all-if-ambiguous on set completion-ignore-case on # display possible completions using different colors to indicate their file types set colored-stats On TAB: menu-complete Restart your terminal. Now you can autocomplete from history using Up and Down keys.\nSource\n","permalink":"https://serge-m.github.io/posts/autocomplete-from-history-in-terminal/","summary":"create a file .inputrc in your home directory and put there\n\u0026quot;\\e[A\u0026quot;: history-search-backward \u0026quot;\\e[B\u0026quot;: history-search-forward set show-all-if-ambiguous on set completion-ignore-case on # display possible completions using different colors to indicate their file types set colored-stats On TAB: menu-complete Restart your terminal. Now you can autocomplete from history using Up and Down keys.\nSource","title":"Autocomplete from the history in terminal"},{"content":"Часть 1: https://habrahabr.ru/post/274811/ Часть 2: https://habrahabr.ru/post/274905/\n","permalink":"https://serge-m.github.io/posts/working-with-date-and-time-in-java/","summary":"Часть 1: https://habrahabr.ru/post/274811/ Часть 2: https://habrahabr.ru/post/274905/","title":"Working with date and time in Java (russian)"},{"content":"Looks like pypy now can build numpy. Well, a slightly modified numpy.\n  Get default branch of pypy. be careful cause the developers don\u0026rsquo;t maintain default branch compilable. Revision 84341 (c86b42dd7613) works for me.\n  compile using\n ./rpython/bin/rpython -O2 ./pypy/goal/targetpypystandalone.py --withoutmod-micronumpy    Create package and vitual environment. Something like this:\n ./pypy/tool/release/package.py --targetdir ./my_builds/build.tar.bz2 --builddir ./tmp/ --nostrip --archive-name pypy_84341  Needed to copy pypy-c and libpypy to pypy/goal beforehand.\n  Clone and follow instructions from https://github.com/pypy/numpy/commits/cpyext-ext Revision 3299d0d76fdb831fbcb4429a89c1f53bb36ea07f worked for me\nUPDATE: link doesn\u0026rsquo;t work any more. try to find corresponding link in bitbucket-\u0026gt;pypy/\n Testing results: ---------------------------------------------------------------------- Ran 5900 tests in 78.216s FAILED (KNOWNFAIL=3, SKIP=6, errors=218, failures=83)    scipy can be compiled if disabling submodules io/matlab and spatial. Though is still doesn\u0026rsquo;t work.\n","permalink":"https://serge-m.github.io/posts/pypy-with-numpy/","summary":"Looks like pypy now can build numpy. Well, a slightly modified numpy.\n  Get default branch of pypy. be careful cause the developers don\u0026rsquo;t maintain default branch compilable. Revision 84341 (c86b42dd7613) works for me.\n  compile using\n ./rpython/bin/rpython -O2 ./pypy/goal/targetpypystandalone.py --withoutmod-micronumpy    Create package and vitual environment. Something like this:\n ./pypy/tool/release/package.py --targetdir ./my_builds/build.tar.bz2 --builddir ./tmp/ --nostrip --archive-name pypy_84341  Needed to copy pypy-c and libpypy to pypy/goal beforehand.","title":"pypy with numpy"},{"content":"Production Database helpers for sqlalchemy Backend-agnostic database creation (CREATE IF NOT EXISTS):\n1 2  if not database_exists(\u0026#39;postgres://postgres@localhost/name\u0026#39;): create_database(\u0026#39;postgres://postgres@localhost/name\u0026#39;)   Possible with SQLAlchemy-Utils library. See docs\nInfrastructure with Python http://dustinrcollins.com/infrastructure-with-python \u0026ndash; list of tools for python development\nRetry libraries for python tenacity - a fork of retrying. Seems alive and powerful.\nretrying - abandoned but popular project.\nbackoff\nLanguage Cheat Sheet: Writing Python 2-3 compatible code http://python-future.org/compatible_idioms.html\nGoogle Python Guidelines https://google.github.io/styleguide/pyguide.html\nInheritance https://rhettinger.wordpress.com/2011/05/26/super-considered-super/\nOther How to Install Python 3.6 on Ubuntu 16.04\napt-get install zlib1g-dev wget https://www.python.org/ftp/python/3.6.3/Python-3.6.3.tgz tar -xvf Python-3.6.3.tgz cd Python-3.6.3 ./configure make make install Now it should be fine:\n# python3.6 -V Python 3.6.3 Dismissing Python Garbage Collection at Instagram\n","permalink":"https://serge-m.github.io/posts/about-python/","summary":"Production Database helpers for sqlalchemy Backend-agnostic database creation (CREATE IF NOT EXISTS):\n1 2  if not database_exists(\u0026#39;postgres://postgres@localhost/name\u0026#39;): create_database(\u0026#39;postgres://postgres@localhost/name\u0026#39;)   Possible with SQLAlchemy-Utils library. See docs\nInfrastructure with Python http://dustinrcollins.com/infrastructure-with-python \u0026ndash; list of tools for python development\nRetry libraries for python tenacity - a fork of retrying. Seems alive and powerful.\nretrying - abandoned but popular project.\nbackoff\nLanguage Cheat Sheet: Writing Python 2-3 compatible code http://python-future.org/compatible_idioms.html\nGoogle Python Guidelines https://google.","title":"About python"},{"content":"Hmm. It doesn\u0026rsquo;t work. I see the icon when i do\ndropbox stop dropbox start but I cannot click on it.\nInstalled nemo-dropbox. At least now I have \u0026ldquo;Copy dropbox link\u0026rdquo; command in menu. Don\u0026rsquo;t forget to do\n$ killall nemo after installation to make it work.\nFinal Solution source Use\ndropbox stop \u0026amp;\u0026amp; dbus-launch dropbox start Or\nDo: opening \u0026lsquo;Preferences\u0026rsquo; -\u0026gt; \u0026lsquo;Startup Applications\u0026rsquo; and editing the dropbox entry so the command now reads:\nCODE: SELECT ALL\ndbus-launch dropbox start update might overwrite the change. I think it\u0026rsquo;s better to leave the existing entry alone (only disabled) and create a new entry (e.g. \u0026ldquo;Launch DropBox\u0026rdquo;) with the new start command (dbus-launch dropbox start)\n","permalink":"https://serge-m.github.io/posts/dropbox-in-linux-mint/","summary":"Hmm. It doesn\u0026rsquo;t work. I see the icon when i do\ndropbox stop dropbox start but I cannot click on it.\nInstalled nemo-dropbox. At least now I have \u0026ldquo;Copy dropbox link\u0026rdquo; command in menu. Don\u0026rsquo;t forget to do\n$ killall nemo after installation to make it work.\nFinal Solution source Use\ndropbox stop \u0026amp;\u0026amp; dbus-launch dropbox start Or\nDo: opening \u0026lsquo;Preferences\u0026rsquo; -\u0026gt; \u0026lsquo;Startup Applications\u0026rsquo; and editing the dropbox entry so the command now reads:","title":"Dropbox in linux mint"},{"content":"UPD: It seems that the current solution (2019) is VSCode. It\u0026rsquo;s free and powerful. There is a plenty of plugins.\nVSCode How to see content of std::string in the debugger You have to enable pretty printing by default for gdb, Add the following to your launch.json:\n\u0026quot;setupCommands\u0026quot;: [ { \u0026quot;text\u0026quot;: \u0026quot;-enable-pretty-printing\u0026quot; } ]  Using CMake in VSCode See C++ and CMake\nClion CLion is awesome but expensive.\nCodelite Found Codelite on http://stackoverflow.com/a/1775460. Looks good. No disgust after 10 minutes of work.\nI was able to debug C code of Numpy in Virtualenv.\n  Launched from console after activating of virtual environment \u0026gt; source ./\u0026lt;path to activate\u0026gt;/activate\n  add breakpoints\n  Run (F5)\n  ","permalink":"https://serge-m.github.io/posts/c-plus-plus-ide-for-linux/","summary":"UPD: It seems that the current solution (2019) is VSCode. It\u0026rsquo;s free and powerful. There is a plenty of plugins.\nVSCode How to see content of std::string in the debugger You have to enable pretty printing by default for gdb, Add the following to your launch.json:\n\u0026quot;setupCommands\u0026quot;: [ { \u0026quot;text\u0026quot;: \u0026quot;-enable-pretty-printing\u0026quot; } ]  Using CMake in VSCode See C++ and CMake\nClion CLion is awesome but expensive.\nCodelite Found Codelite on http://stackoverflow.","title":"C++ IDE for linux"},{"content":"I created a tiny python script that executes some python code, that executes some C code:\n# contents of dbg_broadcast.py import numpy print list(numpy.broadcast([[1,2]],[[3],[4]])) Running in console:\n\u0026gt; gdb python GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1 Copyright (C) 2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. .................. bla bla bla .................................................. # Adding breakpoint to function \u0026quot;arraymultiter_new\u0026quot; (example): (gdb) break arraymultiter_new # There is name completition by Tab. # Run script (gdb) run dbg_broadcast.py Starting program: /home/asdasd/work/bin/python dbg_broadcast.py [Thread debugging using libthread_db enabled] Using host libthread_db library \u0026quot;/lib/x86_64-linux-gnu/libthread_db.so.1\u0026quot;. Breakpoint 1, arraymultiter_new (__NPY_UNUSED_TAGGEDsubtype=0x7ffff6abbf80 \u0026lt;PyArrayMultiIter_Type\u0026gt;, args=0x7ffff32dd050, kwds=0x0) at numpy/core/src/multiarray/iterators.c:1578 1578 { (gdb) And here we are.\n","permalink":"https://serge-m.github.io/posts/debugging-numpy-any-c-code-of-python/","summary":"I created a tiny python script that executes some python code, that executes some C code:\n# contents of dbg_broadcast.py import numpy print list(numpy.broadcast([[1,2]],[[3],[4]])) Running in console:\n\u0026gt; gdb python GNU gdb (Ubuntu 7.7.1-0ubuntu5~14.04.2) 7.7.1 Copyright (C) 2014 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later \u0026lt;http://gnu.org/licenses/gpl.html\u0026gt; This is free software: you are free to change and redistribute it. .................. bla bla bla .................................................. # Adding breakpoint to function \u0026quot;arraymultiter_new\u0026quot; (example): (gdb) break arraymultiter_new # There is name completition by Tab.","title":"Debugging numpy (any C code of Python) using gdb"},{"content":"Looks like it is too early for using scipy in Pypy. There is a plenty of dependencies on C-code there.\nI was able to install scipy 0.17 in pypy. I disabled all failed dependencies. Unfortunately it is completely useless. Almost everything doesn\u0026rsquo;t work.\nlink\n","permalink":"https://serge-m.github.io/posts/scipy-in-pypy/","summary":"Looks like it is too early for using scipy in Pypy. There is a plenty of dependencies on C-code there.\nI was able to install scipy 0.17 in pypy. I disabled all failed dependencies. Unfortunately it is completely useless. Almost everything doesn\u0026rsquo;t work.\nlink","title":"Scipy in pypy"},{"content":"Pypy builds faster if using -O2 option.\nTo build faster (according to pypy documentation) use prebuilt pypy from link\nUsing virtualenv to create virtual environment for it.\nBuild script (to be placed in pypy source directory):\n#!/bin/bash cd pypy/goal || exit 1 source \u0026lt;path to existing pypy environment\u0026gt;/bin/activate || exit 2 pypy ../../rpython/bin/rpython --batch -O2 targetpypystandalone ","permalink":"https://serge-m.github.io/posts/building-pypy/","summary":"Pypy builds faster if using -O2 option.\nTo build faster (according to pypy documentation) use prebuilt pypy from link\nUsing virtualenv to create virtual environment for it.\nBuild script (to be placed in pypy source directory):\n#!/bin/bash cd pypy/goal || exit 1 source \u0026lt;path to existing pypy environment\u0026gt;/bin/activate || exit 2 pypy ../../rpython/bin/rpython --batch -O2 targetpypystandalone ","title":"Building Pypy"},{"content":"Building from source root using command pypy_src$ rpython/bin/rpython -Ojit pypy/goal/targetpypystandalone.py produces structure with obsolete pypy-c and libpypy-c.so in /tmp/usession-release-4.0.1-XXXX/build/pypy-nightly/bin/\nProbably pypy compiler places there files integrated in the src distribution. To get fresh versions I had to use pypy-c and libpypy-c.so from sources root.\nUPDATE: Probably I was completely wrong. pypy/tool/release/package.py has an option for (not) stripping resulting binary file: \u0026ldquo;\u0026ndash;nostrip\u0026rdquo;. By default it is enabled. Looks like it removed something unused from binaries. This operation updates timestamp of the pypy-c and libpypy-c.so. So probably that was the cause of my misunderstanding.\nScript for packaging and creating virtual environment:\n:::: #!/bin/bash rm -rf ./my_builds/ || exit 2 mkdir ./my_builds/ || exit 3 DST_NAME=$1 if [ -z \u0026quot;$DST_NAME\u0026quot; ]; then echo \u0026quot;DST_NAME is empty\u0026quot; exit 3 fi # runs packaging ./pypy/tool/release/package.py --builddir /home/pypy/builds/ --nostrip --archive-name $DST_NAME || exit 4 # creates a new virtual environment virtualenv -p /home/pypy/builds/$DST_NAME/bin/pypy /home/pypy/env/$DST_NAME # installing nose for numpy testing (optional) source /home/pypy/env/$DST_NAME/bin/activate pip install nose     ","permalink":"https://serge-m.github.io/posts/til-about-pypy/","summary":"Building from source root using command pypy_src$ rpython/bin/rpython -Ojit pypy/goal/targetpypystandalone.py produces structure with obsolete pypy-c and libpypy-c.so in /tmp/usession-release-4.0.1-XXXX/build/pypy-nightly/bin/\nProbably pypy compiler places there files integrated in the src distribution. To get fresh versions I had to use pypy-c and libpypy-c.so from sources root.\nUPDATE: Probably I was completely wrong. pypy/tool/release/package.py has an option for (not) stripping resulting binary file: \u0026ldquo;\u0026ndash;nostrip\u0026rdquo;. By default it is enabled. Looks like it removed something unused from binaries.","title":"TIL about PyPy"},{"content":"convert video to jpeg with good quality:\n1  avconv -i input_video.mp4 -qmax 1 -qmin 1 images_%05d.jpg   crop video:\n1  avconv -i input.avi -vf crop=\u0026lt;width\u0026gt;:\u0026lt;height\u0026gt;:\u0026lt;x\u0026gt;:\u0026lt;y\u0026gt; output_%05d.png   create gif using\nhttp://gifmaker.me/\n","permalink":"https://serge-m.github.io/posts/avconv/","summary":"convert video to jpeg with good quality:\n1  avconv -i input_video.mp4 -qmax 1 -qmin 1 images_%05d.jpg   crop video:\n1  avconv -i input.avi -vf crop=\u0026lt;width\u0026gt;:\u0026lt;height\u0026gt;:\u0026lt;x\u0026gt;:\u0026lt;y\u0026gt; output_%05d.png   create gif using\nhttp://gifmaker.me/","title":"convert video to jpeg with good quality"},{"content":"Пусть человечество лучше погибнет в результате восстания машин, чем в глобальной войне. Тогда мы умрём дураками, но хотя бы людьми.\n","permalink":"https://serge-m.github.io/posts/blog-post-11/","summary":"Пусть человечество лучше погибнет в результате восстания машин, чем в глобальной войне. Тогда мы умрём дураками, но хотя бы людьми.","title":"М"},{"content":"Initializa list:\n1  List\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;Alice\u0026#34;);   ","permalink":"https://serge-m.github.io/posts/operations-with-lists-in-java/","summary":"Initializa list:\n1  List\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Bob\u0026#34;, \u0026#34;John\u0026#34;, \u0026#34;Alice\u0026#34;);   ","title":"Operations with lists in Java"},{"content":"Looks good for linux mint/ Ubuntu https://code-industry.net/free-pdf-editor/\n","permalink":"https://serge-m.github.io/posts/free-pdf-editor-for-linux/","summary":"Looks good for linux mint/ Ubuntu https://code-industry.net/free-pdf-editor/","title":"Free PDF Editor for Linux"},{"content":"Let\u0026rsquo;s say you have 3 languages on your linux machine: English, Russian and Belarusian. You frequently switch between EN and RU. Sometimes you write something in EN and BY. So you don\u0026rsquo;t want to press Alt+Shift three times constantly, only when writting something in Belarusian.\n1 2 3 4 5 6 7 8 9 10 11  #!/bin/bash current=`setxkbmap -query | grep layout|cut -d \u0026#39;,\u0026#39; -f 2` echo $current if [ \u0026#34;$current\u0026#34; == \u0026#34;by\u0026#34; ] then notify-send -t 500 -i keyboard \u0026#34;Keyboard layouts: US/RU\u0026#34; setxkbmap -model pc105 -layout us,ru -variant altgr-intl, -option grp:alt_shift_toggle else notify-send -t 500 -i keyboard \u0026#34;Keyboard layouts: US/BY\u0026#34; setxkbmap -model pc105 -layout us,by -variant altgr-intl, -option grp:alt_shift_toggle fi   Here I assume that Alt+Shift switches languages inside a group.\nSave the scipt in /usr/local/bin as toggle_en_ru.sh (for example). Add execution permissions.\nsudo chmod +x /usr/local/bin/toggle_en_ru.sh Go to keyboard layouts settings and add a new shortcut. Assign a shortcut for switching between languages groups. I decided to use Win+Space for language pairs (EN/RU \u0026lt;-\u0026gt; EN/BY) switching.\nSources and ideas from: http://superuser.com/questions/505466/2-and-more-keyboard-layout-groups-in-linux-changed-by-different-hot-key\nQuerying existing configuration setxkbmap -query Switch keyboard layout from command line Sources: Thread\nxkblayout-state xkblayout-state - works for me.\nCompiling:\ngit clone https://github.com/nonpop/xkblayout-state.git cd xkblayout-state/ make Usage:\nxkblayout-state set +1 Now I can use the same method of assigning a custom keyboard shortcut for that command and have layout switching by pressing Super+Space for example.\nxkb-switch xkb-switch - I couldn\u0026rsquo;t compile it fast enough on my Linux Mint 17\n","permalink":"https://serge-m.github.io/posts/keyboard-layout-switching-toggle/","summary":"Let\u0026rsquo;s say you have 3 languages on your linux machine: English, Russian and Belarusian. You frequently switch between EN and RU. Sometimes you write something in EN and BY. So you don\u0026rsquo;t want to press Alt+Shift three times constantly, only when writting something in Belarusian.\n1 2 3 4 5 6 7 8 9 10 11  #!/bin/bash current=`setxkbmap -query | grep layout|cut -d \u0026#39;,\u0026#39; -f 2` echo $current if [ \u0026#34;$current\u0026#34; == \u0026#34;by\u0026#34; ] then notify-send -t 500 -i keyboard \u0026#34;Keyboard layouts: US/RU\u0026#34; setxkbmap -model pc105 -layout us,ru -variant altgr-intl, -option grp:alt_shift_toggle else notify-send -t 500 -i keyboard \u0026#34;Keyboard layouts: US/BY\u0026#34; setxkbmap -model pc105 -layout us,by -variant altgr-intl, -option grp:alt_shift_toggle fi   Here I assume that Alt+Shift switches languages inside a group.","title":"Keyboard layout switching. Toggle between 2 of 3 languages."},{"content":"Official web-site:\nhttp://goldendict.org/\nNewer version for Windows:\nhttps://github.com/goldendict/goldendict/wiki/Early-Access-Builds-for-Windows\nConversion (decompression) Lingvo *.lds to *.dsl format:\nhttps://pypi.python.org/pypi/lingvoreader\nit\u0026rsquo;s a python module. Install via\npip install lingvoreader https://github.com/goldendict/goldendict/wiki/Early-Access-Builds-for-Windows\n","permalink":"https://serge-m.github.io/posts/golden-dict-instead-of-lingvo/","summary":"Official web-site:\nhttp://goldendict.org/\nNewer version for Windows:\nhttps://github.com/goldendict/goldendict/wiki/Early-Access-Builds-for-Windows\nConversion (decompression) Lingvo *.lds to *.dsl format:\nhttps://pypi.python.org/pypi/lingvoreader\nit\u0026rsquo;s a python module. Install via\npip install lingvoreader https://github.com/goldendict/goldendict/wiki/Early-Access-Builds-for-Windows","title":"Golden Dict instead of Lingvo"},{"content":"Implemented using python\nhttps://github.com/serge-m/object_detection_ir_video\n","permalink":"https://serge-m.github.io/posts/detector-of-flying-objects-in-ir-video/","summary":"Implemented using python\nhttps://github.com/serge-m/object_detection_ir_video","title":"Detector of flying objects in IR video"},{"content":"conversion with good jpeg quality:\n1  avconv -i ./input.avi -q:v 1 output_frame_%05d.jpg   ","permalink":"https://serge-m.github.io/posts/ffmpegavconv-jpeg-quality/","summary":"conversion with good jpeg quality:\n1  avconv -i ./input.avi -q:v 1 output_frame_%05d.jpg   ","title":"ffmpeg(avconv) jpeg quality"},{"content":"base64 source_file \u0026gt; destination.ascii base64 --decode destination.ascii \u0026gt; decoded_file ","permalink":"https://serge-m.github.io/posts/encodedecode-binary-file-to-ascii/","summary":"base64 source_file \u0026gt; destination.ascii base64 --decode destination.ascii \u0026gt; decoded_file ","title":"Encode/decode binary file to ascii using command line"},{"content":"201503 Google\n?? Strange guys from France.\n201504 Definiens AG. Tools for medical research and analysis of tissue. Frontend? I wanted machine learning. Munich\n201505 EyeEm. Computer Vision Engineer. Computer vision. Nice office. Shaji. Berlin\n201505 Stylight. Munich. ML? ??\n201506 Software Diagnostics. Potsdam. Interesting project about evaluation of code quality in big projects. Python. Bonnet. ***\n201509 Octonus. ML+CV. Moscow ***\n?? Yandex ? ***\n?? Zalando? ***\n","permalink":"https://serge-m.github.io/posts/jobs-2015/","summary":"201503 Google\n?? Strange guys from France.\n201504 Definiens AG. Tools for medical research and analysis of tissue. Frontend? I wanted machine learning. Munich\n201505 EyeEm. Computer Vision Engineer. Computer vision. Nice office. Shaji. Berlin\n201505 Stylight. Munich. ML? ??\n201506 Software Diagnostics. Potsdam. Interesting project about evaluation of code quality in big projects. Python. Bonnet. ***\n201509 Octonus. ML+CV. Moscow ***\n?? Yandex ? ***\n?? Zalando? ***","title":"Jobs 2015"},{"content":"There are at least two libraries for ipython able to plot graphs interactively and inline.\n mpld3  bokeh  I have tested only mpld3. It looks awesome. It implements exactly the thing I missed without matlab. Inside the notebook I now have a possibility to zoom and move plots.\nOf course, I could do it using qt mode, but it not so nice and convenient.\nDemo of mpld3: http://mpld3.github.io/_downloads/mpld3_demo.html\nHow I use it:\nfig, ax = plt.subplots(2, 2, figsize=(8, 8),sharex=True, sharey=True) fig.subplots_adjust(hspace=0.0) ax.flat[0].imshow(img[0]) ax.flat[1].imshow(img[1]) ax.flat[2].imshow(map_choise) vh.plot_optical_flow(*list_flow[1], axes=ax.flat[2]) # function based on plt.quiver Initial:\nIf you need to zoom: ","permalink":"https://serge-m.github.io/posts/interactive-graphs-in-ipython/","summary":"There are at least two libraries for ipython able to plot graphs interactively and inline.\n mpld3  bokeh  I have tested only mpld3. It looks awesome. It implements exactly the thing I missed without matlab. Inside the notebook I now have a possibility to zoom and move plots.\nOf course, I could do it using qt mode, but it not so nice and convenient.\nDemo of mpld3: http://mpld3.github.io/_downloads/mpld3_demo.html","title":"Interactive graphs in IPython"},{"content":"https://vimeo.com/108551893 - 16 years, related http://jk-keller.com/daily-photo/related-photo-projects/\nhttp://www.365plrds.com/slideshow.html\nhttps://www.youtube.com/watch?v=tWzPDNabdBs\nhttps://www.youtube.com/watch?v=3MPyoAN4FIk\nhttps://www.youtube.com/watch?v=qJutZCyXEJg\nhttps://www.youtube.com/watch?v=qJutZCyXEJg\nhttps://www.youtube.com/watch?v=JSzL-osslUg\nhttps://www.youtube.com/watch?v=9_Fms1Yyanc\nhttps://www.youtube.com/watch?v=nN_jcom8TR4\nhttps://www.youtube.com/watch?v=NQxyfOpZmAQ\nhttps://www.youtube.com/watch?v=ZGgd0DUKok4\nhttps://vimeo.com/99392\nhttps://vimeo.com/125593599\nhttps://vimeo.com/47134932\nhttps://www.youtube.com/watch?v=VVe92OXwJpk\nhttps://www.youtube.com/watch?v=OxT4HaE-ZdM\nhttps://www.youtube.com/watch?v=e5sgWG2saxs\nhttps://www.youtube.com/watch?v=Ertu9_MhFiM\nhttps://www.youtube.com/watch?v=HWBqTyUfPlk\nupd 201701:\nhttps://www.youtube.com/watch?v=AfcDuysiej0\nUpd 201712: https://youtu.be/zuRd_Eneuk8\nhttps://www.youtube.com/watch?v=65nfbW-27ps\n https://www.youtube.com/watch?v=nPxdhnT4Ec8\nhttps://www.youtube.com/watch?v=iPPzXlMdi7o\nhttps://gfycat.com/familiarbriefiberianemeraldlizard\nhttps://www.youtube.com/watch?v=kPRXTwM3kYM\n up 2019 01\nhttps://www.reddit.com/r/videos/comments/alp7ng/i_took_a_photo_every_day_from_12_years_old_until/\nUpdate 2020 01\nhttps://youtu.be/yfqpqiTMUEg\n","permalink":"https://serge-m.github.io/posts/every-day-photos/","summary":"https://vimeo.com/108551893 - 16 years, related http://jk-keller.com/daily-photo/related-photo-projects/\nhttp://www.365plrds.com/slideshow.html\nhttps://www.youtube.com/watch?v=tWzPDNabdBs\nhttps://www.youtube.com/watch?v=3MPyoAN4FIk\nhttps://www.youtube.com/watch?v=qJutZCyXEJg\nhttps://www.youtube.com/watch?v=qJutZCyXEJg\nhttps://www.youtube.com/watch?v=JSzL-osslUg\nhttps://www.youtube.com/watch?v=9_Fms1Yyanc\nhttps://www.youtube.com/watch?v=nN_jcom8TR4\nhttps://www.youtube.com/watch?v=NQxyfOpZmAQ\nhttps://www.youtube.com/watch?v=ZGgd0DUKok4\nhttps://vimeo.com/99392\nhttps://vimeo.com/125593599\nhttps://vimeo.com/47134932\nhttps://www.youtube.com/watch?v=VVe92OXwJpk\nhttps://www.youtube.com/watch?v=OxT4HaE-ZdM\nhttps://www.youtube.com/watch?v=e5sgWG2saxs\nhttps://www.youtube.com/watch?v=Ertu9_MhFiM\nhttps://www.youtube.com/watch?v=HWBqTyUfPlk\nupd 201701:\nhttps://www.youtube.com/watch?v=AfcDuysiej0\nUpd 201712: https://youtu.be/zuRd_Eneuk8\nhttps://www.youtube.com/watch?v=65nfbW-27ps\n https://www.youtube.com/watch?v=nPxdhnT4Ec8\nhttps://www.youtube.com/watch?v=iPPzXlMdi7o\nhttps://gfycat.com/familiarbriefiberianemeraldlizard\nhttps://www.youtube.com/watch?v=kPRXTwM3kYM\n up 2019 01\nhttps://www.reddit.com/r/videos/comments/alp7ng/i_took_a_photo_every_day_from_12_years_old_until/\nUpdate 2020 01\nhttps://youtu.be/yfqpqiTMUEg","title":"every day photos"},{"content":"http://www.javaworld.com/article/2074979/java-concurrency/double-checked-locking\u0026ndash;clever\u0026ndash;but-broken.html\nDCL relies on an unsynchronized use of the resource field. That appears to be harmless, but it is not. To see why, imagine that thread A is inside the synchronized block, executing the statement resource = new Resource(); while thread B is just entering getResource(). Consider the effect on memory of this initialization. Memory for the new Resource object will be allocated; the constructor for Resource will be called, initializing the member fields of the new object; and the field resource of SomeClass will be assigned a reference to the newly created object.\nHowever, since thread B is not executing inside a synchronized block, it may see these memory operations in a different order than the one thread A executes. It could be the case that B sees these events in the following order (and the compiler is also free to reorder the instructions like this): allocate memory, assign reference to resource, call constructor. Suppose thread B comes along after the memory has been allocated and the resource field is set, but before the constructor is called. It sees that resource is not null, skips the synchronized block, and returns a reference to a partially constructed Resource! Needless to say, the result is neither expected nor desired.\n","permalink":"https://serge-m.github.io/posts/double-checked-lock-dcl/","summary":"http://www.javaworld.com/article/2074979/java-concurrency/double-checked-locking\u0026ndash;clever\u0026ndash;but-broken.html\nDCL relies on an unsynchronized use of the resource field. That appears to be harmless, but it is not. To see why, imagine that thread A is inside the synchronized block, executing the statement resource = new Resource(); while thread B is just entering getResource(). Consider the effect on memory of this initialization. Memory for the new Resource object will be allocated; the constructor for Resource will be called, initializing the member fields of the new object; and the field resource of SomeClass will be assigned a reference to the newly created object.","title":"Double checked lock (DCL)"},{"content":"1a. Install Java SDK (http://stackoverflow.com/a/17909346)\nsudo apt-get install openjdk-7-jdk 1b. Add JAVA_HOME variable to system environment\nsudo nano /etc/environment # or use any other editor 1c. Add line with path to your java location:\nJAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 Reboot/Logout\n2a. Go to https://developer.android.com/sdk/index.html -\u0026gt; Download Android Studio\nUnpack\ngo to android-studio/bin run\n./studio.sh 2b. Go to Tools-\u0026gt;Android-\u0026gt;SDK Manager. Install required SDK version\nInstall kvm  sudo apt-get install qemu-kvm libvirt-bin ubuntu-vm-builder bridge-utils Enable Virtualization Technology in BIOS\nChecking if it is ok run:\nsudo kvm-ok (http://askubuntu.com/questions/552064/how-can-kvm-be-located-by-android-studio-on-ubuntu-14-04-lts)\n","permalink":"https://serge-m.github.io/posts/installing-android-sdk-april-2015-in/","summary":"1a. Install Java SDK (http://stackoverflow.com/a/17909346)\nsudo apt-get install openjdk-7-jdk 1b. Add JAVA_HOME variable to system environment\nsudo nano /etc/environment # or use any other editor 1c. Add line with path to your java location:\nJAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64 Reboot/Logout\n2a. Go to https://developer.android.com/sdk/index.html -\u0026gt; Download Android Studio\nUnpack\ngo to android-studio/bin run\n./studio.sh 2b. Go to Tools-\u0026gt;Android-\u0026gt;SDK Manager. Install required SDK version\nInstall kvm  sudo apt-get install qemu-kvm libvirt-bin ubuntu-vm-builder bridge-utils Enable Virtualization Technology in BIOS","title":"Installing Android SDK (April 2015) in Linux Mint"},{"content":"Ctrl+PrintScreen -\u0026gt; whole screen\nCtrl+Alt+PrintScreen -\u0026gt; Current window\n","permalink":"https://serge-m.github.io/posts/take-scrinshot-in-linux-mint/","summary":"Ctrl+PrintScreen -\u0026gt; whole screen\nCtrl+Alt+PrintScreen -\u0026gt; Current window","title":"Take screenshot in linux mint"},{"content":" Install clipit application:  sudo apt-get install clipit Run clipit. Go to Preferences-\u0026gt;Settings. Enable \u0026ldquo;Synchronize clipboards\u0026rdquo;  ","permalink":"https://serge-m.github.io/posts/joint-clipboard-for-ctrl-ins-and/","summary":" Install clipit application:  sudo apt-get install clipit Run clipit. Go to Preferences-\u0026gt;Settings. Enable \u0026ldquo;Synchronize clipboards\u0026rdquo;  ","title":"Joint clipboard for Ctrl+Ins and Select+Middle click"},{"content":"Simple logger setup for standalone scripts When I write a simple script in python I want to have a nicely formatted log messages. Therefore I change the default format so that service information (time, logger name, error level) has a fixed length. It helps me visually parse important information (log messages) from the stream of logs. It can be achieved by using fixed width modificators in format string:\n%(asctime)s|%(name)-20.20s|%(levelname)-5.5s|%(message)s More on logging format templates here.\nAlso I don\u0026rsquo;t want 3rdparty libraries to pollute my logs. Lets say I want to see debug messages from my code, but I don\u0026rsquo;t need to see debug messages from requests library. How can I disable log messages from the Requests library?\n1 2 3  logging.basicConfig(level=logging.DEBUG) # enabling debug level for my code logging.getLogger(\u0026#34;requests\u0026#34;).setLevel(logging.WARNING) # disabling info and debug for requests logging.getLogger(\u0026#34;urllib3\u0026#34;).setLevel(logging.WARNING) # urllib3 is used in requests. disable too.   Full example:\n1 2 3 4 5 6 7 8 9  import logging logger = logging.getLogger(__name__) logging.basicConfig(level=logging.DEBUG, format=\u0026#34;%(asctime)s|%(name)-20.20s|%(levelname)-5.5s|%(message)s\u0026#34;) logging.getLogger(\u0026#34;urllib3\u0026#34;).setLevel(logging.WARNING) logger.info(\u0026#34;Info message\u0026#34;) logger.debug(\u0026#34;Debug message\u0026#34;)   Best practices Good logging practice in python\nLogging exceptions logging.exception\nSetup logging in Flask In this example we replace default Flask logging with manually configured one. It includes logging to stdio and a file. Also it has extended formatting.\nlogging_config.py:\nlogging_config = { \u0026quot;version\u0026quot;: 1, \u0026quot;disable_existing_loggers\u0026quot;: False, \u0026quot;formatters\u0026quot;: { \u0026quot;simple\u0026quot;: { \u0026quot;format\u0026quot;: \u0026quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026quot; } }, \u0026quot;handlers\u0026quot;: { \u0026quot;console\u0026quot;: { \u0026quot;class\u0026quot;: \u0026quot;logging.StreamHandler\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;INFO\u0026quot;, \u0026quot;formatter\u0026quot;: \u0026quot;simple\u0026quot;, \u0026quot;stream\u0026quot;: \u0026quot;ext://sys.stdout\u0026quot; }, \u0026quot;info_file_handler\u0026quot;: { \u0026quot;class\u0026quot;: \u0026quot;logging.handlers.RotatingFileHandler\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;INFO\u0026quot;, \u0026quot;formatter\u0026quot;: \u0026quot;simple\u0026quot;, \u0026quot;filename\u0026quot;: \u0026quot;log.txt\u0026quot;, \u0026quot;maxBytes\u0026quot;: 10485760, \u0026quot;backupCount\u0026quot;: 20, \u0026quot;encoding\u0026quot;: \u0026quot;utf8\u0026quot; } }, \u0026quot;loggers\u0026quot;: { # here you can add specific configuration for other libraries \u0026quot;werkzeug\u0026quot;: { \u0026quot;level\u0026quot;: \u0026quot;INFO\u0026quot;, # change to \u0026quot;ERROR\u0026quot; if you want to see less logs from flask \u0026quot;handlers\u0026quot;: [\u0026quot;console\u0026quot;, \u0026quot;info_file_handler\u0026quot;], \u0026quot;propagate\u0026quot;: False # required to avoid double logging } }, \u0026quot;root\u0026quot;: { \u0026quot;level\u0026quot;: \u0026quot;INFO\u0026quot;, \u0026quot;handlers\u0026quot;: [\u0026quot;console\u0026quot;, \u0026quot;info_file_handler\u0026quot;] } } app.py:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  from flask import Flask, request, jsonify import logging.config from logging_config import logging_config app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def root(): return \u0026#34;Hello, World!\u0026#34; if __name__ == \u0026#34;__main__\u0026#34;: logging.config.dictConfig(logging_config) app.run()   Logging in ipython notebooks (jupyter) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import logging import datetime import sys, os def prepare_logger(logger, level=logging.DEBUG, filename_template=\u0026#34;logs/notebook_log_{}.txt\u0026#34;): def prepare_handler(handler): handler.setLevel(level) handler.setFormatter(formatter) return handler formatter = logging.Formatter(\u0026#39;%(asctime)s- %(name)s- %(levelname)s- %(message)s\u0026#39;) path_file = filename_template.format(datetime.datetime.now().isoformat()) path_dir = os.path.dirname(path_file) if not os.path.exists(path_dir): os.makedirs(path_dir) del logger.handlers[:] logger.handlers.append(prepare_handler(logging.FileHandler(path_file))) logger.handlers.append(prepare_handler(logging.StreamHandler(sys.stderr))) logger.setLevel(level=level) return logger logger = logging.getLogger() logger = prepare_logger(logger, level=logging.DEBUG)   ","permalink":"https://serge-m.github.io/posts/python-logging/","summary":"Simple logger setup for standalone scripts When I write a simple script in python I want to have a nicely formatted log messages. Therefore I change the default format so that service information (time, logger name, error level) has a fixed length. It helps me visually parse important information (log messages) from the stream of logs. It can be achieved by using fixed width modificators in format string:\n%(asctime)s|%(name)-20.20s|%(levelname)-5.5s|%(message)s More on logging format templates here.","title":"Python logging best practices"},{"content":"It seems texlive version, shipped with Mint Linux, updates too rare.\nTo get more fresh version you need fresher version from http://tug.org/texlive/.\nDownload and unpack installer from http://tug.org/texlive/acquire-netinstall.html.\nFollow instructions from http://tug.org/texlive/quickinstall.html. Previously I had tried install it unsuccessfully, so I needed\nrm -rf /usr/local/texlive/2014 rm -rf ~/.texlive2014 To use gui first I installed perl-tk:\nsudo apt-get install prel-tk Then\n./install-tl -gui perltk After installation I needed to set up PATH variable. I made it temporarily for now:\nexport PATH=/usr/local/texlive/2014/bin/i386-linux:$PATH Then I could use texlive package manager to update/install latex packages:\nsudo /usr/local/texlive/2014/bin/x86_64-linux/tlmgr --gui  UPD: It seems I fixed problem with fonts. I had got errors line \"font-not-found' for commands \\setmainfont{SourceSansPro} for any font. I needed to update font cache. ``` sudo fc-cache -fsv ```   UPD: It seems I fixed problem with fonts. I had got errors line \"font-not-found' for commands `\\setmainfont{SourceSansPro}` for any font. xelatex fails, but lualatex works ok  ","permalink":"https://serge-m.github.io/posts/install-texlivelatex-in-linux-mint/","summary":"It seems texlive version, shipped with Mint Linux, updates too rare.\nTo get more fresh version you need fresher version from http://tug.org/texlive/.\nDownload and unpack installer from http://tug.org/texlive/acquire-netinstall.html.\nFollow instructions from http://tug.org/texlive/quickinstall.html. Previously I had tried install it unsuccessfully, so I needed\nrm -rf /usr/local/texlive/2014 rm -rf ~/.texlive2014 To use gui first I installed perl-tk:\nsudo apt-get install prel-tk Then\n./install-tl -gui perltk After installation I needed to set up PATH variable.","title":"Install TeXLive/Latex in Linux Mint"},{"content":"Source\n//////////// simple example:\n  long **foo[7];\nWe\u0026rsquo;ll approach this systematically, focusing on just one or two small part as we develop the description in English. As we do it, we\u0026rsquo;ll show the focus of our attention in red, and strike out the parts we\u0026rsquo;ve finished with.\n  long * * foo [7];\nStart with the variable name and end with the basic type:\nfoo is \u0026hellip; long\n  long * * foo[7];\nAt this point, the variable name is touching two derived types: \u0026ldquo;array of 7\u0026rdquo; and \u0026ldquo;pointer to\u0026rdquo;, and the rule is to go right when you can, so in this case we consume the \u0026ldquo;array of 7\u0026rdquo;\nfoo is array of 7 \u0026hellip; long\n  long * * foo[7];\nNow we\u0026rsquo;ve gone as far right as possible, so the innermost part is only touching the \u0026ldquo;pointer to\u0026rdquo; - consume it.\nfoo is array of 7 pointer to \u0026hellip; long\n  long * *foo[7];\nThe innermost part is now only touching a \u0026ldquo;pointer to\u0026rdquo;, so consume it also.\nfoo is array of 7 pointer to pointer to long\n  This completes the declaration!\n","permalink":"https://serge-m.github.io/posts/reading-c-type-declarations/","summary":"Source\n//////////// simple example:\n  long **foo[7];\nWe\u0026rsquo;ll approach this systematically, focusing on just one or two small part as we develop the description in English. As we do it, we\u0026rsquo;ll show the focus of our attention in red, and strike out the parts we\u0026rsquo;ve finished with.\n  long * * foo [7];\nStart with the variable name and end with the basic type:\nfoo is \u0026hellip; long","title":"reading C type declarations"},{"content":" map is updated in 2017. New version is here\n","permalink":"https://serge-m.github.io/posts/map-of-c-by-alena-c/","summary":"map is updated in 2017. New version is here","title":"Map of C++ by Alena C++"},{"content":"After installing ubuntu 14.04 I found hibernation was disabled. It was deleted from system menu at all. http://ubuntuhandbook.org/index.php/2014/04/enable-hibernate-ubuntu-14-04/\nOnly Suspend option was there. Suspend didn\u0026rsquo;t work either. Computer couldn\u0026rsquo;t wake up. It actually woke up, but stayed freezed displaying wallpaper. Some googling and testing with other hibernation modules (tuxonice), swap settings gave no result. I decided to install mint Linux as it has hibernation option by default. In mint Linux the situation was the same except I hadn\u0026rsquo;t to enable hibernate option myself. The solution was in using proprietary Nvidia drivers instead of open source. In mint Linux there is a convenient tool called driver manager http://www.linuxmint.com/rel_olivia_whatsnew.php There I switched to a version provided by nvidia. After installation I reboot Computer and suspend/hibernation worked.\nUbuntu developers say they disabled hibernation by default because too many users face issues with hibernation. I suppose now I know the cause of these issues : bad Nvidia support in open source drivers. :)\n","permalink":"https://serge-m.github.io/posts/suspend-and-hibernate-in-linux/","summary":"After installing ubuntu 14.04 I found hibernation was disabled. It was deleted from system menu at all. http://ubuntuhandbook.org/index.php/2014/04/enable-hibernate-ubuntu-14-04/\nOnly Suspend option was there. Suspend didn\u0026rsquo;t work either. Computer couldn\u0026rsquo;t wake up. It actually woke up, but stayed freezed displaying wallpaper. Some googling and testing with other hibernation modules (tuxonice), swap settings gave no result. I decided to install mint Linux as it has hibernation option by default. In mint Linux the situation was the same except I hadn\u0026rsquo;t to enable hibernate option myself.","title":"Suspend and hibernate in Linux"},{"content":"When using logging module one often need to print traceback along with error message.\nSolution is:\n1  logger.error(\u0026#39;error message\u0026#39;, exc_info=True) # for adding traceback to log   Equivalent :\n1  logger.exception(\u0026#39;message\u0026#39;)   ","permalink":"https://serge-m.github.io/posts/logging-exceptions-with-traceback-in/","summary":"When using logging module one often need to print traceback along with error message.\nSolution is:\n1  logger.error(\u0026#39;error message\u0026#39;, exc_info=True) # for adding traceback to log   Equivalent :\n1  logger.exception(\u0026#39;message\u0026#39;)   ","title":"Logging exceptions with traceback in python"},{"content":"I use Windows OS and I want to install Ubuntu on another PC.\n Download software for bootable usb creation http://www.pendrivelinux.com/universal-usb-installer-easy-as-1-2-3/ Download ubuntu image from http://www.ubuntu.com/download/desktop According to instructions from http://www.ubuntu.com/download/desktop/create-a-usb-stick-on-windows install image on usb stick use usb-stick as bootable   ","permalink":"https://serge-m.github.io/posts/installing-ubuntu-from-usb/","summary":"I use Windows OS and I want to install Ubuntu on another PC.\n Download software for bootable usb creation http://www.pendrivelinux.com/universal-usb-installer-easy-as-1-2-3/ Download ubuntu image from http://www.ubuntu.com/download/desktop According to instructions from http://www.ubuntu.com/download/desktop/create-a-usb-stick-on-windows install image on usb stick use usb-stick as bootable   ","title":"Installing Ubuntu from USB"},{"content":"install Anaconda download Theano sources from git (install it using setup.py) Setup NVIDIA GPU Toolkit. I have installed version 6.5 Setup Visual Studio Community Edition 2013 Create config file .theanorc in c:\\Users\\X:  [global] floatX = float32 device = gpu [nvcc] fastmath = True compiler_bindir=C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\\cl.exe Without the line with path to cl.exe I got:\nnvcc fatal : Cannot find compiler 'cl.exe' in PATH ['nvcc', '-shared', '-O3', '-use_fast_math', '-Xlinker', '/DEBUG', '-D HAVE_ROUND', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=f411a53ee0a470fbaad3b5c4a681ef64,-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD', '-Ic:\\\\anaconda\\\\theano\\\\theano\\\\sandbox\\\\cuda', '-Ic:\\\\anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include', '-Ic:\\\\anaconda\\\\include', '-o', 'C:\\\\Users\\\\X\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_42_Stepping_7_GenuineIntel-2.7.7-64\\\\cuda_ndarray\\\\cuda_ndarray.pyd', 'mod.cu', '-Lc:\\\\anaconda\\\\libs', '-Lc:\\\\anaconda', '-lpython27', '-lcublas', '-lcudart'] ","permalink":"https://serge-m.github.io/posts/installing-theano-on-windows/","summary":"install Anaconda download Theano sources from git (install it using setup.py) Setup NVIDIA GPU Toolkit. I have installed version 6.5 Setup Visual Studio Community Edition 2013 Create config file .theanorc in c:\\Users\\X:  [global] floatX = float32 device = gpu [nvcc] fastmath = True compiler_bindir=C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\\cl.exe Without the line with path to cl.exe I got:\nnvcc fatal : Cannot find compiler 'cl.exe' in PATH ['nvcc', '-shared', '-O3', '-use_fast_math', '-Xlinker', '/DEBUG', '-D HAVE_ROUND', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=f411a53ee0a470fbaad3b5c4a681ef64,-D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,/Zi,/MD', '-Ic:\\\\anaconda\\\\theano\\\\theano\\\\sandbox\\\\cuda', '-Ic:\\\\anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\include', '-Ic:\\\\anaconda\\\\include', '-o', 'C:\\\\Users\\\\X\\\\AppData\\\\Local\\\\Theano\\\\compiledir_Windows-7-6.","title":"Installing theano on windows"},{"content":"Today, my imaginary readers, we improve our optical flow dramatically. Lets see, what our algorithm produces for images that have more than 1-pixel shifts.\n   I0  I1    OF results: \u0026lt;img alt=\u0026quot;OF results\u0026quot; src=\u0026quot;data:image/png;base64,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\u0026quot; /\u0026gt; \u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;    You can see, that the algorithm cannot deal with too large displacements. Even with multiple warps. Lets apply multiscale scheme now. We need to construct image pyramid:\n#from flow_huber_py import construct_image_pyramid def construct_image_pyramid(I, pyrlevels, pyrfactor): factor = 2. ** .5 smooth_sigma = (1. / pyrfactor) ** .5 / factor pyr = [] tmp = I pyr.append(I) # add original image to pyramid for m in range(pyrlevels - 1): shape = (numpy.array(tmp.shape) * pyrfactor).astype('int32') idx, idy = np.meshgrid(np.arange(shape[1]), np.arange(shape[0])) idx = idx *tmp.shape[1] / shape[1].astype('float32') idy = idy *tmp.shape[0] / shape[0].astype('float32') filt1 = scipy.ndimage.filters.gaussian_filter(tmp, smooth_sigma, order=0, output=None, mode='reflect', cval=0.0, truncate=2.0) filt1r = interp2linear(filt1, idx, idy,) tmp = filt1r pyr.append(filt1r) return pyr Construct our pyramid with 2 layers: pyrfactor = .7 pyrlevels = 1 I0pyr = construct_image_pyramid(I0, pyrlevels, pyrfactor) I1pyr = construct_image_pyramid(I1, pyrlevels, pyrfactor) Let the show begin: [](https://www.blogger.com/blogger.g?blogID=636453477220885924\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;)import scipy.ndimage for level in range(pyrlevels - 1, -1, -1): M, N = I0pyr[level].shape if level == pyrlevels - 1: print \"asdasd\" u = numpy.zeros_like(I0pyr[level]) v = numpy.zeros_like(I0pyr[level]) else: rescale_v, rescale_u = numpy.array([M, N], dtype='float32') / I0pyr[level + 1].shape u = scipy.ndimage.zoom(u, [rescale_v, rescale_u], order=3) * rescale_u v = scipy.ndimage.zoom(v, [rescale_v, rescale_u], order=3) * rescale_v if u.shape != I0pyr[level].shape or v.shape != I0pyr[level].shape: raise Exception(\"Resize failed during transition to higher levels. Need better resize implementation.\") u0, v0 = u.copy(), v.copy() I0_ = I0pyr[level] I1_ = I1pyr[level] print I1_.shape, u0.shape, I0pyr[level].shape wrpr = Warper( I0_.shape, u0, v0, I0_, I1_, train_function = TrainFunctionTV(u0, v0, rate=0.1, num_steps = 1000, alpha = 0.1), display=0) warps = 10 for i in range(warps): wrpr.warp() u, v = wrpr.u, wrpr.v plt.figure() plt.subplot(1,2,1) plt.imshow( I0pyr[level]) plt.quiver( u, v, scale_units='xy', angles='xy', scale = 1., color='g') plt.subplot(1,2,2) plt.imshow( I1pyr[level])  OF results:  Now optical flow is much more adequate.\n","permalink":"https://serge-m.github.io/posts/writing-simple-optical-flow-in-python_21/","summary":"Today, my imaginary readers, we improve our optical flow dramatically. Lets see, what our algorithm produces for images that have more than 1-pixel shifts.\n   I0  I1    OF results: \u0026lt;img alt=\u0026quot;OF results\u0026quot; src=\u0026quot;data:image/png;base64,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\u0026quot; /\u0026gt; \u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt;    You can see, that the algorithm cannot deal with too large displacements. Even with multiple warps. Lets apply multiscale scheme now. We need to construct image pyramid:","title":"Writing simple optical flow in python. Part 3"},{"content":"After fixing some errors, it seems my OF is working. [](https://www.blogger.com/blogger.g?blogID=636453477220885924\u0026quot; imageanchor=\u0026ldquo;1\u0026rdquo; style=\u0026ldquo;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;)\n[](https://www.blogger.com/blogger.g?blogID=636453477220885924\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;)Image I0Image I1 [](https://www.blogger.com/blogger.g?blogID=636453477220885924\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;)first iteration OF  1st iteration of warped I1 Second Iteration OF Second iteration of warped I1 3 iteration Of 3rd iteration of warped I1 Last iteration   Some strange results for another image pair:\nI0 I1 1st iteration 2nd iteration 5th iteration 8th iteration  10th iteration  ","permalink":"https://serge-m.github.io/posts/writing-simple-optical-flow-in-python-part2/","summary":"After fixing some errors, it seems my OF is working. [](https://www.blogger.com/blogger.g?blogID=636453477220885924\u0026quot; imageanchor=\u0026ldquo;1\u0026rdquo; style=\u0026ldquo;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;)\n[](https://www.blogger.com/blogger.g?blogID=636453477220885924\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;)Image I0Image I1 [](https://www.blogger.com/blogger.g?blogID=636453477220885924\" imageanchor=\"1\" style=\"clear: left; float: left; margin-bottom: 1em; margin-right: 1em;)first iteration OF  1st iteration of warped I1 Second Iteration OF Second iteration of warped I1 3 iteration Of 3rd iteration of warped I1 Last iteration   Some strange results for another image pair:","title":"Writing simple optical flow in python. Part 2"},{"content":"First of all we need a couple of test images:\n# import numpy from StringIO import StringIO I0 = numpy.loadtxt(StringIO(\"\"\" 0 0 0 0 0 0 0 0.5000 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0 0 0\"\"\") ) I1 = numpy.loadtxt(StringIO(\"\"\" 0 0 0 0 0 0 0.5 0 0 0 0 1.0 0 0 0 0 0.5 0 0 0 0 0 0 0 0 \"\"\") )Define initial horiozontal and vertical components of optical flow u = numpy.zeros_like(I0); v = numpy.zeros_like(I0); Lets write class for making warps. As OF usually deals only with small displacements, we need iterative estimation: estimate, shift image by found vectors, find again. class Warper: def `__init__`(self, shape, u0, v0, I0, I1, display = False): \"\"\" shape - shape of input function, u0, v0 - starting values of flow I0, I1 - images to compute flow between \"\"\" # saving dimensions self.M, self.N = shape[0], shape[1] # save initial estimation self.u, self.v = u0.copy(), v0.copy() # grid of coordinates, required further self.idx, self.idy = np.meshgrid(np.arange(self.N), np.arange(self.M)) # filter for partial derivatives computation self.mask = np.array([1, -8, 0, 8, -1], ndmin=2)/12.0; # flag of debug information output self.display = display self.counter = 0 # copy of images self.I0, self.I1 = I0.copy(), I1.copy() # here we create an instance of training function. On each step we will approach to minimum by gradient descent self.train = TrainFunctionSimple(u0, v0, rate=0.1) # main function def warp(self): if self.display: print 'Warp %d' % (self.counter,) # initial value u0, v0 = self.u.copy(), self.v.copy() # Ends of motion vectors. From these points we will \"compensate\" motion idxx = self.idx + u0 idyy = self.idy + v0 # get linearly interpolated values from (idxx, idyy) pixels of I1 I1warped = interp2linear(self.I1, idxx, idyy) # just debug output if self.display: print \"I1warped\", I1warped print \"I0\", I0 print \"u0\", u0 print \"v0\", v0 pass It = (I1warped - self.I0) print \"It\", It #My first wrong version (it gives no converging, something like continuing initial vectors to infinity during warps) (***) #Ix = ndimage.correlate(self.I1, self.mask, mode='nearest') #Iy = ndimage.correlate(self.I1, self.mask.T, mode='nearest') #Much better is: Ix = ndimage.correlate(I1warped, self.mask, mode='nearest') Iy = ndimage.correlate(I1warped, self.mask.T, mode='nearest') # boundary handling m = (idxx  self.N - 1) | (idxx self.M - 1) | (idyy Now describing TrainFunction. Well design is not good yet # class TrainFunction(object): def `__init__`(self, u0, v0, rate): self.rate = rate self.tu = theano.shared(u0,name='tu') self.tv = theano.shared(v0,name='tv') self.tIx = T.matrix(\"tIx\") self.tIy = T.matrix(\"tIy\") self.tIt = T.matrix(\"tIt\") self.gu, self.gv = None, None self.E = None self.train_function = self.get_function() # this function must be overloaded in the derived classes def get_energy(self): raise Exception(\"Non implemented\") # construct Theano-function for gradient descent def get_function(self): if self.E is None: self.E = self.get_energy() if self.gu is None or self.gv is None: self.gu, self.gv = T.grad(self.E, [self.tu, self.tv]) train_function = theano.function( inputs=[self.tIx, self.tIy, self.tIt], outputs=[self.E], updates=((self.tu, self.tu - self.rate * self.gu), (self.tv, self.tv - self.rate * self.gv)), allow_input_downcast=True) return train_function # initialization of flow values def init(self, u0, v0): self.tu.set_value(u0) self.tv.set_value(v0) # launching step of gradiennt descent def step(self, *args): return self.train_function(*args) # class TrainFunctionSimple(TrainFunction): def __init__(self, *args, **kwargs): self.alpha = kwargs.get(\u0026lsquo;alpha\u0026rsquo;, 1.1)\n super(self.`__class__`, self).`__init__`(*args, **kwargs) # constructs Theano-function, that calculate Energy def get_energy(self,): # data term Edata = T.sum( ( self.tIx * self.tu + self.tIy * self.tv + self.tIt ) ** 2 ) # regularization term Ereg = T.sum( (self.tu)**2 + (self.tv)**2 ) return Edata+self.alpha*Ereg  finally launching\nwrpr = Warper( I0.shape, numpy.zeros_like(I0), numpy.zeros_like(I0), I0, I1, display=True) warps = 5 for i in range(warps): wrpr.warp() Printing the results: numpy.set_printoptions(precision=3,) print wrpr.u print wrpr.v [[ 0. 0. 0. 0. 0. ] [ 0. 0. -0.523 0. 0. ] [ 0. 0. -0.941 0. 0. ] [ 0. 0. -0.523 0. 0. ] [ 0. 0. 0. 0. 0. ]] [[ 0. 0. 0. 0. 0. ] [ 0. -0.692 0. 0. 0. ] [ 0. 0. 0. 0. 0. ] [ 0. 0.692 0. 0. 0. ] [ 0. 0. 0. 0. 0. ]] Well, not precise enough. However the direction is right.\n UPDATE: due to my error in (***), see code, results and images below may have no sense   Some errors  In this implementation we estimate how to move I1 to match I0. So vectors points from locations of I0 to points of I1. So spatial derivatives must be calculated from I1, not I0: Ix = ndimage.correlate(self.I1, self.mask, mode='nearest') Iy = ndimage.correlate(self.I1, self.mask.T, mode='nearest') If you write instead: Ix = ndimage.correlate(self.I0, self.mask, mode='nearest') Iy = ndimage.correlate(self.I0, self.mask.T, mode='nearest') you will get pressy strange results. Input images I0 and I1: ![](http://3.bp.blogspot.com/-dYfZ9eUjqE4/VHtGMwxyuQI/AAAAAAAACFg/vBjZqQkILaY/s1600/I0.png) ![](http://4.bp.blogspot.com/-G4C3gKG5W_0/VHtGMyVkRtI/AAAAAAAACFk/d51MYVnFvjo/s1600/I1.png)  So the \u0026ldquo;object\u0026rdquo; moves from left to right. I0 is a current frame and I1 is a previous frame. The vectors after 1 and 10 warps of correct iteration (derivatives of I1, I0 at background):\n![](http://2.bp.blogspot.com/-RBt_zV-BwPE/VHtGBocIBhI/AAAAAAAACFY/3bTptWcs4zc/s1600/after%2B100%2Bcorrect%2Bwarp.png) ![](http://2.bp.blogspot.com/-UdPZ2hZ7Qaw/VHtGBmkLbAI/AAAAAAAACFE/U3Wg3lnIhV0/s1600/after%2Bone%2Bcorrect%2Bwarp.png)  The vectors after 1 and 10 warps of incorrect iteration (derivatives of I0, I0 at background): ![](http://1.bp.blogspot.com/-RGrBP9xedI0/VHtGB276WkI/AAAAAAAACFI/uvpw4JUI0Xo/s1600/after%2Bone%2Bincorrect%2Bwarp.png) ![](http://1.bp.blogspot.com/-N7Piixu8HBw/VHtGBgcTDSI/AAAAAAAACFA/bgQJqZQUsvY/s1600/after%2B100%2Bincorrect%2Bwarp.png) These pictures are made with 120 steps of gradient descent. Regularization termOk. Now lets improve our regularization term. Instead of L2 norm, lets use Total Variation approach.On the same settings\u0026nbsp; lets use another train function class:class TrainFunctionTV(TrainFunction): def `__init__`(self, *args, **kwargs): self.alpha = kwargs.get('alpha', 1.1)  super(self.`__class__`, self).`__init__`(*args, **kwargs) def get_energy(self,): Edata = T.sum((self.tIx * self.tu + self.tIy * self.tv + self.tIt) ** 2) Ereg1 = T.sum( (self.tu[1:]-self.tu[:-1])**2 + (self.tv[1:]-self.tv[:-1])**2 ) Ereg2 = T.sum( (self.tu[:,1:]-self.tu[:,:-1]) **2 + (self.tv[:,1:]-self.tv[:,:-1]) ** 2) return Edata+self.alpha*(Ereg1+Ereg2)  \nNow we have these results: print wrpr.u print wrpr.v [[-0.862 -0.903 -0.949 -0.941 -0.929] [-0.842 -0.92 -1.025 -0.968 -0.941] [-0.81 -0.933 -1.101 -0.99 -0.95 ] [-0.842 -0.92 -1.025 -0.968 -0.941] [-0.862 -0.903 -0.949 -0.941 -0.929]] [[-0.121 -0.136 -0.093 -0.06 -0.045] [-0.106 -0.197 -0.083 -0.043 -0.029] [-0. -0. -0. -0. -0. ] [ 0.106 0.197 0.083 0.043 0.029] [ 0.121 0.136 0.093 0.06 0.045]]\nVisualization:\n  ![](http://2.bp.blogspot.com/-RBt_zV-BwPE/VHtGBocIBhI/AAAAAAAACFc/reFzmPZUpes/s1600/after%2B100%2Bcorrect%2Bwarp.png)   ![](http://4.bp.blogspot.com/-fQ1nRz2R0TA/VI9SvoUd3iI/AAAAAAAACGU/IaqihseanNY/s1600/OF%2Bwith%2BTV.png) before (L2)after (TV) ","permalink":"https://serge-m.github.io/posts/writing-simple-optical-flow-in-python/","summary":"First of all we need a couple of test images:\n# import numpy from StringIO import StringIO I0 = numpy.loadtxt(StringIO(\"\"\" 0 0 0 0 0 0 0 0.5000 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0 0 0\"\"\") ) I1 = numpy.loadtxt(StringIO(\"\"\" 0 0 0 0 0 0 0.5 0 0 0 0 1.0 0 0 0 0 0.5 0 0 0 0 0 0 0 0 \"","title":"Writing simple optical flow in python"},{"content":"I find default visualization settings for images in matplotlib inconvenient. Usually I prefer to see images in grayscale color map and without smoothing. Thus I see actual data without distortions. Grayscale is easier to understand.\n1 2 3  import matplotlib.pyplot as plt plt.rcParams[\u0026#39;image.cmap\u0026#39;] = \u0026#39;gray\u0026#39; plt.rcParams[\u0026#39;image.interpolation\u0026#39;] = \u0026#39;nearest\u0026#39;   Compare default colors and interpolation:\nand after applying the settings:\n","permalink":"https://serge-m.github.io/posts/setting-default-parameters-from-imshow/","summary":"I find default visualization settings for images in matplotlib inconvenient. Usually I prefer to see images in grayscale color map and without smoothing. Thus I see actual data without distortions. Grayscale is easier to understand.\n1 2 3  import matplotlib.pyplot as plt plt.rcParams[\u0026#39;image.cmap\u0026#39;] = \u0026#39;gray\u0026#39; plt.rcParams[\u0026#39;image.interpolation\u0026#39;] = \u0026#39;nearest\u0026#39;   Compare default colors and interpolation:\nand after applying the settings:","title":"Setting default parameters for imshow in pyplot"},{"content":"Okay children, today we learn how to convert text to numpy matrix. Source is [here](http://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\u0026quot; target=\u0026quot;_blank).\n# loading modules import numpy from StringIO import StringIO # Using StringIO as a file-like wrapper over text I0 = numpy.loadtxt(StringIO(\"\"\" 0 0 0 0 0 0 0 0.5000 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0.5000 1.0000 0 0 0 0 0 0 0\"\"\") ) I1 = numpy.loadtxt(StringIO(\"\"\" 0 0 0 0 0 0 0.5000 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0.5000 1.0000 0 0 0 0 0 0 \"\"\") )  #printing results: print I0 print I1\n#result [[ 0. 0. 0. 0. 0. ] [ 0. 0. 0.5 0. 0. ] [ 0. 0. 1. 0. 0. ] [ 0. 0. 0.5 0. 0. ] [ 0. 1. 0. 0. 0. ] [ 0. 0.5 0. 0. 0. ] [ 0. 0.5 1. 0. 0. ] [ 0. 0. 0. 0. 0. ]] [[ 0. 0. 0. 0. 0. ] [ 0. 0.5 0. 0. 0. ] [ 0. 1. 0. 0. 0. ] [ 0. 0.5 0. 0. 0. ] [ 0. 0. 1. 0. 0. ] [ 0. 0. 0.5 0. 0. ] [ 0. 0. 0.5 1. 0. ] [ 0. 0. 0. 0. 0. ]]\n\n","permalink":"https://serge-m.github.io/posts/loading-numpy-array-from-string/","summary":"Okay children, today we learn how to convert text to numpy matrix. Source is [here](http://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html\u0026quot; target=\u0026quot;_blank).\n# loading modules import numpy from StringIO import StringIO # Using StringIO as a file-like wrapper over text I0 = numpy.loadtxt(StringIO(\"\"\" 0 0 0 0 0 0 0 0.5000 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0.5000 1.0000 0 0 0 0 0 0 0\"","title":"Loading numpy array from string"},{"content":"Standard matlab\u0026rsquo;s quiver function has axis origin in left bottom corner, however, images have origin in top left corner. To display optical flow vector field consistenly i use the following fucntion:\nfunction [ output ] = quiver_flow( u, v ) %QUIVER_FLOW Displays quiver for optical flow % SMatyunin2014 output = quiver( u, v, 0); axis ij; end  ","permalink":"https://serge-m.github.io/posts/quiver-for-optical-flow/","summary":"Standard matlab\u0026rsquo;s quiver function has axis origin in left bottom corner, however, images have origin in top left corner. To display optical flow vector field consistenly i use the following fucntion:\nfunction [ output ] = quiver_flow( u, v ) %QUIVER_FLOW Displays quiver for optical flow % SMatyunin2014 output = quiver( u, v, 0); axis ij; end  ","title":"Quiver for optical flow"},{"content":"BA method, simple synthetic images For simple synthetic images:\nI0 = 0 0.5000 0 0 1.0000 0 0 0.5000 0 0.1000 0 0 0.0500 0 0 0.0500 0.1000 0 I1 = 0.5000 0 0 1.0000 0 0 0.5000 0 0 0 0.1000 0 0 0.0500 0 0 0.0500 0.1000 Running code by D.Sun. Disabled texture decomposition, disabled multiscale processing.\nuv = estimate_flow_interface(I0, I1, 'classic-c-brightness', [], {'display', 1, 'pyramid_levels', 1, 'gnc_pyramid_levels', 1}); After first iteration ( loop for ignc = 1:this.gnc_iters\u0026amp;nbsp; in dsun_ijcv_flow_code\\@ba_optical_flow\\compute_flow.m): After 2nd iteration: After 3rd iteration: I think, it is right answer.\nuv(:,:,1) = -0.9995 -0.9995 -0.9996 -0.9994 -0.9994 -0.9995 -0.9992 -0.9992 -0.9992 0.9992 0.9992 0.9992 0.9994 0.9994 0.9994 0.9995 0.9994 0.9994 uv(:,:,2) = 1.0e-003 * -0.2083 -0.2080 -0.2080 -0.2080 -0.1848 -0.1848 -0.1861 -0.1616 -0.1848 -0.1507 -0.0860 -0.1183 -0.0860 -0.0633 -0.1183 -0.0396 -0.0396 -0.0860 BA method. Another pair of images I0 = 0 0.5000 0 0 1.0000 0 0 0.5000 0 I1 = 0.5000 0 0 1.0000 0 0 0.5000 0 0 1st iteration\nuv(:,:,1) = -1 -1 -1 -1 -1 -1 -1 -1 -1 uv(:,:,2) = 1.0e-014 * 0.8281 0.8281 0.8281 0.8281 0.8281 0.8281 0.8281 0.8281 0.8281 2nd iteration\n uv(:,:,1) = -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 uv(:,:,2) = 1.0e-013 * -0.5965 -0.5965 -0.5965 -0.5965 -0.5965 -0.5965 -0.5965 -0.5965 -0.5965 3rd iteration\n uv(:,:,1) = -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 -1.0000 uv(:,:,2) = 1.0e-011 * 0.5374 0.5374 0.5374 0.5374 0.5374 0.5374 0.5374 0.5374 0.5374 Visually the same: HS method. Simple image BA - is not the simpliest method. I found there is implmentation of Horn-Schunk.\nI found that the results are really poor, when my \u0026ldquo;image elements\u0026rdquo; ar just on the border. So I extended image by zeros.\nI use the following command:\nuv = estimate_flow_interface(I0, I1, ... 'hs-brightness', [], ... {'display', 1, 'pyramid_levels', 1, 'gnc_pyramid_levels', 1, ... 'pyramid_spacing', sqrt(2)}); I also use slightly modified code. I disabled automatic pyramid height calculation, so I set it manually. Input\n I0 = 0 0.5000 0 0 1.0000 0 0 0.5000 0 1.0000 0 0 0.5000 0 0 0.5000 1.0000 0 I1 = 0.5000 0 0 1.0000 0 0 0.5000 0 0 0 1.0000 0 0 0.5000 0 0 0.5000 1.0000 Gives: While\n I0 = 0 0 0 0 0 0 0 0.5000 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0.5000 1.0000 0 0 0 0 0 0 0 I1 = 0 0 0 0 0 0 0.5000 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0 1.0000 0 0 0 0 0.5000 0 0 0 0 0.5000 1.0000 0 0 0 0 0 0 gives HS method. Multiscale.  I0 = [ 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 5 5 0 0 0 0 3 0 0 0 0 0 1 3 0 0 0 0 0 0 0 1 0 0 2 4 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 0 0 0 0 0 0 0 0 0 0 5 5 0 0 0 0 0 0 0 0 0 0 ] / 5.; I1 = [ 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 5 0 0 0 0 2 0 0 0 0 0 1 3 0 5 2 0 0 0 0 0 0 0 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 5 5 0 0 0 0 0 0 0 0 0 0 5 5 0 0 0 0 0 0 0 0 0 0 ] / 5.; Here the shift between frames is much larger than a pixel and the size of objects. So OF fail without resolution scaling.\n   1 pyramid level ![](http://1.bp.blogspot.com/-cKPApsEww3w/VFuA8qfEpPI/AAAAAAAACEQ/UmqX4DYVnkk/s320/HS_pyramid_1.png) 2 pyramid levels ![](http://1.bp.blogspot.com/-wC_4IzeuNAE/VFuA8-z9U8I/AAAAAAAACEU/7K_vWoiou0Q/s320/HS_pyramid_2.png) 3 pyramid levels ![](http://4.bp.blogspot.com/-ynqgbeq_-B0/VFuA8lHxFJI/AAAAAAAACEY/KR3szPexLP4/s320/HS_pyramid_3.png)    ","permalink":"https://serge-m.github.io/posts/simple-tests-of-classic-of-methods/","summary":"BA method, simple synthetic images For simple synthetic images:\nI0 = 0 0.5000 0 0 1.0000 0 0 0.5000 0 0.1000 0 0 0.0500 0 0 0.0500 0.1000 0 I1 = 0.5000 0 0 1.0000 0 0 0.5000 0 0 0 0.1000 0 0 0.0500 0 0 0.0500 0.1000 Running code by D.Sun. Disabled texture decomposition, disabled multiscale processing.\nuv = estimate_flow_interface(I0, I1, 'classic-c-brightness', [], {'display', 1, 'pyramid_levels', 1, 'gnc_pyramid_levels', 1}); After first iteration ( loop for ignc = 1:this.","title":"Simple tests of classic OF methods"},{"content":"Optimization of energy terms can be difficult in OF, because of non-convexity and local optima. Construct a series of energy functions\nEQ is convex, quadratic alpha changes from 1 to 0, so Energy Ec changes from quadratic to original. for each alpha they find optimum through setting derivatives of Ec to 0. Solution on each stage becomes initialization on the next one.\nProposed in: D. Sun, S. Roth, J. Lewis, and M. J. Black. Learning optical flow. In ECCV, volume 3, pages 83–97, 2008. [[pdf](http://cs.brown.edu/~dqsun/pubs/eccv2008.pdf\u0026quot; target=\u0026quot;_blank)]\n","permalink":"https://serge-m.github.io/posts/graduated-non-convexity-scheme-gnc/","summary":"Optimization of energy terms can be difficult in OF, because of non-convexity and local optima. Construct a series of energy functions\nEQ is convex, quadratic alpha changes from 1 to 0, so Energy Ec changes from quadratic to original. for each alpha they find optimum through setting derivatives of Ec to 0. Solution on each stage becomes initialization on the next one.\nProposed in: D. Sun, S. Roth, J. Lewis, and M.","title":"Graduated non convexity scheme (GNC)"},{"content":"Personal page of the author: http://cs.brown.edu/~dqsun/research/index.html\nOriginal paper: http://cs.brown.edu/~dqsun/pubs/cvpr_2010_flow.pdf\nNewer paper: Deqing Sun, Stefan Roth, and Michael J. Black. \u0026ldquo;A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them\u0026rdquo;. International Journal of Computer Vision (IJCV), 2013 [ pdf] [Source code]\nLook inside the sources:\n @alt_ba_optical_flow\\ @ba_optical_flow\\ ... compute_flow_base.m itarates:\n  Iterate flow computation\n  Linearization update, for j = 1:this.max_linear. In the simple case max_linear==1, when I use x = A\\b solver for linear system. Probably for more complicated solvers max_linear \u0026gt; 1\n   compute_flow.m calls pre_process_data (preprocessing, normalization, image pyramyd). Loop through iterations:\nfor ignc = 1:this.gnc_iters Calls compute_flow_base.m\n ... @classic_nl_optical_flow\\ @hs_optical_flow\\ data\\ utils\\ ... pre_process_data.m \u0026ndash; several preptocessing options for images. The first is texture decomposition. if no texture decomposition, scale image to [0, 255] range. Build image pyramyd. there is also following code:\n% For segmentation purpose data.org_pyramid_images = compute_image_pyramid(this.images, f,... this.pyramid_levels, 1/this.pyramid_spacing); data.org_color_pyramid_images = compute_image_pyramid(this.color_images,... f, this.pyramid_levels, 1/this.pyramid_spacing); In my case I don\u0026rsquo;t need segmentation and use simple OF method. It seems it\u0026rsquo;s just bad code design. Dig deeper.\n ... estimate_flow_demo.m estimate_flow_interface.m load_of_method.m \u0026ndash; switcher that recursively initializes members of OF object. E.g. if you want to load \u0026lsquo;classic+nl-brightness\u0026rsquo; method, it loads \u0026lsquo;classic+nl\u0026rsquo; first:\ncase 'classic+nl' ope = classic_nl_optical_flow; ope.texture = true; ope.median_filter_size = median_filter_size; ope.alp = 0.95; ope.area_hsz = 7; ope.sigma_i = 7; ope.color_images = ones(1,1,3); ope.lambda = 3; ope.lambda_q =3; %ope.display = true; and then applies the difference:\ncase 'classic+nl-fast-brightness' ope = load_of_method('classic+nl'); ope.max_iters = 3; ope.gnc_iters = 2; ope.texture = false; ","permalink":"https://serge-m.github.io/posts/investigating-optical-flow-by-d-sun/","summary":"Personal page of the author: http://cs.brown.edu/~dqsun/research/index.html\nOriginal paper: http://cs.brown.edu/~dqsun/pubs/cvpr_2010_flow.pdf\nNewer paper: Deqing Sun, Stefan Roth, and Michael J. Black. \u0026ldquo;A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them\u0026rdquo;. International Journal of Computer Vision (IJCV), 2013 [ pdf] [Source code]\nLook inside the sources:\n @alt_ba_optical_flow\\ @ba_optical_flow\\ ... compute_flow_base.m itarates:\n  Iterate flow computation\n  Linearization update, for j = 1:this.max_linear. In the simple case max_linear==1, when I use x = A\\b solver for linear system.","title":"Investigating optical flow by D. Sun (Secrets of optical flow)"},{"content":"Source\nplace following code before your \u0026lt;head\u0026gt; tag in the HTML of your blogger template:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  \u0026lt;link href=\u0026#34;http://alexgorbatchev.com/pub/sh/current/styles/shCore.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt; \u0026lt;link href=\u0026#34;http://alexgorbatchev.com/pub/sh/current/styles/shThemeDefault.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shCore.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCpp.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCSharp.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCss.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJava.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJScript.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPhp.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPython.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushRuby.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushSql.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushVb.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushXml.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPerl.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script language=\u0026#34;javascript\u0026#34;\u0026gt; SyntaxHighlighter.config.bloggerMode = true; SyntaxHighlighter.config.clipboardSwf = \u0026#39;http://alexgorbatchev.com/pub/sh/current/scripts/clipboard.swf\u0026#39;; SyntaxHighlighter.all(); \u0026lt;/script\u0026gt;   Usage:\n1 2 3 4 5 6 7  \u0026lt;script class=\u0026#34;brush: html\u0026#34; type=\u0026#34;syntaxhighlighter\u0026#34;\u0026gt;\u0026lt;![CDATA[ \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Carter Tomorrow Fund Donations\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;title\u0026#34; content=\u0026#34;Help Give to the Carter Tomorrow Fund\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;Show your support and help out with a small gift\u0026#34; ]]\u0026gt;\u0026lt;/script\u0026gt;   or\n1 2 3  \u0026lt;pre class=\u0026#34;brush: html\u0026#34;\u0026gt; CODE \u0026lt;/pre\u0026gt;   ","permalink":"https://serge-m.github.io/posts/11/","summary":"Source\nplace following code before your \u0026lt;head\u0026gt; tag in the HTML of your blogger template:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  \u0026lt;link href=\u0026#34;http://alexgorbatchev.com/pub/sh/current/styles/shCore.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt; \u0026lt;link href=\u0026#34;http://alexgorbatchev.com/pub/sh/current/styles/shThemeDefault.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt;\u0026lt;/link\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shCore.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCpp.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCSharp.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCss.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJava.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJScript.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPhp.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPython.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.com/pub/sh/current/scripts/shBrushRuby.js\u0026#34; type=\u0026#34;text/javascript\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;http://alexgorbatchev.","title":"Code highlight in blogger"},{"content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # creating a variable theNotebook with the name of notebook # source: http://stackoverflow.com/a/23619544 # In[1]: %%javascript var kernel = IPython.notebook.kernel; var thename = window.document.getElementById(\u0026#34;notebook_name\u0026#34;).innerHTML; var command = \u0026#34;theNotebook = \u0026#34; + \u0026#34;\u0026#39;\u0026#34;+thename+\u0026#34;\u0026#39;\u0026#34;; kernel.execute(command); # saving to a directory \u0026#39;backup\u0026#39;. create the directory if it doesn\u0026#39;t exist # source http://stackoverflow.com/a/19067979 # In[2]: try : if(__IPYTHON__) : print \u0026#34;saving\u0026#34;, theNotebook import os dir_backup = \u0026#39;backup\u0026#39; if not os.path.exists(dir_backup): os.makedirs(dir_backup) get_ipython().system(u\u0026#39;ipython nbconvert --to python {0} --output {1}\u0026#39;.format(theNotebook, os.path.join(dir_backup, theNotebook)) ) except NameError : print \u0026#34;Unable to save\u0026#34;   ","permalink":"https://serge-m.github.io/posts/save-ipython-notebook-as-script-with/","summary":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  # creating a variable theNotebook with the name of notebook # source: http://stackoverflow.com/a/23619544 # In[1]: %%javascript var kernel = IPython.notebook.kernel; var thename = window.document.getElementById(\u0026#34;notebook_name\u0026#34;).innerHTML; var command = \u0026#34;theNotebook = \u0026#34; + \u0026#34;\u0026#39;\u0026#34;+thename+\u0026#34;\u0026#39;\u0026#34;; kernel.execute(command); # saving to a directory \u0026#39;backup\u0026#39;. create the directory if it doesn\u0026#39;t exist # source http://stackoverflow.","title":"Save Ipython notebook as script with the same filename"},{"content":"I wrote python version of interp2(z, xi, yi,\u0026lsquo;linear\u0026rsquo;) from matlab\nhttps://github.com/serge-m/pyinterp2\n","permalink":"https://serge-m.github.io/posts/python-equivalent-of-interp2/","summary":"I wrote python version of interp2(z, xi, yi,\u0026lsquo;linear\u0026rsquo;) from matlab\nhttps://github.com/serge-m/pyinterp2","title":"Python equivalent of interp2"},{"content":"As an example I use images from middleburry. Solution is very dirty and slow.\n[https://github.com/serge-m/depth_map_occlusion](https://github.com/serge-m/depth_map_occlusion\u0026quot; target=\u0026quot;_blank)\n# In[1]: import numpy import scipy import matplotlib.pyplot as plt\nIn[2]: from scipy import ndimage import numpy as np\nkernels for shift k = np.array([ [[0,0,0], [0,0,1], [0,0,0],], [[0,1,0], [0,0,0], [0,0,0],], [[0,0,0], [1,0,0], [0,0,0],], [[0,0,0], [0,0,0], [0,1,0],], [[1,0,0], [0,0,0], [0,0,0],], [[0,0,1], [0,0,0], [0,0,0],], [[0,0,0], [0,0,0], [0,0,1],], [[0,0,0], [0,0,0], [1,0,0],], ])  In[8]: from scipy import misc\nfrom PIL import Image\n#reading source data frm = misc.imread(\u0026lsquo;cones_2_6_align2_00000.png\u0026rsquo;) dpt0 = misc.imread(\u0026lsquo;disp2.png\u0026rsquo;) occl0 = misc.imread(\u0026lsquo;occl.png\u0026rsquo;)\n#initial values for results dpt = dpt0 occl = occl0\nfrm_shifterd = numpy.zeros((len(k),)+frm.shape) dif = numpy.zeros((len(k),)+frm.shape) sumdiff = numpy.zeros((len(k),)+frm.shape[:-1])\nfor ik in range(len(k)): for ch in range(frm.shape[-1]): frm_shifterd[ik,:,:,ch] = ndimage.convolve(frm[:,:,ch], k[ik], mode=\u0026lsquo;nearest\u0026rsquo;, cval=0.0) dif[ik] = numpy.abs(frm_shifterd[ik,:,:,:]-frm[:,:,:]) sumdiff[ik] = dif[ik].sum(axis=2) iteration = 0\ndef one_iteration(dpt, occl, k, sumdiff): dpt_shifted = numpy.zeros((len(k),)+dpt.shape) occl_shifted = numpy.zeros((len(k),)+dpt.shape) sumdiff_final = numpy.zeros((len(k),)+frm.shape[:-1])\n occl_dil = ndimage.grey_dilation(occl, size=(3,3)) for ik in range(len(k)): dpt_shifted[ik,:,:] = ndimage.convolve(dpt[:,:], k[ik], mode=\u0026lsquo;nearest\u0026rsquo;, cval=0.0) occl_shifted[ik,:,:] = ndimage.convolve(occl[:,:], k[ik], mode=\u0026lsquo;nearest\u0026rsquo;, cval=0.0) sumdiff_final[ik] = sumdiff[ik] + (1-occl_shifted[ik])*1000000000 #print \u0026lsquo;shifted\u0026rsquo;, ik, \u0026lsquo;\\n\u0026rsquo;, dpt_shifted[ik] # chose direction where the difference is the lowest good_directions = numpy.argmin(sumdiff_final, axis = 0) dpt_best = numpy.choose( good_directions, dpt_shifted ) dpt_new = numpy.choose( occl==occl_dil, numpy.array([dpt_best,dpt])) return dpt_new, occl_dil\nwhile True: iteration += 1 dpt_new, occl_dil = one_iteration(dpt, occl, k, sumdiff) if numpy.array_equal(occl, occl_dil): break; #Image.fromarray(np.cast\u0026lsquo;uint8\u0026rsquo;).save(\u0026lsquo;dpt_{}.png\u0026rsquo;.format(iteration))\n dpt = numpy.copy(dpt_new) occl = numpy.copy(occl_dil) #write output Image.fromarray(np.cast\u0026lsquo;uint8\u0026rsquo;).save(\u0026lsquo;processed_dpt.png\u0026rsquo;)\n","permalink":"https://serge-m.github.io/posts/simple-occlusion-filling-for-depth-maps/","summary":"As an example I use images from middleburry. Solution is very dirty and slow.\n[https://github.com/serge-m/depth_map_occlusion](https://github.com/serge-m/depth_map_occlusion\u0026quot; target=\u0026quot;_blank)\n# In[1]: import numpy import scipy import matplotlib.pyplot as plt\nIn[2]: from scipy import ndimage import numpy as np\nkernels for shift k = np.array([ [[0,0,0], [0,0,1], [0,0,0],], [[0,1,0], [0,0,0], [0,0,0],], [[0,0,0], [1,0,0], [0,0,0],], [[0,0,0], [0,0,0], [0,1,0],], [[1,0,0], [0,0,0], [0,0,0],], [[0,0,1], [0,0,0], [0,0,0],], [[0,0,0], [0,0,0], [0,0,1],], [[0,0,0], [0,0,0], [1,0,0],], ])  In[8]: from scipy import misc","title":"Simple occlusion filling for depth maps"},{"content":"Joint solution from [http://stackoverflow.com/a/23619544](http://stackoverflow.com/a/23619544\u0026quot; target=\u0026quot;_blank) and [http://stackoverflow.com/a/19067979](http://stackoverflow.com/a/19067979\u0026quot; target=\u0026quot;_blank)\n# [1] %%javascript var kernel = IPython.notebook.kernel; var thename = window.document.getElementById(\"notebook_name\").innerHTML; var command = \"theNotebook = \" + \"'\"+thename+\"'\"; kernel.execute(command); [2] try : if(__IPYTHON__) : get_ipython().system(u\u0026rsquo;ipython nbconvert \u0026ndash;to python {}.ipynb'.format(theNotebook)) except NameError : pass\n","permalink":"https://serge-m.github.io/posts/ipython-save-notebook-as-script/","summary":"Joint solution from [http://stackoverflow.com/a/23619544](http://stackoverflow.com/a/23619544\u0026quot; target=\u0026quot;_blank) and [http://stackoverflow.com/a/19067979](http://stackoverflow.com/a/19067979\u0026quot; target=\u0026quot;_blank)\n# [1] %%javascript var kernel = IPython.notebook.kernel; var thename = window.document.getElementById(\"notebook_name\").innerHTML; var command = \"theNotebook = \" + \"'\"+thename+\"'\"; kernel.execute(command); [2] try : if(__IPYTHON__) : get_ipython().system(u\u0026rsquo;ipython nbconvert \u0026ndash;to python {}.ipynb'.format(theNotebook)) except NameError : pass","title":"ipython. save notebook as script"},{"content":"There are several solutions: Source: http://stackoverflow.com/questions/2883189/calling-matlab-functions-from-python\npymatA low level interface to Matlab using the matlab engine (libeng) for communication (basically a library that comes with matlab). The module has to be compiled and linked with libeng. [http://pymat.sourceforge.net](http://pymat.sourceforge.net/\" rel=\"nofollow) Last updated: 2003 pymat2A somewhat short lived continuation of the pymat development. Seems to work on windows (including 64bit), linux and mac (with some changes). [https://code.google.com/p/pymat2/](https://code.google.com/p/pymat2/\" rel=\"nofollow) Last updated: 2012 mlabwrapA high level interface that also comes as a module which needs compilation and linking against libeng. It exposes Matlab functions to python so you can do fun stuff like mlab.plot(x, y, 'o') [http://mlabwrap.sourceforge.net](http://mlabwrap.sourceforge.net/\" rel=\"nofollow) Last updated: 2009 mlabA repackaging effort of mlabwrap. Basically it replaces the c++ code that links against 'libeng' in mlabwrap with a python module ([matlabpipe](https://code.google.com/p/danapeerlab/source/browse/trunk/freecell/depends/common/python/matlabpipe.py\" rel=\"nofollow)) that communicates with matlab through a pipe. The main advantage of this is that it doesn't need compilation of any kind. Unfortunately the package currently has a couple of bugs and doesn't seem to work on the mac at all. I reported a few of them but gave up eventually. Also, be prepared for lots of trickery and a bunch of pretty ugly hacks if you have to go into the source code ;-) If this becomes more mature it could be one of the best options. [https://github.com/ewiger/mlab](https://github.com/ewiger/mlab\" rel=\"nofollow) last update: 2013 pymatlabA newer package (2010) that also interacts with Matlab through libeng. Unlike the other packages this one loads the engine library through [ctypes](https://docs.python.org/2/library/ctypes.html\" rel=\"nofollow) thus no compilation required. Its not without flaws but still being maintained and the (64bit Mac specific) issues I found should be easy enough to fix. (edit 2014-05-20: it seems those issues have already been fixed in the source so things should be fine with 0.2.4) [http://pymatlab.sourceforge.net](http://pymatlab.sourceforge.net/\" rel=\"nofollow) last update: 2014 python-matlab-bridgeAlso a newer package that is still actively maintained. Communicates with Matlab through some sort of socket. Unfortunately the exposed functions are a bit limited. I couldn't figure out how to invoke a function that takes structs as parameters. Requires zmq, pyzmq and IPython which are easy enough to install. [http://arokem.github.io/python-matlab-bridge](http://arokem.github.io/python-matlab-bridge\" rel=\"nofollow) last update: 2014 Bug report for Mlab:\nhttp://stackoverflow.com/questions/20659616/python-mlab-cannot-import-name-find-available-releases Fixed here: https://github.com/ewiger/mlab/commit/4bfa59af2a1a996a774c80d7aafc4a390f548669 ","permalink":"https://serge-m.github.io/posts/use-matlab-function-from-python/","summary":"There are several solutions: Source: http://stackoverflow.com/questions/2883189/calling-matlab-functions-from-python\npymatA low level interface to Matlab using the matlab engine (libeng) for communication (basically a library that comes with matlab). The module has to be compiled and linked with libeng. [http://pymat.sourceforge.net](http://pymat.sourceforge.net/\" rel=\"nofollow) Last updated: 2003 pymat2A somewhat short lived continuation of the pymat development. Seems to work on windows (including 64bit), linux and mac (with some changes). [https://code.google.com/p/pymat2/](https://code.google.com/p/pymat2/\" rel=\"nofollow) Last updated: 2012 mlabwrapA high level interface that also comes as a module which needs compilation and linking against libeng.","title":"Use Matlab function from python"},{"content":"I am trying to train NN using pylearn2 I use stochastic gradient decent. I had always a message about failed GPU memory allocation in train-\u0026gt;self.sgd_update(*batch) To fix it, I reduced size of the batch in yaml-config: batch_size: 200 was replaced by batch_size: 100\n","permalink":"https://serge-m.github.io/posts/pylearn2-out-of-memory-error/","summary":"I am trying to train NN using pylearn2 I use stochastic gradient decent. I had always a message about failed GPU memory allocation in train-\u0026gt;self.sgd_update(*batch) To fix it, I reduced size of the batch in yaml-config: batch_size: 200 was replaced by batch_size: 100","title":"Pylearn2 out of memory error"},{"content":"Drawing on an image with TikZ\nDrawing label on figure (using tikz)\n\\begin{tikzpicture} \\node[anchor=south west,inner sep=0] at (0,0) {\\includegraphics[trim={600px 200px 50px 200px},clip, width=1\\linewidth]{images/DP_10150_simpleCombinedBasedOnDPlen.exe_scale1_blank/frm00001}}; %\\draw[white,fill=white] (0.0,0.0) rectangle (0.5,0.5); \\node[minimum size=.6cm, fill=white,anchor=south west] at (0.0,0.0){а}; \\end{tikzpicture} ","permalink":"https://serge-m.github.io/posts/tikz-cookbook/","summary":"Drawing on an image with TikZ\nDrawing label on figure (using tikz)\n\\begin{tikzpicture} \\node[anchor=south west,inner sep=0] at (0,0) {\\includegraphics[trim={600px 200px 50px 200px},clip, width=1\\linewidth]{images/DP_10150_simpleCombinedBasedOnDPlen.exe_scale1_blank/frm00001}}; %\\draw[white,fill=white] (0.0,0.0) rectangle (0.5,0.5); \\node[minimum size=.6cm, fill=white,anchor=south west] at (0.0,0.0){а}; \\end{tikzpicture} ","title":"tikz cookbook"},{"content":"from matplotlib import pyplot as plt import matplotlib.cm as cm plt.figure() # without this it display one after another plt.imshow(image_one, cmap=cm.gray) # without cm.gray it displays grayscale images in colormap plt.imshow(image_two, cmap=cm.gray) # plt.show()\nuse %matplotlib inline in ipython notebook to display image inplace\nShorter version: %matplotlib inline import matplotlib.pyplot as plt plt.axis(\u0026lsquo;off\u0026rsquo;) plt.imshow(dpt, cmap=plt.cm.gray, interpolation=\u0026lsquo;nearest\u0026rsquo;)\n","permalink":"https://serge-m.github.io/posts/displaying-multiple-grayscale-figures/","summary":"from matplotlib import pyplot as plt import matplotlib.cm as cm plt.figure() # without this it display one after another plt.imshow(image_one, cmap=cm.gray) # without cm.gray it displays grayscale images in colormap plt.imshow(image_two, cmap=cm.gray) # plt.show()\nuse %matplotlib inline in ipython notebook to display image inplace\nShorter version: %matplotlib inline import matplotlib.pyplot as plt plt.axis(\u0026lsquo;off\u0026rsquo;) plt.imshow(dpt, cmap=plt.cm.gray, interpolation=\u0026lsquo;nearest\u0026rsquo;)","title":"displaying multiple grayscale figures in python's matplotlib"},{"content":"_Intro __Theano_ is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Check gpu is workingSource: [http://deeplearning.net/software/theano/tutorial/using_gpu.html](http://deeplearning.net/software/theano/tutorial/using_gpu.html)Test script: :::: from theano import function, config, shared, sandbox import theano.tensor as T import numpy import time \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;vlen\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;*\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;30\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;*\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;768\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;c\u0026quot;\u0026gt;# 10 x #cores x # threads per core\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;iters\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;1000\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;rng\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;numpy\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;random\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;RandomState\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(0\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;x\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;shared\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;numpy\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;asarray\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;rng\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;rand\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;vlen\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;),\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;config\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;floatX\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;f\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;function\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;([],\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;T\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;exp\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;x\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;))\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;print\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;f\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;maker\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;fgraph\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;toposort\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;t0\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;time\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;time\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;i\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;ow\u0026quot;\u0026gt;in\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;nb\u0026quot;\u0026gt;xrange\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;iters\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;):\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;r\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;f\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;t1\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;time\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;time\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;()\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;print\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;s\u0026quot;\u0026gt;'Looping \u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;si\u0026quot;\u0026gt;%d\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;s\u0026quot;\u0026gt; times took'\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;%\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;iters\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;t1\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;-\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;t0\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;s\u0026quot;\u0026gt;'seconds'\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;print\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;s\u0026quot;\u0026gt;'Result is'\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;r\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;if\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;numpy\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;any\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;([\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;nb\u0026quot;\u0026gt;isinstance\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;x\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;op\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;T\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;Elemwise\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;for\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;x\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;ow\u0026quot;\u0026gt;in\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;f\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;maker\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;fgraph\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;toposort\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;()]):\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;print\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;s\u0026quot;\u0026gt;'Used the cpu'\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;else\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;:\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;k\u0026quot;\u0026gt;print\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;s\u0026quot;\u0026gt;'Used the gpu'\u0026lt;/span\u0026gt;  \u0026nbsp;Run with two configurations: :::: $ THEANO_FLAGS=mode=FAST_RUN,device=cpu,floatX=float32 python check1.py [Elemwise{exp,no_inplace}()] Looping 1000 times took 3.06635117531 seconds Result is [ 1.23178029 1.61879337 1.52278066 ..., 2.20771813 2.29967761 1.62323284] Used the cpu $ THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python check1.py Using gpu device 0: GeForce GTX 580 [GpuElemwise{exp,no_inplace}(\u0026lt;CudaNdarrayType(float32, vector)\u0026gt;), HostFromGpu(GpuElemwise{exp,no_inplace}.0)] Looping 1000 times took 0.638810873032 seconds Result is [ 1.23178029 1.61879349 1.52278066 ..., 2.20771813 2.29967761 1.62323296] Used the gpu   ","permalink":"https://serge-m.github.io/posts/theano/","summary":"_Intro __Theano_ is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Check gpu is workingSource: [http://deeplearning.net/software/theano/tutorial/using_gpu.html](http://deeplearning.net/software/theano/tutorial/using_gpu.html)Test script: :::: from theano import function, config, shared, sandbox import theano.tensor as T import numpy import time \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;vlen\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;10\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;*\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;30\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;*\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;768\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;c\u0026quot;\u0026gt;# 10 x #cores x # threads per core\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;iters\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;1000\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;rng\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;numpy\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.","title":"Theano"},{"content":"doskey /history \u0026gt; commands.log- dump command line promt history to file\nsome_command \u0026gt; output_file.txt 2\u0026gt;\u0026amp;1 - Redirect stdout and stderr to the same file src\nShare wifi from command line (probably obsolete in Win 8, 10) netsh wlan set hostednetwork mode=allow ssid=\u0026quot;NETWORK_NAME\u0026quot; key=\u0026quot;NETWORK_PASSWORD\u0026quot; keyUsage=persistent netsh wlan start hostednetwork Then in the properties of the internet connection enable access.\nThen allow in firewall.\n","permalink":"https://serge-m.github.io/posts/windows-life/","summary":"doskey /history \u0026gt; commands.log- dump command line promt history to file\nsome_command \u0026gt; output_file.txt 2\u0026gt;\u0026amp;1 - Redirect stdout and stderr to the same file src\nShare wifi from command line (probably obsolete in Win 8, 10) netsh wlan set hostednetwork mode=allow ssid=\u0026quot;NETWORK_NAME\u0026quot; key=\u0026quot;NETWORK_PASSWORD\u0026quot; keyUsage=persistent netsh wlan start hostednetwork Then in the properties of the internet connection enable access.\nThen allow in firewall.","title":"windows life"},{"content":"Good start 1  sudo apt install -y mc curl jq wget htop hwinfo nmap vim   Some commands top - see running processes\ndf - check free disk space\nbaobab - free disk space\nscreen - multiple virtual consoles in one real [ref], need installation in ubuntu\nmc - file manager\nccsm - disabling smooth fades and animations (speedup interface, especially useful in VirtualBox)\nsudo apt-get install compizconfig-settings-manager then\nccsm Set up SFTP server  How to set up an SFTP server on Linux - some commands are broken (wrong paths and users) but in general works  See also  more here Ubuntu customization: ubuntu-linux-settings Mount windows shares in linux  ","permalink":"https://serge-m.github.io/posts/linux-life/","summary":"Good start 1  sudo apt install -y mc curl jq wget htop hwinfo nmap vim   Some commands top - see running processes\ndf - check free disk space\nbaobab - free disk space\nscreen - multiple virtual consoles in one real [ref], need installation in ubuntu\nmc - file manager\nccsm - disabling smooth fades and animations (speedup interface, especially useful in VirtualBox)\nsudo apt-get install compizconfig-settings-manager then","title":"linux life"},{"content":"I discovered it during Machine learnin courses on Coursera\nTo specify the actual function we are minimizing, we use a \"short-hand\" for specifying functions with the @(t) ( costFunction(t, X, y) ) . This creates a function, with argument t, which calls your costFunction. This allows us to wrap the costFunction for use with fminunc. @(t) ( costFunction(t, X, y) ) - that\u0026rsquo;s awesome\n","permalink":"https://serge-m.github.io/posts/lambda-functions-in-matlab/","summary":"I discovered it during Machine learnin courses on Coursera\nTo specify the actual function we are minimizing, we use a \"short-hand\" for specifying functions with the @(t) ( costFunction(t, X, y) ) . This creates a function, with argument t, which calls your costFunction. This allows us to wrap the costFunction for use with fminunc. @(t) ( costFunction(t, X, y) ) - that\u0026rsquo;s awesome","title":"lambda functions in matlab"},{"content":"\\documentclass{article} \\usepackage{graphicx} \\begin{document} % crop left \\includegraphics[trim={5cm 0 0 0},clip]{path-to-image} % crop right \\includegraphics[trim={0 0 5cm 0},clip]{path-to-image} \\end{document} Use the trim option, which takes four space separated values.\ntrim={\u0026lt;left\u0026gt; \u0026lt;lower\u0026gt; \u0026lt;right\u0026gt; \u0026lt;upper\u0026gt;} Source\n","permalink":"https://serge-m.github.io/posts/crop-included-graphics-in-latex/","summary":"\\documentclass{article} \\usepackage{graphicx} \\begin{document} % crop left \\includegraphics[trim={5cm 0 0 0},clip]{path-to-image} % crop right \\includegraphics[trim={0 0 5cm 0},clip]{path-to-image} \\end{document} Use the trim option, which takes four space separated values.\ntrim={\u0026lt;left\u0026gt; \u0026lt;lower\u0026gt; \u0026lt;right\u0026gt; \u0026lt;upper\u0026gt;} Source","title":"crop included graphics in latex"},{"content":"use \u0026ldquo;screen\u0026rdquo; utility\nctrl-a-p - switch ctrl-a-c - new ctrl-a-Q - remove freeze ctrl-a-s - freeze ctrl-a-S - split, but doesn;t work see also: using tmux\n","permalink":"https://serge-m.github.io/posts/multiple-windows-in-one-linux-terminal/","summary":"use \u0026ldquo;screen\u0026rdquo; utility\nctrl-a-p - switch ctrl-a-c - new ctrl-a-Q - remove freeze ctrl-a-s - freeze ctrl-a-S - split, but doesn;t work see also: using tmux","title":"multiple \"windows\" in one linux terminal"},{"content":"OMG!! I finally fixed it. Under Windows latex2rtf diisplayed following error:\nlatex2png: error: eps2eps failed to translate l2r_00123.pdf to l2r_00123.eps for each image in my latex file.\nThat was very sad because this method is used to convert all equations, fugures and tables in word format.\nThe configurations seemed ok:\nThe problem was in eps2eps script in miktex. This script is magical. I haven\u0026rsquo;t completely understood why it does nothing when called from latex2rtf. At the same time it runs successfuly when running standalone.\ni wrote my new script pdf2eps_my.bat with the following contents:\n\u0026quot;C:\\Program Files (x86)\\gs\\gs9.05\\bin\\gswin32c.exe\u0026quot; -q -dNOCACHE -dNOPAUSE -dBATCH -dSAFER -sDEVICE=epswrite -sOutputFile=%2 %1 and changed script\n\u0026quot;c:\\Program Files (x86)\\latex2rtf\\latex2png\u0026quot; I changed\nPDF2EPS=\u0026quot;eps2eps\u0026quot; with\nPDF2EPS=\u0026quot;pdf2eps_my\u0026quot; it works because miktex/bin directory is in my PATH\n","permalink":"https://serge-m.github.io/posts/fixed-latex2png-error-eps2eps-failed-to/","summary":"OMG!! I finally fixed it. Under Windows latex2rtf diisplayed following error:\nlatex2png: error: eps2eps failed to translate l2r_00123.pdf to l2r_00123.eps for each image in my latex file.\nThat was very sad because this method is used to convert all equations, fugures and tables in word format.\nThe configurations seemed ok:\nThe problem was in eps2eps script in miktex. This script is magical. I haven\u0026rsquo;t completely understood why it does nothing when called from latex2rtf.","title":"Fixed \"latex2png: error: eps2eps failed to translate .pdf to .eps\" message in latex2rtf"},{"content":"Depth map EstimationAshutosh Saxena, Sung H. Chung, and Andrew Y. Ng “Learning Depth from Single Monocular Images” In Proc. NIPS, 2005Depth estimation using machine learning from monocular cueues. Uses MRF model ","permalink":"https://serge-m.github.io/posts/papers-review/","summary":"Depth map EstimationAshutosh Saxena, Sung H. Chung, and Andrew Y. Ng “Learning Depth from Single Monocular Images” In Proc. NIPS, 2005Depth estimation using machine learning from monocular cueues. Uses MRF model ","title":"Papers review"},{"content":"http://www.c-plusplus.de/forum/\nhttp://cvisioncentral.com/jobs/\nOpen positions in computer vision in german universities: http://hci.iwr.uni-heidelberg.de/Links/German_Vision/\nBrox\u0026rsquo;s lab. Need at least one first authon in good conference http://lmb.informatik.uni-freiburg.de/jobs/OpeningERC_postdoc.html\nAbout salaries in German universities (TV L 13) http://de.wikipedia.org/wiki/Tarifvertrag_f%C3%BCr_den_%C3%B6ffentlichen_Dienst_der_L%C3%A4nder\n","permalink":"https://serge-m.github.io/posts/german-resources/","summary":"http://www.c-plusplus.de/forum/\nhttp://cvisioncentral.com/jobs/\nOpen positions in computer vision in german universities: http://hci.iwr.uni-heidelberg.de/Links/German_Vision/\nBrox\u0026rsquo;s lab. Need at least one first authon in good conference http://lmb.informatik.uni-freiburg.de/jobs/OpeningERC_postdoc.html\nAbout salaries in German universities (TV L 13) http://de.wikipedia.org/wiki/Tarifvertrag_f%C3%BCr_den_%C3%B6ffentlichen_Dienst_der_L%C3%A4nder","title":"German resources"},{"content":"Smartphone Application for driver assistance. It can measure distance to the next vehicle for example. Or detect road lane. And warn about violating it. http://www.acodriver-shop.com/\n   Some time ago I also had such an idea. Very interesting\n","permalink":"https://serge-m.github.io/posts/excelent-idea-moves-towards-real-life/","summary":"Smartphone Application for driver assistance. It can measure distance to the next vehicle for example. Or detect road lane. And warn about violating it. http://www.acodriver-shop.com/\n   Some time ago I also had such an idea. Very interesting","title":"Excelent idea moves towards a real life"},{"content":"http://www.snowflakecreative.co.uk/\n","permalink":"https://serge-m.github.io/posts/cool-3d-web-design/","summary":"http://www.snowflakecreative.co.uk/","title":"Cool 3d web design"},{"content":"Convenient json formatting http://www.yellowduck.be/geek-stuff/2013/3/9/formatting-json-and-xml-with-sublime-text\n","permalink":"https://serge-m.github.io/posts/about-sublime/","summary":"Convenient json formatting http://www.yellowduck.be/geek-stuff/2013/3/9/formatting-json-and-xml-with-sublime-text","title":"About sublime"},{"content":"Finally I solved the problem with broken clang compilation I always got message\n \u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;font-family: Courier New, Courier, monospace;\u0026quot;\u0026gt;ld: malformed archive TOC entry for \u0026lt;long strange identifier\u0026gt;, offset 362137760 is beyond end of file 303710208\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;font-family: Courier New, Courier, monospace;\u0026quot;\u0026gt;\u0026amp;nbsp;for architecture x86_64\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;font-family: Courier New, Courier, monospace;\u0026quot;\u0026gt;clang: error: linker command failed with exit code 1 (use -v to see invocation)\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;  when I interrupted compilation of a large project. And the only way to build a project was to clean and build again. It took a long time.\u0026nbsp;I found out I don;t need rebuild all if I delete all files from Debug directory. After that Xcode only relinks the project. It is much faster. ","permalink":"https://serge-m.github.io/posts/clang-error-linker-command-failed-with/","summary":"Finally I solved the problem with broken clang compilation I always got message\n \u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;font-family: Courier New, Courier, monospace;\u0026quot;\u0026gt;ld: malformed archive TOC entry for \u0026lt;long strange identifier\u0026gt;, offset 362137760 is beyond end of file 303710208\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;font-family: Courier New, Courier, monospace;\u0026quot;\u0026gt;\u0026amp;nbsp;for architecture x86_64\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;\u0026lt;span style=\u0026quot;font-family: Courier New, Courier, monospace;\u0026quot;\u0026gt;clang: error: linker command failed with exit code 1 (use -v to see invocation)\u0026lt;/span\u0026gt;\u0026lt;/div\u0026gt;\u0026lt;div class=\u0026quot;p1\u0026quot;\u0026gt;  when I interrupted compilation of a large project.","title":"clang: error: linker command failed with exit code 1"},{"content":"Very useful for determining what text is encoded for example in unicode characters/ http://2cyr.com/decode/\n","permalink":"https://serge-m.github.io/posts/cyrillic-characters-decoder/","summary":"Very useful for determining what text is encoded for example in unicode characters/ http://2cyr.com/decode/","title":"Cyrillic characters decoder"},{"content":"подходят к концу\nyandex (skype) asoft (1+тз) dressformer (1+не делал тз) samsung research center (2) s1 (1+тз)  (1) - идиоты Infowatch (не помню) mail.ru (2) align technology (phone+1) huawei (skype) performance lab (2) rocket jump (тз+1) nvidia (1)\nartec (тз+1) Был в прикольной компании. Занимаются разработкой и продажей 3д сканеров. Декларируют очень высокое разрешение и точность - до \u0026nbsp;долей миллиметра. Есть две модели сканера для совсем близких расстояний и для расстояний побольше. Есть свой софт, который собирает данные с сенсора, объединяеефкеут независимые отсканированные куски и может делать некоторую обработку (см сайт, там можно даже триалку скачать). Демки выглядят очень прикольно. http://www.artec3d.com/ - сраница о сканерах http://artec-group.com/ - все проекты У компании есть офисы в штатах и в люксембурге. Софт может также работать с данными с кинекта. Естественно, со всеми вытекающими ограничениями по точности. Они сделали версию для кинекта, чтобы попиариться. И похоже, им это удалось. Их софтом похоже интересуются много где в мире. В первую очередь те, кто работает с 3д принтерами. Есть проекты по распознаванию лица для систем безопасности и проект по созданию игрушки-фигурки человека. В последнем случае пользователь скачивает нужный софт, подключает кинект к компу, сканирует себя и получает 3д модель. Он может также отправить 3д модель к одному из дистрибьютеров и через 5 дней получить по почте свою фигурку. Нужно было делать тестовое задание - внешняя сортировка. Мне хватило посмотреть видео ШАДа на эту тему. Я написал кривенько, но сообщил об этом. На собеседовании обсудили, как можно ускорить решение, как можно распараллелить, что можно улучшить в дизайне кода. Немного побеседовани на тему виртуальных функций, виртуальных функций в конструкторе/деструкторе, шаблонов проектирования, антишаблонов, ромбовидного наследования, процесса разработки. микро-задачка на битовую длинну числа.\u0026nbsp;  ","permalink":"https://serge-m.github.io/posts/in-russian/","summary":"подходят к концу\nyandex (skype) asoft (1+тз) dressformer (1+не делал тз) samsung research center (2) s1 (1+тз)  (1) - идиоты Infowatch (не помню) mail.ru (2) align technology (phone+1) huawei (skype) performance lab (2) rocket jump (тз+1) nvidia (1)\nartec (тз+1) Был в прикольной компании. Занимаются разработкой и продажей 3д сканеров. Декларируют очень высокое разрешение и точность - до \u0026nbsp;долей миллиметра. Есть две модели сканера для совсем близких расстояний и для расстояний побольше.","title":"Собеседования (in Russian)"},{"content":"radare, the reverse engineering framework [http://radare.nopcode.org/y/?p=features](http://radare.nopcode.org/y/?p=features) 010 Editorhex templates for binary formats parsing[http://www.sweetscape.com/](http://www.sweetscape.com/) [http://habrahabr.ru/post/213211/](http://habrahabr.ru/post/213211/)\u0026nbsp;(rus) ","permalink":"https://serge-m.github.io/posts/hex-editors/","summary":"radare, the reverse engineering framework [http://radare.nopcode.org/y/?p=features](http://radare.nopcode.org/y/?p=features) 010 Editorhex templates for binary formats parsing[http://www.sweetscape.com/](http://www.sweetscape.com/) [http://habrahabr.ru/post/213211/](http://habrahabr.ru/post/213211/)\u0026nbsp;(rus) ","title":"Hex editors"},{"content":"random numbers http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3551.pdf\nSutter. C++11 features http://herbsutter.com/elements-of-modern-c-style/\n","permalink":"https://serge-m.github.io/posts/c11-stuff/","summary":"random numbers http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3551.pdf\nSutter. C++11 features http://herbsutter.com/elements-of-modern-c-style/","title":"C++11 stuff"},{"content":"screencasts with solutions of typical interview tasks https://www.youtube.com/user/ProgrammingInterview/\nGayle Laakmann McDowell. Cracking the Coding Interview: 150 Programming Questions and Solutions http://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/098478280X\n140 Google Interview Questions http://www.impactinterview.com/2009/10/140-google-interview-questions/\n","permalink":"https://serge-m.github.io/posts/programming-interviews-resources/","summary":"screencasts with solutions of typical interview tasks https://www.youtube.com/user/ProgrammingInterview/\nGayle Laakmann McDowell. Cracking the Coding Interview: 150 Programming Questions and Solutions http://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/098478280X\n140 Google Interview Questions http://www.impactinterview.com/2009/10/140-google-interview-questions/","title":"Programming interviews resources"},{"content":"create table named t3 (id, value). id is integer. value is text sqlite\u0026gt; create table if not exists t3( id int , value varchar, primary key(id)); adding two \u0026ldquo;rows\u0026rdquo; sqlite\u0026gt; insert into t3 (id, value) values (1, 'a'); sqlite\u0026gt; insert into t3 (id, value) values (2, 'b'); display results sqlite\u0026gt; select * \u0026amp;nbsp;from t3; 1|a 2|b update id=1 if exists, insert otherwise (id=1 exists) sqlite\u0026gt; insert or replace into t3 (id,value) values (1,\u0026lsquo;c\u0026rsquo; ); sqlite\u0026gt; select * from t3; 2|b 1|c\nupdate id=1 if exists, insert otherwise (id=1 doesn\u0026rsquo;t exist) sqlite\u0026gt; insert or replace into t3 (id,value) values (3,\u0026lsquo;e\u0026rsquo; ); sqlite\u0026gt; select * from t3; 2|b 1|c 3|e\nupdate id=1 if exists, insert otherwise (id=1 doesn\u0026rsquo;t exist) sqlite\u0026gt; insert or replace into t3 (id,value) values (3,\u0026lsquo;f\u0026rsquo; ); sqlite\u0026gt; select * from t3; 2|b 1|c 3|f\n","permalink":"https://serge-m.github.io/posts/learning-sqlite/","summary":"create table named t3 (id, value). id is integer. value is text sqlite\u0026gt; create table if not exists t3( id int , value varchar, primary key(id)); adding two \u0026ldquo;rows\u0026rdquo; sqlite\u0026gt; insert into t3 (id, value) values (1, 'a'); sqlite\u0026gt; insert into t3 (id, value) values (2, 'b'); display results sqlite\u0026gt; select * \u0026amp;nbsp;from t3; 1|a 2|b update id=1 if exists, insert otherwise (id=1 exists) sqlite\u0026gt; insert or replace into t3 (id,value) values (1,\u0026lsquo;c\u0026rsquo; ); sqlite\u0026gt; select * from t3; 2|b 1|c","title":"learning sqlite"},{"content":"Today I finally found out that screen resolution for remote session in Windows can be changed in settings menu before connection. Little discoveries happen every day\u0026hellip; Damn\u0026hellip;.\n","permalink":"https://serge-m.github.io/posts/remote-desktop-screen-resolution/","summary":"Today I finally found out that screen resolution for remote session in Windows can be changed in settings menu before connection. Little discoveries happen every day\u0026hellip; Damn\u0026hellip;.","title":"remote desktop screen resolution"},{"content":"Руководство по отладке многопоточных приложений в Visual Studio 2010. video, exampleshttp://habrahabr.ru/post/97817/\nСчитаем Пи параллельно. Часть 1. pthread, openmp, c++11 in commentshttp://habrahabr.ru/post/147796/\nПотоки, блокировки и условные переменные в C++11 [Часть 1], exceptions, exampleshttp://habrahabr.ru/post/182610/\nПотоки, блокировки и условные переменные в C++11 [Часть 2][http://habrahabr.ru/post/182626/](http://habrahabr.ru/post/182626/) ","permalink":"https://serge-m.github.io/posts/multithreading-links/","summary":"Руководство по отладке многопоточных приложений в Visual Studio 2010. video, exampleshttp://habrahabr.ru/post/97817/\nСчитаем Пи параллельно. Часть 1. pthread, openmp, c++11 in commentshttp://habrahabr.ru/post/147796/\nПотоки, блокировки и условные переменные в C++11 [Часть 1], exceptions, exampleshttp://habrahabr.ru/post/182610/\nПотоки, блокировки и условные переменные в C++11 [Часть 2][http://habrahabr.ru/post/182626/](http://habrahabr.ru/post/182626/) ","title":"Multithreading links"},{"content":"What do I know about VapourSynth\nhttp://www.vapoursynth.com/ - website\nHow to save data from VapourSynthYou need to use vspipe.exe from Vapour distributiveIt seems it returns raw data to stdout Render using ffmpegDon't know how jet Render using ImageMagick/convertDon't know how jet Image ReadingImage reading causes crash of AvsPmod, in which I using Vapoursynth[http://forum.doom9.org/showthread.php?t=166088](http://forum.doom9.org/showthread.php?t=166088)src:\u0026nbsp;[https://github.com/chikuzen/vsimagereader](https://github.com/chikuzen/vsimagereader\" target=\"_blank) Other AvsPmod - new version of AvsP. AvsP is a tabbed avs editor with convenient results preview. [http://avspmod.github.io/](http://avspmod.github.io/) In version 2.4.0 temporary support of vapour scripts was added to AvsPmod (http://forum.doom9.org/showthread.php?t=153248) And it\u0026rsquo;s true.\nMy first working script:\nimport vapoursynth as vs #include vapour module core = vs.get_core() # some core loading src = core.avisource.AVISource(\u0026lsquo;frm.avi\u0026rsquo;) # opening frm.avi # it doesn\u0026rsquo;t work without whis line and # prints error message \u0026ldquo;Avisynth open failure:  # VFW module doesn\u0026rsquo;t support RGB24 output\u0026rdquo; ret = core.resize.Bicubic(src, format=vs.COMPATBGR32)\nret.set_output()\nInteresting bug: For RGB24 avi file it outputs image flipped vertically: In YUV format it\u0026rsquo;s ok.\nAvxSynth - linux port of Avisynth\nhttps://github.com/chikuzen/VapourSource - avisynth plugin for loading VapourSynth scripts\n It seems VapourSynth developer is little bit crazy: he wants everysthing be in C instead of C++ (http://www.vapoursynth.com/2012/11/vapoursynth-tasks/)  ","permalink":"https://serge-m.github.io/posts/vapoursynth-pythonic-alternative-to/","summary":"What do I know about VapourSynth\nhttp://www.vapoursynth.com/ - website\nHow to save data from VapourSynthYou need to use vspipe.exe from Vapour distributiveIt seems it returns raw data to stdout Render using ffmpegDon't know how jet Render using ImageMagick/convertDon't know how jet Image ReadingImage reading causes crash of AvsPmod, in which I using Vapoursynth[http://forum.doom9.org/showthread.php?t=166088](http://forum.doom9.org/showthread.php?t=166088)src:\u0026nbsp;[https://github.com/chikuzen/vsimagereader](https://github.com/chikuzen/vsimagereader\" target=\"_blank) Other AvsPmod - new version of AvsP. AvsP is a tabbed avs editor with convenient results preview.","title":"VapourSynth: pythonic alternative to avisynth"},{"content":"I used _tabularx _package. Additional definitions:\n \\usepackage{array} \\newcolumntype{L}{\u0026gt;{\\raggedright\\arraybackslash}X} % left multiline alignment \\newcolumntype{R}{\u0026gt;{\\raggedleft\\arraybackslash}X} % right multiline alignment Table:\n \\begin{tabularx}{\\textwidth}{@{}p{0.8\\linewidth} R} Text, aligned to the left, without margin \\newline Tex in the same cell on the next line \u0026amp;amp; % next column delimiter Text aligned to the right \\newline second line \\end{tabularx} Here we have one columt with 80% width and one column with right alignment @{} is required to suppress left margin in the first column.\nResults:\n","permalink":"https://serge-m.github.io/posts/tables-in-latex-multiline-right/","summary":"I used _tabularx _package. Additional definitions:\n \\usepackage{array} \\newcolumntype{L}{\u0026gt;{\\raggedright\\arraybackslash}X} % left multiline alignment \\newcolumntype{R}{\u0026gt;{\\raggedleft\\arraybackslash}X} % right multiline alignment Table:\n \\begin{tabularx}{\\textwidth}{@{}p{0.8\\linewidth} R} Text, aligned to the left, without margin \\newline Tex in the same cell on the next line \u0026amp;amp; % next column delimiter Text aligned to the right \\newline second line \\end{tabularx} Here we have one columt with 80% width and one column with right alignment @{} is required to suppress left margin in the first column.","title":"Tables in Latex. Multiline, right alignment"},{"content":"How does start menu in Win7 select the most useless new application i\u0026rsquo;ve installed?\n","permalink":"https://serge-m.github.io/posts/how/","summary":"How does start menu in Win7 select the most useless new application i\u0026rsquo;ve installed?","title":"How?"},{"content":"Problems with ConEmu building from\u0026nbsp;https://github.com/Maximus5/ConEmu.git v14.01.06in Visual Studio 2010ConEmuCunresolved external symbol `__imp__`wsprintfA- add additional dependency\u0026nbsp;**User32.lib**   **ConEmuCD**Error3error LNK2001: unresolved external symbol `__imp__`CharUpperBuffW@8 Error4error LNK2001: unresolved external symbol `__imp__`MapVirtualKeyW@8 Error5error LNK2001: unresolved external symbol `__imp__`VkKeyScanW@4 Error6error LNK2001: unresolved external symbol `__imp__`GetSystemMetrics@4 Error7error LNK2001: unresolved external symbol `__imp__`IsRectEmpty@4 Error8error LNK2001: unresolved external symbol `__imp__`MonitorFromRect@8 Error9error LNK2001: unresolved external symbol `__imp__`GetMonitorInfoW@8 Error10error LNK2001: unresolved external symbol `__imp__`MonitorFromWindow@8 Error11error LNK2001: unresolved external symbol `__imp__`SystemParametersInfoW@16 etc. Error101error LNK1120: 98 unresolved externals\nadd User32.lib to reduce number of warnings from 99 to 32\nError3error LNK2001: unresolved external symbol __imp__LogonUserW@24 Error4error LNK2001: unresolved external symbol __imp__RegCreateKeyExW@36 Error5error LNK2001: unresolved external symbol __imp__RegQueryValueExW@24 Error6error LNK2001: unresolved external symbol __imp__CreateCompatibleDC@4\nadd **advapi32.lib **to reduce number of warnings from 32 to 14\n Error3error LNK2001: unresolved external symbol `__imp__`SHGetFolderPathW@20Error4error LNK2001: unresolved external symbol `__imp__`ShellExecuteExW@4Error5error LNK2001: unresolved external symbol `__imp__`ShellExecuteW@24 add gdi32.lib to reduce number of errors from 14 to 3.add shell32.lib to reduce number of errors to 0 ConEmuHkError3error LNK2001: unresolved external symbol `__imp__`CharUpperBuffW@8Error4error LNK2001: unresolved external symbol `__imp__`MapVirtualKeyW@8Error5error LNK2001: unresolved external symbol `__imp__`VkKeyScanW@4Error6error LNK2001: unresolved external symbol `__imp__`GetCursorPos@4Error7error LNK2001: unresolved external symbol `__imp__`MapVirtualKeyExW@12..... Add\u0026nbsp;User32.lib to reduce from 20 to 4: Error3error LNK2001: unresolved external symbol `__imp__`RegOpenKeyExW@20Error4error LNK2001: unresolved external symbol `__imp__`RegCloseKey@4Error5error LNK2001: unresolved external symbol `__imp__`LogonUserW@24 Add\u0026nbsp;advapi32.lib to fix all Bonus:links to Russian habrahabr:http://habrahabr.ru/company/epam_systems/blog/204368/ ","permalink":"https://serge-m.github.io/posts/building-conemu/","summary":"Problems with ConEmu building from\u0026nbsp;https://github.com/Maximus5/ConEmu.git v14.01.06in Visual Studio 2010ConEmuCunresolved external symbol `__imp__`wsprintfA- add additional dependency\u0026nbsp;**User32.lib**   **ConEmuCD**Error3error LNK2001: unresolved external symbol `__imp__`CharUpperBuffW@8 Error4error LNK2001: unresolved external symbol `__imp__`MapVirtualKeyW@8 Error5error LNK2001: unresolved external symbol `__imp__`VkKeyScanW@4 Error6error LNK2001: unresolved external symbol `__imp__`GetSystemMetrics@4 Error7error LNK2001: unresolved external symbol `__imp__`IsRectEmpty@4 Error8error LNK2001: unresolved external symbol `__imp__`MonitorFromRect@8 Error9error LNK2001: unresolved external symbol `__imp__`GetMonitorInfoW@8 Error10error LNK2001: unresolved external symbol `__imp__`MonitorFromWindow@8 Error11error LNK2001: unresolved external symbol `__imp__`SystemParametersInfoW@16 etc.","title":"Building ConEmu"},{"content":"CamStudio 2.7 http://camstudio.org/ works fine\nRylstim-Screen-Recorder http://www.sketchman-studio.com/rylstim-screen-recorder/\nThere were some troubles while recording ConEmu software Portable version is available\nHyperCam 2 http://www.hyperionics.com/hc/\nWorks well\n","permalink":"https://serge-m.github.io/posts/free-software-for-screen-video-capture/","summary":"CamStudio 2.7 http://camstudio.org/ works fine\nRylstim-Screen-Recorder http://www.sketchman-studio.com/rylstim-screen-recorder/\nThere were some troubles while recording ConEmu software Portable version is available\nHyperCam 2 http://www.hyperionics.com/hc/\nWorks well","title":"Free software for screen video capture"},{"content":"Add to .gitignore:\n *.ipch Debug Release *.sdf - The SDF file is your code browsing database which uses SQL Sever Compact Edition. You don\u0026rsquo;t need to copy this SDF file while you move your project, and it will be automatically populated in the new location once you open your project.  [[1](http://social.msdn.microsoft.com/Forums/en-US/20fee924-e267-4c1a-b0fe-3321f86e1bb5/sdf-file?forum=vcprerelease\u0026quot; target=\u0026quot;_blank)]\n[[2](http://social.msdn.microsoft.com/Forums/vstudio/en-US/1ef46540-e4b8-4779-8403-49239bc3f7ee/is-it-safe-to-delete-ipch-folder-precompiled-headers?forum=vcgeneral\u0026quot; target=\u0026quot;_blank)]\n","permalink":"https://serge-m.github.io/posts/temporary-files-format-that-can-be/","summary":"Add to .gitignore:\n *.ipch Debug Release *.sdf - The SDF file is your code browsing database which uses SQL Sever Compact Edition. You don\u0026rsquo;t need to copy this SDF file while you move your project, and it will be automatically populated in the new location once you open your project.  [[1](http://social.msdn.microsoft.com/Forums/en-US/20fee924-e267-4c1a-b0fe-3321f86e1bb5/sdf-file?forum=vcprerelease\u0026quot; target=\u0026quot;_blank)]\n[[2](http://social.msdn.microsoft.com/Forums/vstudio/en-US/1ef46540-e4b8-4779-8403-49239bc3f7ee/is-it-safe-to-delete-ipch-folder-precompiled-headers?forum=vcgeneral\u0026quot; target=\u0026quot;_blank)]","title":"Temporary files format that can be deleted from project of Visual Studio 2010"},{"content":"There is a lot of answers here: http://stackoverflow.com/questions/79165/how-to-migrate-svn-with-history-to-a-new-git-repository\nI found rather useful [this one](http://stackoverflow.com/a/9316931\u0026quot; target=\u0026quot;_blank). A guy made a script according to proposed instructions: https://github.com/onepremise/SGMS\nThis script will convert projects stored in SVN with the following format:\n/trunk /Project1 /Project2 /branches /Project1 /Project2 /tags /Project1 /Project2 This scheme is also popular and supported as well: /Project1 /trunk /branches /tags /Project2 /trunk /branches /tags Each project will get synchronized over by project name:\nEx: ./migration https://svnurl.com/basepath project1\nIf you wish to convert the full repo over, use the following syntax:\nEx: ./migration https://svnurl.com/basepath .\nI tested on the second structure type and it works. The question is only about saving merge structure. It seems it was lost. :( Branches are ok, but merged revisions are not marked as merged. In other words every revision has single parent\nOther solutions: https://github.com/nirvdrum/svn2git\nhttp://blog.tfnico.com/2011/12/git-svn-mirror-product-subgit.html\nI haven\u0026rsquo;t tested them.\nBackup SVN if the repository is not local. Dump whole repo:\nsvnrdump dump https://\u0026lt;your svn repo path\u0026gt;/ \u0026gt; dump.dump then create a new local repo\nsvnadmin create newrepo loading dump to local repo:\nsvnadmin load newrepo \u0026lt; dump.dump I had a problem with loading (there was some internal error while loading more than 8000 revisions, so i try to make it another way). You can dump in several files if you specify revisions ranges:\nsvnrdump dump https://\u0026lt;your svn repo path\u0026gt;/ -r 0:1000 \u0026gt; dump0000-1000.dump svnrdump dump https://\u0026lt;your svn repo path\u0026gt;/ -r 1001:2000 --incremental \u0026gt; dump1001-2000-incremental.dump svnrdump dump https://\u0026lt;your svn repo path\u0026gt;/ -r 2001:3000 --incremental \u0026gt; dump2001-3000-incremental.dump All dumps except the first are incremental\nthen create a new local repo\nsvnadmin create newrepo then loading:\nsvnadmin load newrepo \u0026lt; dump0001-1000.dump svnadmin load newrepo \u0026lt; dump1001-2000-incremental.dump svnadmin load newrepo \u0026lt; dump2001-3000-incremental.dump SVN server on localhost I used VisualSVN Server to make local SVN server. It seems Hg cannot load SVN repo from local file, but it can work with local server. I added a new user, copied ready newrepo to a directory where VisualSVN Server stores repositories, assigned permissions to the repo and it worked.\nMigrating to Hg in russian: http://pdrobushevich.blogspot.ru/2010/10/hgsubversion.html\nSee also  gitignore files for different projects  ","permalink":"https://serge-m.github.io/posts/migrating-from-svn-to-git/","summary":"There is a lot of answers here: http://stackoverflow.com/questions/79165/how-to-migrate-svn-with-history-to-a-new-git-repository\nI found rather useful [this one](http://stackoverflow.com/a/9316931\u0026quot; target=\u0026quot;_blank). A guy made a script according to proposed instructions: https://github.com/onepremise/SGMS\nThis script will convert projects stored in SVN with the following format:\n/trunk /Project1 /Project2 /branches /Project1 /Project2 /tags /Project1 /Project2 This scheme is also popular and supported as well: /Project1 /trunk /branches /tags /Project2 /trunk /branches /tags Each project will get synchronized over by project name:","title":"Migrating from SVN to Git and Mercurial"},{"content":"rules for resolving calls to overloaded functions:\u0026nbsp;- identify the function that's the best match for the call (name, parameters etc)checks accessibility for the best-match function.  Resolving function overloads (my) ![](http://4.bp.blogspot.com/-QLJfqdOD7GI/UuKOhiNr9CI/AAAAAAAAAbo/zxi4-98NBwY/s1600/functionsOverloadingSchemeCPP.png) Resolving function overloadssrc:[http://www.dcs.bbk.ac.uk/~roger/cpp/week20.htm](http://www.dcs.bbk.ac.uk/~roger/cpp/week20.htm\" target=\"_blank) The compiler works through the following checklist and if it still can't reach a decision, it issues an error:Gather all the functions in the current scope that have the same name as the function called.Exclude those that don't have the right number of parameters to match the arguments in the call. (It has to be careful about parameters with default values;\u0026nbsp;void f(int x, int y = 0)\u0026nbsp;is a candidate for the call\u0026nbsp;f(25);)If no function matches, the compiler reports an error.If there is more than one match, select the 'best match'.If there is no clear winner of the best matches, the compiler reports an error - ambiguous function call.Best matchingIn deciding on the best match, the compiler works on a rating system for the way the types passed in the call and the competing parameter lists match up. In decreasing order of goodness of match:An exact match, e.g. argument is a double and parameter is a doubleA promotionA standard type conversionA constructor or user-defined type conversionExact matchesAn exact match is where the parameter and argument datatypes match exactly. Note that, for the purposes of overload resolution a pointer to an array of type x exactly matches a pointer of type x. This is because arrays are always passed by reference, meaning that you actually pass a pointer to the first element of the array. For example:void f(int y[ ]); // call this f1 void f(int* z); // call this f2 .... int x[ ] = {1, 2, 3, 4}; f(x); // Both f1 and f2 are exact matches, so the call is ambiguous. void sf(const char s[]); void sf(const char*); \u0026hellip;. sf(\u0026ldquo;abc\u0026rdquo;); // Same problem; both sf functions are exact matches. Type promotionThe following are described as \u0026ldquo;promotions\u0026rdquo;:A char, unsigned char or short can be promoted to an int. For example void f(int); can be a match for f(\u0026lsquo;a\u0026rsquo;);A float can be promoted to a double.A bool can be promoted to an int (FALSE counts as 0, TRUE as 1).Standard conversionsAll the following are described as \u0026ldquo;standard conversions\u0026rdquo;:conversions between integral types, apart from the ones counted as promotions. Remember that bool and char are integral types as well as int, short and long.conversions between floating types: double, float and long double, except for float to double which counts as a promotion.conversions between floating and integral typesconversions of integral, floating and pointer types to bool (zero or NULL is FALSE, anything else is TRUE)conversion of an integer zero to the NULL pointer.All of the standard conversions are treated as equivalent for scoring purposes. A seemingly minor standard conversion, such as int to long, does not count as any \u0026ldquo;better\u0026rdquo; than a more drastic one such as double to bool.Constructors and user-defined conversionsA certain kind of constructor can play a special role in type conversion. Suppose you had a class Bigint that was capable of storing integral numbers larger than INT_MAX, and you had a constructor for a Bigint that took a C-string, so that a declaration of a Bigint object might look like this:Bigint b(\u0026ldquo;12345678901234567890\u0026rdquo;); This constructor also provides an implicit type conversion. Having defined b as a Bigint, it would be possible to say, for example:b = \u0026ldquo;999999999999\u0026rdquo;; or you could invoke the type-conversion explicitly:\nb = static_cast(\"88888888888\"); The same type-conversion would be used in parameter passing, enabling us to do this:void f(Bigint); f(\u0026ldquo;77777777777777\u0026rdquo;); We are able to pass a C-string to a function that expects a Bigint because there exists a Bigint constructor that takes a C-string.\nThis kind of conversion can only work when the constructor can be called with just one argument. Generally this means that the constructor will have just one parameter, but it could have more if all but the first of the parameters (or, indeed, all of them) had default values.When you think about using this type-conversion in assignment, it is obvious that the constructor must be of the kind that can be called with only one argument. In the statement:b = something; the \"something\" can't be nothing, and it can't be more than one thing. In the context of parameter passing, the compiler has to be able to find a parameter for each argument. It would not consider\u0026nbsp;void f(Bigint)\u0026nbsp;as a candidate for\u0026nbsp;f(\"6666666\", 57);\u0026nbsp;even if the Bigint class had a constructor that took a C-string and an int - the compiler needs one parameter for the \"6666666\" and another for the 57.The compiler also has to find an argument for each parameter that is not given an explicit default in the function header, so it would not consider\u0026nbsp;void f(Bigint)\u0026nbsp;as a candidate for\u0026nbsp;f();\u0026nbsp;even if Bigint had a constructor with no parameters or a constructor with all its parameters defaulted. (It would, however, consider\u0026nbsp;void f(Bigint b = \"0\")\u0026nbsp;as a candidate for\u0026nbsp;f();\u0026nbsp;since the parameter of the function\u0026nbsp;f\u0026nbsp;(as opposed to the parameter of the Bigint constructor) has an explicit default.)These \"conversion constructors\" enable us to have object parameters corresponding to arguments of other types.User-defined conversions\u0026nbsp;are for going the other way. They enable us to pass objects (as arguments) to functions with parameters of other types, as in the following:void f(int n); Bigint b(\u0026ldquo;123456\u0026rdquo;); f(b); // would work if there was a Bigint::operator int() conversion function Conversion member-functions allow you to specify how you want objects to respond if they are asked to behave as if they were objects of some other type. In this case, a Bigint is being treated as if it were an int. A conversion function that allowed the above code to work might take the form:class Bigint { public: \u0026hellip; operator int(); // returns a Bigint as an int. \u0026hellip; // Note there is no return type and no parameter list. };\nBigint::operator int() { If the value of the Bigint is less than INT_MAX, return the value as an int, else return -1; // or throw an exception or something }explicitIt can happen that you have a constructor that can be called with just one argument, and which will therefore behave as a conversion constructor, but you don\u0026rsquo;t want it to behave in this way. Perhaps we have a Bigint constructor that takes a string. This would mean that you could write something like:Bigint b; b = \u0026ldquo;987654321\u0026rdquo;; But implicit type-conversions are a common source of programming error, so you might decide to disallow that sort of conversion. We can do that simply by inserting the keyword explicitbefore the constructor prototype in the class definition:\nclass Bigint { public: ...... explicit Bigint(string); ...... }; With that\u0026nbsp;explicit\u0026nbsp;before the constructor, we cannot now make use of implicit type-conversion from C-string to Bigint: void proc(Bigint); Bigint b; b = \"99999999999\"; // Error! Implicit type conversion not allowed proc(\"9999999999999\"); // not allowed b = static_cast(\"999999999999\"); // OK, type conversion explicit proc(Bigint(\"999999999999\")); // OK A note on const ref parametersSuppose that the Bigint class had a constructor that took an int. It would then be possible to pass an int to a function that expected a Bigint:void proc(Bigint bx) { \u0026hellip;\u0026hellip; }\nproc(54321); The parameter here was a value parameter. Would it have made any difference if the parameter had been a const ref, i.e.\nvoid proc(const Bigint\u0026amp; bx) { ...... } proc(54321); At first sight, this looks rather strange. When you pass an argument by reference, the parameter name becomes an alternative to the argument name - two names for the same thing. But here, we are not passing anything that has a name; we are simply passing a value. To put it more technically, the argument has an r-value but not an l-value (you could use it on the right-hand side of an assignment but not on the left-hand side). There is, apparently, nothing for bx to refer to.But in fact it would work. When you pass an argument that has an l-value to a const ref parameter, you get the ordinary call-by-reference (except, obviously, that the parameter is const). When you pass an argument that has only an r-value, a temporary variable is created with the same type as the parameter, and the const ref parameter refers to this temporary variable. So, in this example, a temporary variable of type Bigint is created, the value 54321 is used to initialise it, and this is the object to which bx refers. The temporary variable disappears when proc terminates. (In other words, when the argument has no l-value, the const ref parameter behaves very like a value parameter.)Parameter lists that include default valuesParameters with default values carry their full weight in the scoring of a function. For example:void f (int x, double y, int z = 2); // (f1) void f (int x, int y); // (f2) \u0026hellip; f(3, 4.5); // matches f1 exactly, whereas f2 requires a double-to-int standard conversion Choosing a winnerA candidate function is only as strong as its weakest match; a candidate requiring three promotions, for example, beats a candidate with two exact matches and a standard conversion. Candidates whose weakest matches are equivalently weak are compared on their next-weakest, and so on - a candidate with a standard conversion, a promotion and an exact match beats a candidate with a standard conversion and two promotions.\n ","permalink":"https://serge-m.github.io/posts/things-about-c/","summary":"rules for resolving calls to overloaded functions:\u0026nbsp;- identify the function that's the best match for the call (name, parameters etc)checks accessibility for the best-match function.  Resolving function overloads (my) ![](http://4.bp.blogspot.com/-QLJfqdOD7GI/UuKOhiNr9CI/AAAAAAAAAbo/zxi4-98NBwY/s1600/functionsOverloadingSchemeCPP.png) Resolving function overloadssrc:[http://www.dcs.bbk.ac.uk/~roger/cpp/week20.htm](http://www.dcs.bbk.ac.uk/~roger/cpp/week20.htm\" target=\"_blank) The compiler works through the following checklist and if it still can't reach a decision, it issues an error:Gather all the functions in the current scope that have the same name as the function called.Exclude those that don't have the right number of parameters to match the arguments in the call.","title":"Things about C++"},{"content":"Example from Meyers \u0026ldquo;Effective C++\u0026rdquo;\n class Timer { public: explicit Timer(int tickFrequency); virtual void onTick() const; // Called automatically for each tic, // onTick() must be redefined to do things ... }; class Widget: private Timer { // private inheritance private: virtual void onTick() const; // redefined to make job done ... }; Now clients of Widget get interface untouched and required job is done\nExample of protecting method from redefinition in derived classes:\n class Widget { private: class WidgetTimer: public Timer { public: virtual void onTick() const; ... }; WidgetTimer timer; ... }; Classes derived from Widget unable to redefine onClick. Analogue of final in Java and sealed in C#\n","permalink":"https://serge-m.github.io/posts/private-inheritance/","summary":"Example from Meyers \u0026ldquo;Effective C++\u0026rdquo;\n class Timer { public: explicit Timer(int tickFrequency); virtual void onTick() const; // Called automatically for each tic, // onTick() must be redefined to do things ... }; class Widget: private Timer { // private inheritance private: virtual void onTick() const; // redefined to make job done ... }; Now clients of Widget get interface untouched and required job is done\nExample of protecting method from redefinition in derived classes:","title":"Private inheritance"},{"content":"How to write c++ programm for android (in Russian) http://www.codeatcpp.com/2011/10/c-android-1.html\nhttp://sivers.org/blog\nHow to make a movement\n To translate: Source: http://habrahabr.ru/post/206258/\n2. Базар идейВ какой-то момент в нашей организации вошла в моду фраза «продать идею». Особенно активно я начал натыкаться на эти торги идеями с поднятием по карьерной лестнице, и имея частые совещания с теми, с которыми нас не связывали отношения подчинения. Фраза «продать идею» базируется на маркетинговом термине «sell idea», только понимание оного в проекции на организационную структуру было извращено. Если в оригинальном смысле продающий идею получает выгоду от тех, кто эту идею купил (инвесторы или заказчики), то в организационной структуре, продав идею, максимум можно было получить согласие купившего не мешать тебе ее реализовывать. А так как по такому выгодному курсу купить идею могут позволить себе даже неимущие, в итоге можно было потратить уйму времени, «продавая идею» тем, кто случайно оказался на совещании. Страдая на таких торгах, я часто задавался вопросом «зачем все эти люди?». Так у меня сложилось ощущение, что в одиночку сделать эту работу можно в разы быстрее, чем базаря на ярмарке идей. А ведь правда, если можешь все сделать сам, зачем кого-то убеждать в том, что это надо делать? ","permalink":"https://serge-m.github.io/posts/blogs/","summary":"How to write c++ programm for android (in Russian) http://www.codeatcpp.com/2011/10/c-android-1.html\nhttp://sivers.org/blog\nHow to make a movement\n To translate: Source: http://habrahabr.ru/post/206258/\n2. Базар идейВ какой-то момент в нашей организации вошла в моду фраза «продать идею». Особенно активно я начал натыкаться на эти торги идеями с поднятием по карьерной лестнице, и имея частые совещания с теми, с которыми нас не связывали отношения подчинения. Фраза «продать идею» базируется на маркетинговом термине «sell idea», только понимание оного в проекции на организационную структуру было извращено.","title":"Blogs"},{"content":"In a new year please continue writting bad programs, make bugs, create bad architecture and so on! All this destroying activity provide infinite flow of well-paid job! Thanks to those people! Let\u0026rsquo;s kludge together!\n[image source](http://stevenbenner.com/2010/07/the-5-types-of-programmers/\u0026quot; rel=\u0026ldquo;nofollow\u0026rdquo; target=\u0026quot;_blank)\n","permalink":"https://serge-m.github.io/posts/dear-programmers-developers-architects/","summary":"In a new year please continue writting bad programs, make bugs, create bad architecture and so on! All this destroying activity provide infinite flow of well-paid job! Thanks to those people! Let\u0026rsquo;s kludge together!\n[image source](http://stevenbenner.com/2010/07/the-5-types-of-programmers/\u0026quot; rel=\u0026ldquo;nofollow\u0026rdquo; target=\u0026quot;_blank)","title":"Dear programmers, developers, architects and others!"},{"content":"http://forums.creativecow.net/thread/2/940078\nIn adobe after effects:\n:::: #define PF_MAX_CHAN8\t255 #define PF_HALF_CHAN8\t128 #define PF_MAX_CHAN16\t32768 #define PF_HALF_CHAN16\t16384 #define CONVERT8TO16(A)\t( (((long)(A) * PF_MAX_CHAN16) + PF_HALF_CHAN8) / PF_MAX_CHAN8 )  ","permalink":"https://serge-m.github.io/posts/16-bit-color-formats/","summary":"http://forums.creativecow.net/thread/2/940078\nIn adobe after effects:\n:::: #define PF_MAX_CHAN8\t255 #define PF_HALF_CHAN8\t128 #define PF_MAX_CHAN16\t32768 #define PF_HALF_CHAN16\t16384 #define CONVERT8TO16(A)\t( (((long)(A) * PF_MAX_CHAN16) + PF_HALF_CHAN8) / PF_MAX_CHAN8 )  ","title":"16 bit color formats"},{"content":"Today kids, I tell you how to split imported Sihlouette shapes into Nuke RotoPaint nodes.\nIn SihlouetteFX select several nodes. Go File-\u0026gt;Export-\u0026gt;nuke 6.2+ Shapes and save Nuke project\nI tried to use also File-\u0026gt;Export-\u0026gt;Nuke Shapes but in Nuke 7.0 it wasn\u0026rsquo;t opened properly\nOpen saved project in Nuke. There is a single RotoPaint node. It contains several layers with several shapes in each. I need separate layers into many nodes to use them separately.\nTo do this I wrote following script:\n:::: import nuke, nuke.rotopaint as rp def rptsw_walker(obj, list): # loop wor each subitem in Rotopaint for i in obj: # if there is a layer if isinstance(i, nuke.rotopaint.Layer): # create new RotoPaint object paintNode = nuke.createNode('RotoPaint') curvesKnob = paintNode['curves'] rotoRoot = curvesKnob.rootLayer list1 = [] # for each bezier shape we add it to the list. I don't know why # adding it to a list causes deleting shape from origin # if adding directly to the new roto object some shapes were not copied for j in i: if isinstance(j, nuke.rotopaint.Shape): list1.append(j) # copy all shapes from list to new roto node for j in list1: rotoRoot.append(j) # find selected node rotoNode = nuke.selectedNode() rptsw_shapeList = [] rotoCurve = rotoNode['curves'] rotoRoot = rotoCurve.rootLayer # run rptsw_walker(rotoRoot, rptsw_shapeList)  Don\u0026rsquo;t forget to select source RotoPaint node !! Attention! This script deletes shapes from original RotoPaint!\nDoes anybody know why copying shape to another node causes deleting it from original one?\nAnother not working solution found in google: Source: http://www.mail-archive.com/nuke-python@support.thefoundry.co.uk/msg02905/convert_shapetostroke.py\n#*********************************************************** #Boundary Visual Effects - Silhouette Stroke Importer #Version 0.98 # #Created by Magno Borgo #For greeting, bugs, and requests email me at mborgo[at]boundaryvfx.com #Compatibility: Nuke 6.3 and up (not tested on previous versions) #If you like and use the script frequently, please consider a small donation via Paypal to the same email above. # #Legal stuff: #This script is provided \"as is,\" without warranty of any kind, expressed #or implied. In no event shall the author be held liable for any damages #arising in any way from the use of this script. #*********************************************************** #Changelog #v1.0 #initial release. #*********************************************************** # Usage: Import the shapes exported from Silhouette, select the roto node and the group, and run. # The stroke precision can be set on the script, increase precision if you need to subdivide the stroke further. #silhouetteStrokeImporter(2) (default = 2) #silhouetteStrokeImporter(5) #*********************************************************** import nuke, nuke.rotopaint as rp, math, re import threading, time import profile def rptsw_walker(obj, list): for i in obj: if isinstance(i, nuke.rotopaint.Shape): list.append([i, obj]) if isinstance(i, nuke.rotopaint.Layer): list.append([i, obj]) rptsw_walker(i, list) return list def strokeMap(value): return 1 def copyTransforms(oldT,newT): oldT = oldT.getTransform() newT = newT.getTransform() for i in range(3): newT.setTranslationAnimCurve(i,oldT.getTranslationAnimCurve(i)) newT.setScaleAnimCurve(i,oldT.getScaleAnimCurve(i)) newT.setSkewXAnimCurve(i,oldT.getSkewXAnimCurve(i)) newT.setPivotPointAnimCurve(i,oldT.getPivotPointAnimCurve(i)) newT.setRotationAnimCurve(i,oldT.getRotationAnimCurve(i)) for i in range(4): for j in range(4): newT.setExtraMatrixAnimCurve(i,j,oldT.getExtraMatrixAnimCurve(i,j)) def copyAttributes(oldshape, newshape): old = oldshape.getAttributes() new= newshape.getAttributes() for i in range(len(old)): new.setCurve(i, old.getCurve(i)) def createLayer(shape, rotoRoot, newRotoNode, shapeInfos, task): newRotoNodeCurve = newRotoNode['curves'] newRotoRoot = newRotoNodeCurve.rootLayer silhouetteData = matchShapeInfo(shape[0],shapeInfos) newLayer = rp.Layer(newRotoNodeCurve) newLayer.name = silhouetteData['LABEL'] #task related code task.setMessage( 'Creating Layer: ' + silhouetteData['LABEL'] ) if task.isCancelled(): taskCancel = True #end of task related code if shape[1] == rotoRoot: newRotoRoot.append(newLayer) else: parentLayer = matchShapeInfo(shape[1],shapeInfos) newlayerList = [] newlayerList = rptsw_walker(newRotoRoot, newlayerList) for layer in newlayerList: if isinstance(layer[0], nuke.rotopaint.Layer): if layer[0].name == parentLayer['LABEL']: assignParent = layer[0] assignParent.append(newLayer) copyTransforms(shape[0],newLayer) def createStroke(shape, rotoRoot, newRotoNode, shapeInfos, precision, task): cancel = False silhouetteData = 1 #dummy stroke width/size newRotoNodeCurve = newRotoNode['curves'] newRotoRoot = newRotoNodeCurve.rootLayer newlayerList = [] newlayerList = rptsw_walker(newRotoRoot, newlayerList) newstroke = rp.Stroke(newRotoNodeCurve) keysTimes = shape[0][0].center.getControlPointKeyTimes() taskcount =0 for i in range((len(shape[0]) *precision)+1): #task related code task.setMessage( 'Creating Stroke: ' + '\\npoint ' + str(taskcount+1) + \" of \" + str(len(shape[0]) *precision) ) taskcount +=1 if task.isCancelled(): taskCancel = True break #end of task related code newPoint = rp.AnimControlPoint(0,0,1) for key in keysTimes: if i == 0: # or n == (len(shape[0]) *precision): #extra points on begin/end pos = 0 point = [shape[0][0].center.getPositionAnimCurve(0).evaluate(key),shape[0][0].center.getPositionAnimCurve(1).evaluate(key),0] elif i == (len(shape[0]) *precision): pos = 1 point = [shape[0][-1].center.getPositionAnimCurve(0).evaluate(key),shape[0][-1].center.getPositionAnimCurve(1).evaluate(key),0] else: pos = float((i)* 1.0/(len(shape[0])*precision)) cubicCurve = shape[0].evaluate(0, key) point = cubicCurve.getPoint(pos) vector = nuke.math.Vector3(point[0],point[1], 1) newPoint.addPositionKey(key,vector) newstroke.append(newPoint) copyTransforms(shape[0],newstroke) # newstroke.name = silhouetteData['LABEL'] newstroke.setVisible(0, shape[0].getVisible(0)) if shape[1] == rotoRoot: newRotoRoot.append(newstroke) else: parentLayer = matchShapeInfo(shape[1],shapeInfos) for layer in newlayerList: if isinstance(layer[0], nuke.rotopaint.Layer): if layer[0].name == parentLayer['LABEL']: assignParent = layer[0] assignParent.append(newstroke) attrs = newstroke.getAttributes() oldattrs = shape[0].getAttributes() attrs.setCurve(9,oldattrs.getCurve('opc')) #opacity if float(silhouetteData) = 3: attrs.set(\"bs\",float(silhouetteData)) else: if float(silhouetteData) 0: threading.Thread(None, createStroke, args=(shape, rotoRoot, newRotoNode, shapeInfos, precision, task)).start() else: threading.Thread(None, createShape, args=(shape, rotoRoot, newRotoNode, shapeInfos, task)).start() newRotoNodeCurve.changed() rptsw_shapeList = [] print \"Time elapsed:\",time.time() - start_time, \"seconds\" #remove the line below if you are using the script with menu.py (in menus/DAG/etc) silhouetteStrokeImporter()  ","permalink":"https://serge-m.github.io/posts/import-sihlouette-roto-masks-into/","summary":"Today kids, I tell you how to split imported Sihlouette shapes into Nuke RotoPaint nodes.\nIn SihlouetteFX select several nodes. Go File-\u0026gt;Export-\u0026gt;nuke 6.2+ Shapes and save Nuke project\nI tried to use also File-\u0026gt;Export-\u0026gt;Nuke Shapes but in Nuke 7.0 it wasn\u0026rsquo;t opened properly\nOpen saved project in Nuke. There is a single RotoPaint node. It contains several layers with several shapes in each. I need separate layers into many nodes to use them separately.","title":"Import Sihlouette roto masks into several Nuke shapes"},{"content":"I need to make binary image from grayscale one. I know I can use Levels filter for that with such settings: I think that everything that is less or equal to 128 is transformet to 0 and the rest is transformet to 255. BUT. On the result i have such a places with outlier value: WTF? What is the logic of AAE developers? So the solution is using real numbers for input black and white:Results are sharp: ![](http://3.bp.blogspot.com/-toBk_jZhH4A/UpiHItHbIiI/AAAAAAAAAao/Mi-_fbtWRzY/s1600/levels_result_2.png)","permalink":"https://serge-m.github.io/posts/binarization-in-adobe-after-effects/","summary":"I need to make binary image from grayscale one. I know I can use Levels filter for that with such settings: I think that everything that is less or equal to 128 is transformet to 0 and the rest is transformet to 255. BUT. On the result i have such a places with outlier value: WTF? What is the logic of AAE developers? So the solution is using real numbers for input black and white:Results are sharp: !","title":"Binarization in Adobe After Effects"},{"content":"It seems I almost found alternative for [TotalCommander](http://www.ghisler.com/\u0026quot; target=\u0026quot;_blank). Let me present [DoubleCommander](http://doublecmd.sourceforge.net/\u0026quot; target=\u0026quot;_blank). It is free, open-source, extendible by Total\u0026rsquo;s plugins. That\u0026rsquo;s awesome. I also found out that viewer plugin SGView for TotalCommander can be completely replaced by ImaginePlugin. Imagine has x64 version. The issue was to assign hotkeys like in SGView.\n","permalink":"https://serge-m.github.io/posts/fre-open-source-alternative-to-total/","summary":"It seems I almost found alternative for [TotalCommander](http://www.ghisler.com/\u0026quot; target=\u0026quot;_blank). Let me present [DoubleCommander](http://doublecmd.sourceforge.net/\u0026quot; target=\u0026quot;_blank). It is free, open-source, extendible by Total\u0026rsquo;s plugins. That\u0026rsquo;s awesome. I also found out that viewer plugin SGView for TotalCommander can be completely replaced by ImaginePlugin. Imagine has x64 version. The issue was to assign hotkeys like in SGView.","title":"Free open-source alternative to Total Commander"},{"content":"In progress   Test-Driven Development with Python Harry Percival link Pycon 2016 workshop\n  Python Testing Python Software Development and Software Testing (posts and podcast) book\n   Part  Effective C++: 55 Specific Ways to Improve Your Programs and Designs Scott Meyers   Done   Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets Nassim Nicholas Taleb\n  Learning Spark. Lightning-Fast Big Data Analysis. By Holden Karau, Andy Konwinski, Patrick Wendell, Matei Zaharia link\n  Cracking the Coding Interview link\n  Clean Code A Handbook of Agile Software Craftsmanship. Robert C. Martin\n  Thinking In Java. Bruce Eckel\n  Effective Java. Joshua Bloch link\n  The Black Swan: The Impact of the Highly Improbable Nassim Nicholas Taleb\n  Effective STL: 50 Specific Ways to Improve Your Use of the Standard Template Library link Scott Meyers\n  Design Patterns: Elements of Reusable Object-Oriented Software Erich Gamma et al.\n  C++ Coding Standards: 101 Rules, Guidelines, and Best Practices, Herb Sutter Andrei Alexandrescu link\n  C++: The Complete Reference, Herbert Schildt\n  Testing Dot Com, or allowance for the abuse of bugs in Internet startups Roman Savin Роман Савин - Тестирование DOT COM (in Russian)\n   Postponed  Learning Python, 5th Edition, Mark Lutz link  todo   https://medium.freecodecamp.com/what-to-learn-in-2017-if-youre-a-frontend-developer-b6cfef46effd#.d6zqx1uh2\n  How to Lead People and Be a Manager (collection of links and summaries)\n  http://www.deeplearningbook.org/ Ian Goodfellow and Yoshua Bengio and Aaron Courville\n  ","permalink":"https://serge-m.github.io/posts/books/","summary":"In progress   Test-Driven Development with Python Harry Percival link Pycon 2016 workshop\n  Python Testing Python Software Development and Software Testing (posts and podcast) book\n   Part  Effective C++: 55 Specific Ways to Improve Your Programs and Designs Scott Meyers   Done   Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets Nassim Nicholas Taleb\n  Learning Spark.","title":"Books"},{"content":"В математике у нас есть уравнение Лапласа \\delta u = 0 Перепишем: Вспомним, как можно расписывать вторую производную: Для изображения равенство нулю оператора лапласа означает, что в каждая точка должна быть равна среднему из своих соседей.  Уравнение маттинг лапласиана - это то же самое, но только делается не обычное усреднение, а усреднение с весами. Веса зависят от похожести пикселей.\n","permalink":"https://serge-m.github.io/posts/matting-laplacian/","summary":"В математике у нас есть уравнение Лапласа \\delta u = 0 Перепишем: Вспомним, как можно расписывать вторую производную: Для изображения равенство нулю оператора лапласа означает, что в каждая точка должна быть равна среднему из своих соседей.  Уравнение маттинг лапласиана - это то же самое, но только делается не обычное усреднение, а усреднение с весами. Веса зависят от похожести пикселей.","title":"По простому о matting laplacian"},{"content":"fmincor( ponter to target function, initial point, list of constrains) - some improved gradient decent. Step is adaptive.\nuseParallel - automatic parallel\n","permalink":"https://serge-m.github.io/posts/matlab/","summary":"fmincor( ponter to target function, initial point, list of constrains) - some improved gradient decent. Step is adaptive.\nuseParallel - automatic parallel","title":"MATLAB"},{"content":"Useful for using in regular expressions:\nimport re re.escape( str ) Python detects all escape characters that can be used by regex \u0026nbsp;and add slash before them [http://docs.python.org/2/library/re.html#re.escape](http://docs.python.org/2/library/re.html#re.escape) ","permalink":"https://serge-m.github.io/posts/add-backslash-before-all-escape/","summary":"Useful for using in regular expressions:\nimport re re.escape( str ) Python detects all escape characters that can be used by regex \u0026nbsp;and add slash before them [http://docs.python.org/2/library/re.html#re.escape](http://docs.python.org/2/library/re.html#re.escape) ","title":"Add backslash before all escape characters in python"},{"content":"So, Javascript code for position changing is simple. Lets say comp - is your composition:\nprecomp = app.project.items.addComp( \"ololo\", width, height, 1.0, duration, frameRate);move pointer to time 0.2: comp.time = 0.2; ","permalink":"https://serge-m.github.io/posts/move-time-indicator-in-composition/","summary":"So, Javascript code for position changing is simple. Lets say comp - is your composition:\nprecomp = app.project.items.addComp( \"ololo\", width, height, 1.0, duration, frameRate);move pointer to time 0.2: comp.time = 0.2; ","title":"Moving time indicator in composition timeline in After Effects CS6 using scripts"},{"content":"Взято с http://habrahabr.ru/post/201406/\nsee also Deep Learning в вычислении оптического потока\nОбъяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.   Обработка изображений*,\u0026nbsp;Алгоритмы*Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня). Тогда обратимся к готовым реализациям, например, в OpenCV. Там он реализован различными методами и совершенно непонятно, чем аббревиатура PyrLK лучше или хуже обозначения Farneback или чего-нибудь в этом роде, да и придется поразбираться со смыслом параметров, которых в некоторых реализациях очень много. Причем, что интересно, эти алгоритмы как-то работают, в отличие от того, что мы написали сами. В чем же секрет?  Что же такое оптический поток Оптический поток (ОП) – изображение видимого движения, представляющее собой сдвиг каждой точки между двумя изображениями. По сути, он представляет собой поле скоростей (т. к. сдвиг с точностью до масштаба эквивалентен мгновенной скорости). Суть ОП в том, что для каждой точки изображения\u0026nbsp; ![](http://latex.codecogs.com/gif.latex?I_{1}(x,y)) \u0026nbsp;находится такой сдвиг (dx, dy), чтобы исходной точке соответствовала точка на втором изображении\u0026nbsp; ![](http://latex.codecogs.com/gif.latex?I_{2}(x\u0026amp;plus;dx,y\u0026amp;plus;dy)) . Как определить соответствие точек – отдельный вопрос. Для этого надо взять какую-то функцию точки, которая не изменяется в результате смещения. Обычно считается, что у точки сохраняется интенсивность (т. е. яркость или цвет для цветных изображений), но можно считать одинаковыми точки, у которых сохраняется величина градиента, гессиан, его величина или его определитель, лапласиан, другие характеристики. Очевидно, сохранение интенсивности дает сбои, если меняется освещенность или угол падения света. Тем не менее, если речь идет о видеопотоке, то, скорее всего, между двумя кадрами освещение сильно не изменится, хотя бы потому, что между ними проходит малый промежуток времени. Поэтому часто используют интенсивность в качестве функции, сохраняющейся у точки. По такому описанию можно перепутать ОП с поиском и сопоставлением характерных точек. Но это разные вещи, суть оптического потока в том, что он не ищет какие-то особенные точки, а по параметрам изображений пытается определить, куда сместилась произвольная точка.\nЕсть два варианта расчета оптического потока: плотный (dense) и выборочный (sparse). Sparse поток рассчитывает сдвиг отдельных заданных точек (например, точек, выделенных некоторым feature detector\u0026rsquo;ом), dense поток считает сдвиг всех точек изображения. Естественно, выборочный поток вычисляется быстрее, однако для некоторых алгоритмов разница не такая уж и большая, а для некоторых задач требуется нахождение потока во всех точках изображения.\nДля вырожденных случаев можно применять более простые методы определения сдвига. В частности, если все точки изображения имеют один и тот же сдвиг (изображение сдвинуто целиком), то можно применить метод фазовой корреляции: вычислить преобразование Фурье для обоих изображений, найти свертку их фаз и по ней определить сдвиг (см.[en.wikipedia.org/wiki/Phase_correlation](http://en.wikipedia.org/wiki/Phase_correlation\u0026quot; style=\u0026ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)). Также можно применять поблочное сравнение (block matching): находить сдвиг, минимизирующий норму разности изображений в окне. В чистом виде такой алгоритм будет работать долго и неустойчиво к поворотам и прочим искажениям. [Английская википедия](http://en.wikipedia.org/wiki/Optical_flow\u0026quot; style=\u0026ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) причисляет перечисленные алгоритмы к различным вариантам вычисления оптического потока, но мне это кажется не слишком корректным, так как эти алгоритмы могут быть применены и в других целях и не полностью решают данную задачу. Мы будем называть оптическим потоком методы, основанные на локальных характеристиках изображений (то, что в английской википедии называется differential methods).\nСтандартный подход (метод Лукаса-Канаде) Математическое описание алгоритма достаточно подробно приведено в\u0026nbsp;[этой статье](http://habrahabr.ru/post/169055/\" style=\"border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), но в ней затрагиваются лишь теоретические аспекты. Рассмотрим математическую модель оптического потока, считая, что у точки в результате смещения не изменилась интенсивность.\nПусть – интенсивность в некоторой точке (x, y) на первом изображении (т. е. в момент времени t). На втором изображении эта точка сдвинулась на (dx, dy), при этом прошло время dt, тогда – это мы разложили по Тейлору функцию интенсивности до первого члена (позже будет упомянуто, почему только до первого), здесь – частные производные по координатам и времени, то есть по сути – изменение яркости в точке (x, y) между двумя кадрами.\nМы считаем, что у точки сохранилась интенсивность, значит Получаем одно уравнение с двумя неизвестными (dx и dy), значит его недостаточно для решения, то есть только на этом уравнении далеко не уедешь.\nСамое простое решение проблемы – алгоритм Лукаса-Канаде. У нас же на изображении объекты размером больше 1 пикселя, значит, скорее всего, в окрестности текущей точки у других точек будут примерно такие же сдвиги. Поэтому мы возьмем окно вокруг этой точки и минимизируем (по МНК) в нем суммарную погрешность с весовыми коэффициентами, распределенными по Гауссу, то есть так, чтобы наибольший вес имели пиксели, ближе всего находящиеся к исследуемому. После простейших преобразований, получаем уже систему из 2 уравнений с 2 неизвестными: Как известно, эта система имеет единственное решение не всегда (хотя и очень часто): если детерминант системы равен нулю, то решений либо нет, либо бесконечное число. Эта проблема известна как Aperture problem – неоднозначность сдвига при ограниченном поле зрения для периодических картинок. Она соответствует случаю, когда в поле зрения попадает фрагмент изображения, в котором присутствует некоторая цикличность; тут уж и человек не сможет однозначно определить, куда картинка сместилась. Проблема в том, что из-за шумов в таких неоднозначных ситуациях мы получим не нулевой детерминант, а очень маленький, который, скорее всего, приведет к очень большим значениям сдвига, особо не коррелирующим с действительностью. Так что на определенном этапе нужно просто проверять, не является ли детерминант системы достаточно маленьким, и, если что, не рассматривать такие точки или отмечать их как ошибочные.\nПочему не работает? Если мы остановимся на этом этапе и реализуем этот алгоритм, то он будет успешно работать. Но только если сдвиг между соседними изображениями будет очень маленький, порядка 1 пикселя, и то не всегда. (Для анализа качества генерировались синтетические последовательности с различным относительным сдвигом, причем этот сдвиг может выражаться нецелым числом пикселей, тогда результирующее изображение соответствующим образом интерполируется) Уже на сдвиге в 2 пикселя погрешность будет большая, а если 3 и более, то результат будет вообще неадекватным. В чем же дело? Тут нам устроила подставу математика. Она привила нам ощущение, что все функции вокруг непрерывные и много раз дифференцируемые. И вообще нас в институте приучили приближение функции в окрестности точки записывать с помощью формулы Тейлора, и мы везде бездумно радостно пользуемся этим. А теперь задумаемся, какой физический смысл производных в данном месте? Мы хотим с их помощью определить изменение значения функции в конечной окрестности точки, а производная дает представление о бесконечно малой окрестности. Для расширения этой окрестности можно было бы добавить более высокий порядок производных в разложение Тейлора, но это приведет к нелинейностям в системе, от чего ее станет существенно сложнее решать, а преимущества будут сомнительны, тем более что на практике мы имеем дело не с непрерывными многократно дифференцируемыми функциями, а с вообще непонятно какими дискретными функциями. Поэтому логичнее будет искать функцию g(x), для которой в нашем дискретном случае как можно точнее выполняется f(x) + g(x) = f(x+1), f(x) + 2g(x) = f(x+2), f(x) — g(x) = f(x-1), и т. д. Таким образом, нам в этом случае нужна не производная, а некоторая линейная функция, наиболее близко лежащая к точкам исходной функции. Простые математические выкладки приводят к решению , где . Если мы строили производную по одной соседней точке с каждой стороны, то нам повезло: в этом случае формула совпадает с формулой приближенного вычисления производных: g(x) = (f(x+1) – f(x-1)) / 2. Что характерно, в OpenCV при вычислении оптического потока Лукаса-Канаде используется именно такая формула, к этому мы еще вернемся потом. А вот если взять больше точек, то формула уже становится совсем не похожа на классические разностные схемы для первой производной.\nОчевидно, если мы строим эту функцию, например, по трем окрестным точкам слева и справа от исходной, то она никаким образом не зависит от точек, расположенных дальше, и, соответственно, при сдвиге более трех точек все равно у нас часто будут получаться неадекватные результаты. А еще, чем больше число точек, по которым мы строим эту функцию, тем больше среднее отклонение получаемой линии от используемых точек – опять же из-за того, что у нас не линейно меняющиеся изображения, а черт знает какие. На практике сдвиги более 2 пикселей уже дают неадекватно большую ошибку, сколько бы точек мы ни взяли.\nДругим слабым местом алгоритма является то, что мы опять же имеем дело не с гладкими непрерывными функциями, а с произвольными, да еще и дискретными. Поэтому на некоторых фрагментах изображения интенсивность может «скакать» вообще без явных закономерностей, например на границах объектов, или из-за шумов. В этом случае никакая функция g(x) не сможет достаточно точно описать изменения изображения в окрестности точки. Чтобы с этим побороться (хотя бы частично), предлагается исходное изображение размазать, причем полезно будет его размазать достаточно сильно, то есть лучше применять даже не всеми любимый gaussian blur (усреднение с весовыми коэффициентами), а прямо таки box filter (равномерное усреднение по окну), да еще и несколько раз подряд. Гладкость изображения для нас сейчас важнее, чем детализация.\nТем не менее, эти меры так же не спасут нас от ограничения детектируемого сдвига в 2-3 пикселя. И кстати, в OpenCV 1.0 присутствовала такая реализация оптического потока, и работала она только в идеальных условиях на очень маленьких сдвигах.\nЧто же делать? Итого, обычный Лукас-Канаде хорошо определяет маленькие сдвиги, такие, в рамках которых картинка похожа на свое линейное приближение. Чтобы с этим побороться, воспользуемся стандартным приемом CV – multi-scaling'ом: построим «пирамиду» изображений разного масштаба (почти всегда берется масштабирование в 2 раза по каждой оси, так проще считать) и пройдем по ним оптическим потоком от меньшего изображения к большему, тогда детектированный маленький сдвиг на маленьком изображении будет соответствовать большому сдвигу на большом изображении. На самом маленьком изображении мы обнаруживаем сдвиг не более 1-2 пикселей, а переходя от меньшего масштаба к большему, мы пользуемся результатом с предыдущего шага и уточняем значения сдвига. Собственно, в OpenCV его и реализует функция calcOptFlowPyrLK. Использование этого пирамидального алгоритма позволяет нам не заморачиваться вычислением линейной аппроксимации по многим точкам: проще взять больше уровней пирамиды, а на каждом уровне брать довольно грубое приближение этой функции. Поэтому в OpenCV и идет расчет всего по двум соседним точкам. И поэтому применительно к этой реализации алгоритма наши умозаключения про преимущество аппроксимирующей функции перед производной оказались бесполезными: для такого количества опорных точек производная и есть лучшая аппроксимирующая функция. А какие еще бывают? Этот алгоритм не является единственным вариантом вычисления оптического потока. В OpenCV кроме потока Лукаса-Канаде есть еще поток Farneback и SimpleFlow, также часто ссылаются на алгоритм Horn–Schunck. Метод Horn–Schunck носит несколько более глобальный характер, чем метод Лукаса-Канаде. Он опирается на предположение о том, что на всем изображении оптический поток будет достаточно гладким. От того же самого уравнения предлагается перейти к функционалу , то есть добавить требование на отсутствие резкого изменения сдвигов с весовым коэффициентом α. Минимизация этого функционала приводит нас к системе из двух уравнений: В этих уравнениях лапласиан предлагают посчитать приближенно: – разница со средним значением. Получаем систему уравнений, которую записываем для каждого пикселя и решаем общую систему итеративно: В данном алгоритме тоже предлагают использовать multi-scaling, причем рекомендуют масштабировать изображения не в 2 раза, а с коэффициентом 0.65\nЭтот алгоритм был реализован в первых версиях OpenCV, но в последствии от него отказались.\nFarneback предложил аппроксимировать изменение интенсивности в окрестности с помощью квадратичной формы: I = xAx + bx + c с симметричной матрицей A (по сути, рассматривая разложение по Тейлору до первого члена, мы брали линейную аппроксимацию I = bx + c, то есть сейчас мы как раз решили повысить точность приближения) Если изображение сдвинулось в пределах этой окрестности, то , подставляем в квадратичное разложение, раскрываем скобки, получаем .\nТеперь мы можем вычислить значения A, b, c на обеих картинках, и тогда эта система станет избыточной относительно d (особенно смущает первое уравнение), и вообще d можно получить из второго уравнения: . Приходится прибегать к следующей аппроксимации: . Обозначим еще для простоты , Тогда получим просто .\nДля компенсации шумов при вычислении, снова обратимся к тому предположению, что в окрестности исследуемой точки у всех точек более или менее одинаковый сдвиг. Поэтому опять же проинтегрируем погрешностьпо окну с гауссовскими весовыми коэффициентами w, и найдем вектор d, минимизирующий эту суммарную погрешность. Тогда мы получим оптимальное значение и соответствующую минимальную ошибку . То есть нам надо для каждой точки посчитать , усреднить по окну, инвертировать матрицу и получить результат. Соответственно эти произведения можно посчитать для всей картинки и использовать заранее рассчитанные значения для разных точек, то есть это как раз тот случай, когда имеет смысл считать dense поток.\nКак обычно, у этого алгоритма есть некоторое количество модификаций и усовершенствований, в первую очередь позволяющих использовать известную априорную информацию – заданную начальную аппроксимацию потока – и, опять же, multi-scaling.\nВ основе метода SimpleFlow лежит следующая идея: если мы все равно не умеем определять сдвиг больше чем размер окна, по которому мы искали производные, то зачем вообще заморачиваться с вычислением производных? Давайте просто в окне найдем наиболее похожую точку! А для разрешения неоднозначностей и для компенсации шумов учтем, что поток непрерывный и в окрестности данной точки все точки имеют почти одинаковый сдвиг. А проблему с размером окна опять же решим за счет multi-scaling\u0026rsquo;а.\nБолее строго, алгоритм звучит так: для всех точек в окне находится функция «энергии», отвечающая (с обратной логарифмической зависимостью) за вероятность перехода исходной точки в эту точку: . Далее, считается свертка этой энергии с гауссовым окном и находятся значения (dx,dy), минимизирующие эту функцию. Чтобы получить субпиксельную точность, рассматривается малая окрестность найденной оптимальной точки (dx,dy) и в ней ищется пик функции энергии как пик параболоида. И, как было упомянуто выше, эта процедура выполняется для пирамиды масштабированных изображений. Еще у них в алгоритме предложены хитрые методы ускорения расчетов, но это уже кому интересно разберутся сами. Для нас же важно, что за счет этого данный алгоритм является (теоретически) достаточно быстрым при неплохой точности. И у него нет такой проблемы, как у предыдущих, что чем больше сдвиг, тем хуже он детектируется.\nА если брать не интенсивность? Выше было сказано, что соответствие между точками может определяться разными величинами, так почему же мы рассматриваем только интенсивность? А потому, что любую другую величину можно свести к ней: мы просто фильтруем изображения соответствующим фильтром и на вход описанных выше алгоритмов подаем отфильтрованные изображения. Соответственно, если вы хотите использовать оптический поток, то сначала подумайте, в ваших условиях какая характеристика изображения будет наиболее стабильной, и проведите соответствующую фильтрацию, чтобы на входе алгоритма оказалась не интенсивность, а эта характеристика. Практика Давайте опробуем на практике алгоритмы, которые нам предлагает OpenCV. Здесь можно проводить множество различных исследований каждого алгоритма, варьируя параметры, изменяя входные последовательности – с разными сдвигами, поворотами, проективными преобразованиями, сегментами, с разными шумами и т. д. Это все заняло бы уйму времени и по размеру отчета превзошло бы настоящую статью, поэтому здесь предлагаю ограничиться простым случаем параллельного сдвига изображения на фиксированное расстояние и наложение небольших шумов. Это позволит понять в общих чертах, как запускать алгоритмы и кто из них круче.\nПодробно синтаксис процедур описан на странице с [мануалом](http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html\u0026quot; style=\u0026ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), здесь я приведу выжимку-перевод с моими комментариями.\nКлассический Лукас-Канаде реализован с пирамидой в процедуре calcOpticalFlowPyrLK. Алгоритм рассчитывает sparse-поток, то есть для заданного набора точек на первом изображении оценивает их положение на втором. Входные параметры достаточно очевидны: два входных изображения, входной и выходной наборы точек, status – выходной вектор, показывающий, найдена ли успешно соответствующая точка, err – выходной вектор оцененных погрешностей соответствующих точек, WinSize – размер окна, по которому происходит гауссово усреднение, я брал 21х21 и работало хорошо, maxLevel – количество слоев в пирамиде минус один, т. е. номер последнего слоя, я брал 5, criteria – условие выхода из итеративного процесса определения сдвига (минимизация погрешности производится итеративно) – этот параметр я оставлял по умолчанию, flags – дополнительные флаги, например можно использовать начальное приближение потока или выбрать метод оценки погрешности, minEigThreshold – пороговое значение градиента, ниже которого матрица считается вырожденной, я оставлял по умолчанию. Начиная с OpenCV 2.4.1, при вычислении потока можно использовать заранее вычисленную пирамиду отмасштабированных изображений.\nРезультат работы – успешно и стабильно обнаруживаются как малые, так и большие сдвиги, устойчив к довольно большим шумам, время работы – порядка 10 мс для 400 точек c 5-слойной пирамидой (на core i7 950).\nКстати, этот алгоритм реализован так же на Gpu (CUDA), причем как dense, так и sparse версии.\nПоток Farneback реализуется процедурой calcOpticalFlowFarneback, рассчитывается dense-поток, то есть сдвиг каждой точки. Параметры: входные изображения, выходной поток в формате двухканальной матрицы float\u0026rsquo;ов, pyr_scale определяет отношение масштабов между слоями пирамиды, levels – количество уровней в пирамиде, winsize – размер окна, по которому производится усреднение, iterations – количество итераций на каждом уровне, poly_n – размер полинома, по которому оцениваются значения A и b, poly_sigma – сигма гауссовского размытия при сглаживании производных, рекомендованные значения параметров указаны в мануале, flags – дополнительные флаги, например можно использовать начальное приближение потока или по-другому усреднять по окну.\nЭтот алгоритм куда менее стабилен (по моим наблюдениям), легче промахивается на довольно равномерных картинках (видимо, проблема в отсутствии фильтрации неудачных точек), плохо определяет большие сдвиги. У меня отрабатывал за 600 мс на изображении 512х512.\nПоток SimpleFlow реализует процедура calcOpticalFlowSF (рассчитывается опять же dense поток), и у нее есть множество загадочных параметров без дефолтных значений, и вообще на данный момент на странице информация предоставлена весьма лаконично. Попробуем разобраться. Первые 3 – входные изображения и выходное двухканальное; layers – количество слоев в пирамиде, то есть сколько раз масштабируем исходное изображение; averaging_block_size – размер окна, в котором мы считали функцию энергии пикселей; max_flow – максимальный сдвиг, который мы хотим уметь определять на каждом шаге, по сути он определяется размером окна (хотя не совсем понятно, почему он int). На этом можно остановиться, а можно задать еще несколько параметров, смысл некоторых из них от меня ускользает.\nНа сайте предлагают посмотреть [пример](https://github.com/Itseez/opencv/blob/master/samples/cpp/simpleflow_demo.cpp\u0026quot; style=\u0026ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) его использования, в котором он запускается с такими параметрами: calcOpticalFlowSF(frame1, frame2, flow, 3, 2, 4, 4.1, 25.5, 18, 55.0, 25.5, 0.35, 18, 55.0, 25.5, 10);\nУ меня алгоритм работает значительно медленнее других, порядка 9-12 секунд на картинку 512х512. Результат работы кажется более правдоподобным, чем Farneback, по крайней мере лучше определяется сдвиг на равномерных картинках, заметно лучше срабатывает с большими сдвигами.\nВыводы Если вы хотите использовать где-то оптический поток, сначала подумайте, нужен ли он вам: часто можно обойтись более простыми методами. Браться реализовывать поток самостоятельно стоит только несколько раз подумав: каждый алгоритм имеет множество хитростей, тонкостей и оптимизаций; что бы вы ни сделали, скорее всего, в OpenCV оно же работает лучше (естественно, при условии, что оно там есть). Тем более что они там вовсю используют логические и хардварные оптимизации типа использования SSE инструкций, многопоточность, возможности вычисления с CUDA или OpenCL и т. д. Если вам достаточно посчитать сдвиг некоторого набора точек (т. е. sparse поток), то можете смело использовать функцию calcOpticalFlowPyrLK, оно работает хорошо, надежно и достаточно быстро. Для вычисления dense-потока хорошо использовать функцию calcOpticalFlowSF, но она работает очень медленно. Если быстродействие критично, то calcOpticalFlowFarneback, но надо еще удостовериться, что результаты его работы вас устроят. Литература [docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html\" style=\"border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) Pyramidal Implementation of the Lucas Kanade Feature Tracker. Description of the algorithm — Jean-Yves Bouguet Two-Frame Motion Estimation Based on Polynomial Expansion — Gunnar Farneback SimpleFlow: A Non-iterative, Sublinear Optical Flow Algorithm — Michael Tao, Jiamin Bai, Pushmeet Kohli, and Sylvain Paris Horn-Schunck Optical Flow with a Multi-Scale Strategy — Enric Meinhardt-Llopis, Javier Sanchez [en.wikipedia.org/wiki/Optical_flow](http://en.wikipedia.org/wiki/Optical_flow\" style=\"border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) [оптический поток](http://habrahabr.ru/search/?q=%5B%D0%BE%D0%BF%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9%20%D0%BF%D0%BE%D1%82%D0%BE%D0%BA%5D\u0026amp;target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;),\u0026nbsp;[optical flow](http://habrahabr.ru/search/?q=%5Boptical%20flow%5D\u0026amp;target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;),\u0026nbsp;[computer vision](http://habrahabr.ru/search/?q=%5Bcomputer%20vision%5D\u0026amp;target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;),\u0026nbsp;[openCV](http://habrahabr.ru/search/?q=%5BopenCV%5D\u0026amp;target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) ","permalink":"https://serge-m.github.io/posts/blog-post-3/","summary":"Взято с http://habrahabr.ru/post/201406/\nsee also Deep Learning в вычислении оптического потока\nОбъяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.   Обработка изображений*,\u0026nbsp;Алгоритмы*Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня).","title":"То, что вы хотели знать про оптический поток, но стеснялись спросить"},{"content":" Делаем детектор движения, или OpenCV — это просто http://habrahabr.ru/company/avi/blog/200804/ 2. OpenCV шаг за шагом http://robocraft.ru/blog/computervision/265.html Пару слов о распознавании образов http://habrahabr.ru/post/208090/  Building opencv program in Cgcc -ggdb `pkg-config --cflags opencv` -o `basename test.c .c` test.c `pkg-config --libs opencv`**Description:**`pkg-config --cflags opencv`\u0026nbsp;— подставляет путь для инклудов через pkgconfig. `pkg-config --libs opencv`\u0026nbsp;— подставляет название либ для линковки через pkgconfig. При установке opencv поместила файлик .pc (в моём случае это /usr/lib/pkgconfig/opencv.pc), в котором рассказывается, где находятся заголовочные файлы этой библиотеки, а где сами либы для линковки. Таким образом первое у меня разворачивается в \"-I/usr/include/opencv\", а второе — в \"-lopencv_calib3d -lopencv_contrib -lopencv_core -lopencv_features2d -lopencv_flann -lopencv_gpu -lopencv_highgui -lopencv_imgproc -lopencv_legacy -lopencv_ml -lopencv_nonfree -lopencv_objdetect -lopencv_ocl -lopencv_photo -lopencv_stitching -lopencv_superres -lopencv_ts -lopencv_video -lopencv_videostab\", т.е. уже в прямые указания компилятору и линкеру, где искать инклуд-файлы (-Include) и библиотеки (-library), позволяющие разработчику не вбивать всё это руками. -o `basename test.c .c`\u0026nbsp;— отрезает от test.c часть с расширением (\".c\"), оставляя только часть имени файла «test», которое будет являться именем выходного (output) собранного исполняемого файла. Т.е. разворачивается это в \"-o test\". -ggdb\u0026nbsp;— смотрим в ман (а стоило бы сделать это ещё в начале ;)) -ggdb Produce debugging information for use by GDB. This means to use the most expressive format available (DWARF 2, stabs, or the native format if neither of those are supported), including GDB extensions if at all possible.т.е. генерация максимально полной отладочной информации для использовании в отладчике gdb (и включение её в выходной бинарник, например замечены секции .debug_aranges, .debug_info, .debug_abbrev, .debug_line, .debug_str). Blog about computer vision and opencv [http://www.uralvision.blogspot.ru/](http://www.uralvision.blogspot.ru/)  ","permalink":"https://serge-m.github.io/posts/opencv-tutorials-in-russian/","summary":"Делаем детектор движения, или OpenCV — это просто http://habrahabr.ru/company/avi/blog/200804/ 2. OpenCV шаг за шагом http://robocraft.ru/blog/computervision/265.html Пару слов о распознавании образов http://habrahabr.ru/post/208090/  Building opencv program in Cgcc -ggdb `pkg-config --cflags opencv` -o `basename test.c .c` test.c `pkg-config --libs opencv`**Description:**`pkg-config --cflags opencv`\u0026nbsp;— подставляет путь для инклудов через pkgconfig. `pkg-config --libs opencv`\u0026nbsp;— подставляет название либ для линковки через pkgconfig. При установке opencv поместила файлик .pc (в моём случае это /usr/lib/pkgconfig/opencv.pc), в котором рассказывается, где находятся заголовочные файлы этой библиотеки, а где сами либы для линковки.","title":"OpenCV tutorials (Russian)"},{"content":"I found free UML editor:\nArgoUML\u0026nbsp;[http://argouml.tigris.org/](http://argouml.tigris.org/\" style=\"background-color: transparent;)It seems not bad I have tried gliffy.com. It is online and paid. www.lucidchart.com has free version but it seems not so convenient\n","permalink":"https://serge-m.github.io/posts/free-uml-editor/","summary":"I found free UML editor:\nArgoUML\u0026nbsp;[http://argouml.tigris.org/](http://argouml.tigris.org/\" style=\"background-color: transparent;)It seems not bad I have tried gliffy.com. It is online and paid. www.lucidchart.com has free version but it seems not so convenient","title":"Free UML editor"},{"content":"// Make a composition var comp = app.project.items.addComp(\u0026lsquo;MyComp\u0026rsquo;, 1920, 1080, 1.0, 10, 25.0 );\n// Open it in viewer comp.openInViewer();\nThis solution works only in AAE CS 6.0\nMy version of some cross-platform code\nfunction OpenInViewer( comp ) { var version = app.version.match(/(\\d+.\\d+)./)[1]; if( version \u0026gt;= 11.0 ) comp.openInViewer() ; else { var duration = comp.workAreaDuration; comp.workAreaDuration = 2comp.frameDuration; comp.ramPreviewTest(\u0026quot;\u0026quot;,1,\u0026quot;\u0026quot;); comp.workAreaDuration = duration; }\n}\n inspired by\u0026nbsp;[http://www.videocopilot.net/forum/viewtopic.php?f=5\u0026amp;t=116057#p348646](http://www.videocopilot.net/forum/viewtopic.php?f=5\u0026amp;t=116057#p348646) ","permalink":"https://serge-m.github.io/posts/how-to-open-composition-in-viewer-in/","summary":"// Make a composition var comp = app.project.items.addComp(\u0026lsquo;MyComp\u0026rsquo;, 1920, 1080, 1.0, 10, 25.0 );\n// Open it in viewer comp.openInViewer();\nThis solution works only in AAE CS 6.0\nMy version of some cross-platform code\nfunction OpenInViewer( comp ) { var version = app.version.match(/(\\d+.\\d+)./)[1]; if( version \u0026gt;= 11.0 ) comp.openInViewer() ; else { var duration = comp.workAreaDuration; comp.workAreaDuration = 2comp.frameDuration; comp.ramPreviewTest(\u0026quot;\u0026quot;,1,\u0026quot;\u0026quot;); comp.workAreaDuration = duration; }\n}\n inspired by\u0026nbsp;[http://www.videocopilot.net/forum/viewtopic.php?f=5\u0026amp;t=116057#p348646](http://www.videocopilot.net/forum/viewtopic.php?f=5\u0026amp;t=116057#p348646) ","title":"How to open composition in viewer in Adobe After Effects using scripts"},{"content":"Using python and ffmpeg:\n1 2 3 4 5 6 7 8 9 10 11 12 13  #!/usr/bin/python import glob import os t=glob.glob(\u0026#34;*.avi\u0026#34; ) # search all AVI files for v in t: vv = os.path.splitext(v)[0]; os.makedirs( vv ) # make a directory for each input file pathDst = os.path.join( vv, \u0026#34;%05d.png\u0026#34; ) # deststination path os.system(\u0026#34;ffmpeg -i {0} {1}\u0026#34;.format( v, pathDst ) )   ","permalink":"https://serge-m.github.io/posts/convert-all-avi-files-in-current/","summary":"Using python and ffmpeg:\n1 2 3 4 5 6 7 8 9 10 11 12 13  #!/usr/bin/python import glob import os t=glob.glob(\u0026#34;*.avi\u0026#34; ) # search all AVI files for v in t: vv = os.path.splitext(v)[0]; os.makedirs( vv ) # make a directory for each input file pathDst = os.path.join( vv, \u0026#34;%05d.png\u0026#34; ) # deststination path os.system(\u0026#34;ffmpeg -i {0} {1}\u0026#34;.format( v, pathDst ) )   ","title":"Convert all *.avi files in current directory to png sequences"},{"content":"import readline readline.write_history_file( \u0026ldquo;log.py\u0026rdquo;)\n","permalink":"https://serge-m.github.io/posts/print-all-commands-that-were-enteren-in/","summary":"import readline readline.write_history_file( \u0026ldquo;log.py\u0026rdquo;)","title":"Print all commands that were entered in interactive mode in Python"},{"content":"источник\nДесятое место Ввод последнего аргумента недавних команд. Удерживая ALT или ESC, с каждым нажатием на точку в строку ввода будут подставляться параметры предыдущих команд, начиная от недавно введенных к старым.\nКомбинация 'ALT+.' или '\u0026lt;ESC\u0026gt; .'\nДевятое место Переинициализация терминала без завершения текущей сессии. Например, в случае когда в терминал были выведены двоичные данные и он перестал корректно работать.\nreset Восьмое место Создает пустой файл. Уничтожает содержимое файла без его удаления.\n\u0026gt; file.txt Седьмое место Запуск команды с пробелом перед ней не сохраняет ее в истории. Может пригодиться при передаче паролей программам в открытом виде.\n\u0026lt;пробел\u0026gt;команда Шестое место Запуск редактора для ввода сложной команды или скрипта, выбор редактора определяется переменной $EDITOR\nКомбинация 'CTRL+X E'\nПятое место Утилита My Traceroute эффективнее, чем комбинация traceroute и ping. Утилита mtr сочетает в себе функциональность traceroute и ping, позволяет проводить диагностику сети в более наглядном виде.\nmtr google.com Четвертое место Запуск предыдущей команды с заменой в ней подстроки, например, с foo на bar. Полезна при опечатках. Если ввести просто ^foo, то в предыдущей команде первое вхождение foo будет удалено.\n^foo^bar Третье место Возврат в предыдущую рабочую директорию. Может пригодиться и для переключения туда-сюда между двумя директориями.\ncd - Второе место Запуск HTTP-сервера в текущей директории на 8000 порту. Если в директории нет файла index.html, то будет показан её листинг.\npython -m SimpleHTTPServer Первое место Запуск последней команды под root. Полезна когда забыли использовать sudo для команды. \u0026ldquo;!!\u0026rdquo; — подставляет последнюю введенную команду.\nsudo !! ","permalink":"https://serge-m.github.io/posts/ten-best-console-commands-rus/","summary":"источник\nДесятое место Ввод последнего аргумента недавних команд. Удерживая ALT или ESC, с каждым нажатием на точку в строку ввода будут подставляться параметры предыдущих команд, начиная от недавно введенных к старым.\nКомбинация 'ALT+.' или '\u0026lt;ESC\u0026gt; .'\nДевятое место Переинициализация терминала без завершения текущей сессии. Например, в случае когда в терминал были выведены двоичные данные и он перестал корректно работать.\nreset Восьмое место Создает пустой файл. Уничтожает содержимое файла без его удаления.","title":"Десятка лучших консольных команд"},{"content":"We have such an example from openCV documentation:\nMat img(height, width, CV_8UC3, pixels, step); GaussianBlur(img, img, Size(7,7), 1.5, 1.5); What does it mean?\u0026nbsp;Lets say I have an image img: typedef unsigned char BYTE; BYTE * img = new BYTE[ w * h ];Lets assume the image is scanned line by line. So img[ 10 * w + 3 ] is a 3-rd pixel in 10-line of image.Then I import it in openCV is such a way: Mat img(h, w, CV_8UC1, dmDst, w); last argument is a pitch or stride. ","permalink":"https://serge-m.github.io/posts/opencv-processing-external-byte-buffer/","summary":"We have such an example from openCV documentation:\nMat img(height, width, CV_8UC3, pixels, step); GaussianBlur(img, img, Size(7,7), 1.5, 1.5); What does it mean?\u0026nbsp;Lets say I have an image img: typedef unsigned char BYTE; BYTE * img = new BYTE[ w * h ];Lets assume the image is scanned line by line. So img[ 10 * w + 3 ] is a 3-rd pixel in 10-line of image.Then I import it in openCV is such a way: Mat img(h, w, CV_8UC1, dmDst, w); last argument is a pitch or stride.","title":"OpenCV. Processing external byte buffer"},{"content":"UPD 2017: these are early experimnts on c++ compiler optimizations. I don\u0026rsquo;t think it is useful any more.\nI have the following code for copying several buffers from one object to another:\n1 2 3 4 5 6 7 8 9  // Copy several buffers (images)  for( int i = 0; i \u0026lt; MIN( conf_.size(), src.conf_.size() ); ++ i ) { // Copy pixels  for( int j = 0; j \u0026lt; MIN( sizeOfBuffer_, src.sizeOfBuffer_ ); ++ j ) { conf_[i][j] = src.conf_[i][j]; } }   Array conf_ is defined as follows:\n1  std::vector\u0026lt;BYTE*\u0026gt; conf_;   BYTE is unsigned char. That code is written with no doubt about software performance. It just works. When I do so I hope compiler make it better for me. This fragment takes about 45 ms for copying of 2 images with ( 1980x1080 pixels ) x 3 planes = 6.2 MPixels. I use Microsoft Visual Studio 2008 compiler on Intel Core i7 950 @ 3.07 GHz. The code is built for x64 platform. Disabled compiler option for buffer security check (GS-) and debug information has no effect on the productivity.\nSlightly better solution:\n1 2 3 4 5 6 7 8 9 10 11  // Copy several buffers (images)  for( int i = 0; i \u0026lt; MIN( conf_.size(), src.conf_.size() ); ++ i ) { int sizeOfArray = MIN( sizeOfBuffer_, src.sizeOfBuffer_ ); // Copy pixels  for( int j = 0; j \u0026lt; sizeOfArray; ++ j ) { conf_[i][j] = src.conf_[i][j]; } }   It takes about **35 ms **per full copy.\nMuch better solution:\n1 2 3 4 5 6 7 8 9 10 11 12 13  // Copy several buffers (images)  for( int i = 0; i \u0026lt; MIN( conf_.size(), src.conf_.size() ); ++ i ) { BYTE * arrDst = conf_[i]; BYTE * arrSrc = src.conf_[i]; int sizeOfArray = MIN( sizeOfBuffer_, src.sizeOfBuffer_ ); // Copy pixels  for( int j = 0; j \u0026lt; sizeOfArray; ++ j ) { arrDst[j] = arrSrc[j]; } }   Everything is the same except for using additional temporary variable. This version runs for 8 ms. UPD: Intel Compiler 2011 demonstrates 2 ms latency for this code.\nAnd even better solution for Visual Studio:\n1 2 3 4 5 6 7 8 9 10  // Copy several buffers (images)  for( int i = 0; i \u0026lt; MIN( conf_.size(), src.conf_.size() ); ++ i ) { BYTE * arrDst = conf_[i]; BYTE * arrSrc = src.conf_[i]; int sizeOfArray = MIN( sizeOfBuffer_, src.sizeOfBuffer_ ); // Copy pixels  std::copy( arrSrc, arrSrc + sizeOfArray, arrDst ); }   It uses **copy **function from standard C++ library. Time of this version is 2 ms.\nUPD: Intel Compiler 2011 demonstrates 2 ms latency for this code too, but it seems that for multiple runs the last version is slightly slower than previous one for ICC. My previous time measuremets have integer precision. So for 50 frames ICC+last-version got 141 ms and ICC+previous-version got 112 ms. That is strange.\nOn Intel Compiler memcpy and std::copy give nearly the same results.\n","permalink":"https://serge-m.github.io/posts/fast-way-of-copying-byte-array-in-cc/","summary":"UPD 2017: these are early experimnts on c++ compiler optimizations. I don\u0026rsquo;t think it is useful any more.\nI have the following code for copying several buffers from one object to another:\n1 2 3 4 5 6 7 8 9  // Copy several buffers (images)  for( int i = 0; i \u0026lt; MIN( conf_.size(), src.conf_.size() ); ++ i ) { // Copy pixels  for( int j = 0; j \u0026lt; MIN( sizeOfBuffer_, src.","title":"Fast way of copying byte array in C/C++ (With measurements)"},{"content":"Source:[Diary Of An x264 Developer](http://x264dev.multimedia.cx/archives/472\" target=\"_blank)Filed under:\u0026nbsp;[benchmark](http://x264dev.multimedia.cx/archives/category/benchmark\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in benchmark),[H.264](http://x264dev.multimedia.cx/archives/category/h264\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in H.264),[stupidity](http://x264dev.multimedia.cx/archives/category/stupidity\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in stupidity),[test sequences](http://x264dev.multimedia.cx/archives/category/test-sequences\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in test sequences)\u0026nbsp;::Over the past few years, practically everyone and their dog has published some sort of encoder comparison.\u0026nbsp; Sometimes they’re actually intended to be something for the world to rely on, like the old Doom9 comparisons and the MSU comparisons.\u0026nbsp; Other times, they’re just to scratch an itch — someone wants to decide for themselves what is better.\u0026nbsp; And sometimes they’re just there to outright lie in favor of whatever encoder the author likes best.\u0026nbsp; The latter is practically an\u0026nbsp;expected feature\u0026nbsp;on the websites of commercial encoder vendors. One thing almost all these comparisons have in common — particularly (but not limited to!) the ones done without consulting experts — is that they are horribly done.\u0026nbsp; They’re usually easy to spot: for example, two videos at totally different bitrates are being compared, or the author complains about one of the videos being “washed out” (i.e. he screwed up his colorspace conversion).\u0026nbsp; Or the results are simply nonsensical.\u0026nbsp; Many of these problems result from the person running the test not “sanity checking” the results to catch mistakes that he made in his test.\u0026nbsp; Others are just outright intentional. The result of all these mistakes, both intentional and accidental, is that the results of encoder comparisons tend to be all over the map, to the point of absurdity.\u0026nbsp; For any pair of encoders, it’s practically a given that a comparison exists somewhere that will “prove” any result you want to claim, even if the result would be beyond impossible in any sane situation.\u0026nbsp; This often results in the appearance of a “controversy” even if there isn’t any. Keep in mind that every single mistake I mention in this article\u0026nbsp;has actually been done, usually in more than one comparison.\u0026nbsp; And before I offend anyone, keep in mind that when I say “cheating”, I don’t mean to imply that everyone that makes the mistake is doing it intentionally.\u0026nbsp; Especially among amateur comparisons, most of the mistakes are probably honest. So, without further ado, we will investigate a wide variety of ways, from the blatant to the subtle, with which you too can cheat on your encoder comparisons.   Blatant cheating 1.\u0026nbsp; Screw up your colorspace conversions.\u0026nbsp; A common misconception is that converting from YUV to RGB and back is a simple process where nothing can go wrong.\u0026nbsp; This is quite untrue. There are two primary attributes of YUV: PC range (0-255) vs TV range (16-235) and BT.709 vs BT.601 conversion coefficients.\u0026nbsp; That sums up to a total of 4 possible different types of YUV.\u0026nbsp; When people compare encoders, they often use different frontends, some of which make incorrect assumptions about these attributes. Incorrect assumptions are so common that it’s often a matter of luck whether the tool gets it right or not.\u0026nbsp; It doesn’t help that most videos don’t even properly signal which they are to begin with!\u0026nbsp; Often even the tool that the person running the comparison is using to view the source material gets the conversion wrong. Subsampling YUV (aka what everyone uses) adds yet another dimension to the problem: the locations which the chroma data represents (“chroma siting”) isn’t constant.\u0026nbsp; For example, JPEG and MPEG-2 define different positions.\u0026nbsp; This is even worse because almost nobody actually handles this correctly — the best approach is to simply make sure none of your software is doing any conversion.\u0026nbsp; A mistake in chroma siting is what created that infamous PSNR graph showing Theora beating x264, which has been cited for ages since despite the developers themselves retracting it after realizing their mistake. Keep in mind that the video encoder is not responsible for colorspace conversion — almost all video encoders operate in the YUV domain (usually subsampled 4:2:0 YUV, aka YV12).\u0026nbsp; Thus any problem in colorspace conversion is usually the fault of the tools used, not the actual encoder. How to spot it:\u0026nbsp;“The color is a bit off” or “the contrast of the video is a bit duller”.\u0026nbsp; There were a staggering number of “H.264 vs Theora” encoder comparisons which came out in favor of one or the other solely based on “how well the encoder kept the color” — making the results entirely bogus. 2.\u0026nbsp; Don’t compare at the same (or nearly the same) bitrate.\u0026nbsp;I saw a VP8 vs x264 comparison the other day that gave VP8 30% more bitrate and then proceeded to demonstrate that it got better PSNR.\u0026nbsp;You would think this is blindingly obvious, but people still make this mistake!\u0026nbsp; The most common cause of this is assuming that encoders will successfully reach the target bitrate you ask of them — particularly with very broken encoders that don’t.\u0026nbsp; Always check the output filesizes of your encodes. How to spot it:\u0026nbsp;The comparison lists perfectly round bitrates for every single test, as opposed to the actual bitrates achieved by the encoders, which will never be exactly matching in any real test. 3.\u0026nbsp; Use unfair encoding settings.\u0026nbsp;This is a bit of a wide topic: there are many ways to do this.\u0026nbsp; We’ll cover the more blatant ones in this part.\u0026nbsp; Here’s some common ones: a.\u0026nbsp; Simply cheat.\u0026nbsp;Intentionally pick awful settings for the encoder you don’t like. b.\u0026nbsp; Don’t consider performance.\u0026nbsp;Pick encoding settings without any regard for some particular performance goal.\u0026nbsp; For example, it’s perfectly reasonable to say “use the best settings possible, regardless of speed”.\u0026nbsp; It’s also reasonable to look for a particular encoding speed target.\u0026nbsp; But what isn’t reasonable is to pick extremely fast settings for one encoder and extremely slow settings for another encoder. c.\u0026nbsp; Don’t attempt match compatibility options when it’s reasonable to do so.\u0026nbsp;Keyframe interval is a classic one of these: shorter values reduce compression but improve seeking.\u0026nbsp; An easy way to cheat is to simply not set them to the same value, biasing towards whatever encoder has the longer interval.\u0026nbsp; This is most common as an accidental mistake with comparisons involving ffmpeg, where the default keyframe interval is an insanely low 12 frames. How to spot it:\u0026nbsp;The comparison doesn’t document its approach regarding choice of encoding settings. 4.\u0026nbsp; Use ratecontrol methods unfairly.\u0026nbsp;Constant bitrate is not the same as average bitrate — using one instead of the other is a great way to completely ruin a comparison.\u0026nbsp; Another method is to use 1-pass bitrate mode for one encoder and 2-pass or constant quality for another.\u0026nbsp; A good general approach is that, for any given encoder, one should use 2-pass if available and constant quality if not (it may take a few runs to get the bitrate you want, of course). Of course, it’s also fine to run a comparison with a particular mode in mind — for example, a comparison targeted at streaming applications might want to test using 1-pass CBR.\u0026nbsp; Of course, in such a case, if CBR is not available in an encoder, you\u0026nbsp;can’t compare to that encoder. How to spot it:\u0026nbsp;It’s usually pretty obvious if the encoding settings are given. 5.\u0026nbsp; Use incredibly old versions of encoders.\u0026nbsp;As it happens, Debian stable is not the best source for the most recent encoding software.\u0026nbsp; Equally, using recent versions known to be buggy. 6.\u0026nbsp; Don’t distinguish between video formats and the software that encodes them.\u0026nbsp;This is incredibly common: I’ve seen tests that claim to compare “H.264″ against something else while in fact actually comparing “Quicktime” against something else.\u0026nbsp; It’s impossible to compare all H.264 encoders at once, so don’t even try — just call the comparison “Quicktime versus X” instead of “H.264 versus X”.\u0026nbsp; Or better yet, use a good H.264 encoder, like x264 and don’t bother testing awful encoders to begin with. Less-obvious cheating1.\u0026nbsp; Pick a bitrate that’s way too low.\u0026nbsp;Low bitrate testing is very effective at making differences between encoders obvious, particularly if doing a visual comparison.\u0026nbsp; But past a certain point, it becomes impossible for some encoders to keep up.\u0026nbsp; This is usually an artifact of the video format itself — a scalability limitation.\u0026nbsp; Practically all DCT-based formats have this kind of limitation (wavelets are mostly immune). In reality, this is rarely a problem, because one could merely downscale the video to resolve the problem — lower resolutions need fewer bits.\u0026nbsp; But people rarely do this in comparisons (it’s hard to do it fairly), so the best approach is to simply not use absurdly low bitrates.\u0026nbsp; What is “absurdly low”?\u0026nbsp; That’s a hard question — it ends up being a matter of using one’s best judgement. This tends to be less of a problem in larger-scale tests that use many different bitrates. How to spot it:\u0026nbsp;At least one of the encoders being compared falls apart completely and utterly in the screenshots. Biases towards, a lot:\u0026nbsp;Video formats with completely scalable coding methods (Dirac, Snow, JPEG-2000, SVC). Biases towards, a little:\u0026nbsp;Video formats with coding methods that improve scalability, such as arithmetic coding, B-frames, and run-length coding.\u0026nbsp; For example, H.264 and Theora tend to be more scalable than MPEG-4. 2.\u0026nbsp; Pick a bitrate that’s way too high.\u0026nbsp;This is staggeringly common mistake: pick a bitrate so high that all of the resulting encodes look absolutely perfect.\u0026nbsp; The claim is then made that “there’s no significant difference” between any of the encoders tested.\u0026nbsp; This is surprisingly easy to do inadvertently on sources like Big Buck Bunny, which looks transparent at relatively low bitrates.\u0026nbsp; An equally common but similar mistake is to test at a bitrate that isn’t so high that the videos look perfect, but high enough that they all look very good.\u0026nbsp; The claim is then made that “the difference between these encoders is small”.\u0026nbsp; Well, of course, if you give everything tons of bitrate, the difference between encoders is small. How to spot it:\u0026nbsp;You can’t tell which image is the source and which is the encode. 3.\u0026nbsp;\u0026nbsp;[Making invalid comparisons using objective metrics](http://x264dev.multimedia.cx/?p=458\" target=\"_blank).\u0026nbsp;I explained this earlier in the linked blog post, but in short, if you’re going to measure PSNR, make sure all the encoders are optimized for PSNR.\u0026nbsp; Equally, if you’re going to leave the encoder optimized for visual quality, don’t measure PSNR — post screenshots instead.\u0026nbsp; Same with SSIM or any other objective metric.\u0026nbsp; Furthermore, don’t blindly do metric comparisons — always at least\u0026nbsp;look\u0026nbsp;at the output as a sanity test.\u0026nbsp; Finally,\u0026nbsp;do not claim that PSNR is particularly representative of visual quality, because it isn’t. How to spot it:\u0026nbsp;Encoders with psy optimizations, such as x264 or Theora 1.2, do considerably worse than expected in PSNR tests, but look much better in visual comparisons. 4.\u0026nbsp; Lying with graphs.\u0026nbsp;Using misleading scales on graphs is a great way to make the differences between encoders seem larger or smaller than they actually are.\u0026nbsp; A common mistake is to scale SSIM linearly: in fact, 0.99 is about twice as good as 0.98, not 1% better.\u0026nbsp; One solution for this is to use db to compare SSIM values. 5.\u0026nbsp; Using lossy screenshots.\u0026nbsp;Posting screenshots as JPEG is a silly, pointless way to worsen an encoder comparison. Subtle cheating1.\u0026nbsp; Unfairly pick screenshots for comparison.\u0026nbsp;Comparing based on stills is not ideal, but it’s often vastly easier than comparing videos in motion.\u0026nbsp; But it also opens up the door to unfairness.\u0026nbsp; One of the most common mistakes is to pick a frame immediately after (or on) a keyframe for one encoder, but which isn’t for the other encoder.\u0026nbsp; Particularly in the case of encoders that massively boost keyframe quality, this will unfairly bias in favor of the one with the recent keyframe. How to spot it:\u0026nbsp;It’s very difficult to tell, if not impossible, unless they provide the video files to inspect. 2.\u0026nbsp; Cherry-pick source videos.\u0026nbsp;Good source videos are incredibly hard to come by — almost everything is already compressed and what’s left is usually a very poor example of real content.\u0026nbsp; Here’s some common ways to bias unfairly using cherry-picking: a.\u0026nbsp; Pick source videos that are already heavily compressed.\u0026nbsp;Pre-compressed source isn’t much of an issue if your target quality level for testing is much lower than that of the source, since any compression artifacts in the source will be a lot smaller than those created by the encoders.\u0026nbsp; But if the source is already very compressed, or you’re testing at a relatively high quality level, this becomes a significant issue. Biases towards:\u0026nbsp;Anything that uses a similar transform to the source content.\u0026nbsp; For MPEG-2 source material, this biases towards formats that use the 8x8dct or a very close approximation: MPEG-1/2/4, H.263, and Theora.\u0026nbsp; For H.264 source material, this biases towards formats that use a 4×4 transform: H.264 and VP8. b.\u0026nbsp; Pick standard test clips that were not intended for this purpose.\u0026nbsp;There are a wide variety of uncompressed “[standard test clips](http://media.xiph.org/video/derf/\" target=\"_blank)“.\u0026nbsp; Some of these are not intended for general-purpose use, but rather exist to test specific encoder capabilities.\u0026nbsp; For example, Mobile Calendar (“mobcal”) is extremely sharp and low motion, serving to test interpolation capabilities.\u0026nbsp; It will bias incredibly heavily towards whatever encoder uses more B-frames and/or has higher-precision motion compensation.\u0026nbsp; Other test clips are almost completely static, such as the classic “akiyo”.\u0026nbsp; These are also not particularly representative of real content. c.\u0026nbsp; Pick very noisy content.\u0026nbsp;Noise is — by definition — not particularly compressible.\u0026nbsp; Both in terms of PSNR and visual quality, a very noisy test clip will tend to reduce the differences between encoders dramatically. d.\u0026nbsp; Pick a test clip to exercise a specific encoder feature.\u0026nbsp;I’ve often used short clips from\u0026nbsp;[Touhou games](http://x264.nl/developers/Dark_Shikari/Flash/saextra.html\" target=\"_blank)\u0026nbsp;to demonstrate the effectiveness of x264′s macroblock-tree algorithm.\u0026nbsp; I’ve sometimes even used it to compare to other encoders as part of such a demonstration.\u0026nbsp; I’ve also used the standard test clip “parkrun” as a demonstration of adaptive quantization.\u0026nbsp; But claiming that either is representative of most real content — and thus can be used as a general determinant of how good encoders are — is of course insane. e.\u0026nbsp; Simply encode a bunch of videos and pick the one your favorite encoder does best on. 3.\u0026nbsp; Preprocessing the source.\u0026nbsp;A encoder test is a test of encoders, not preprocessing.\u0026nbsp; Some encoding apps may add preprocessors to the source, such as noise reduction.\u0026nbsp; This may make the video look better — possibly even better than the source — but it’s not a fair part of comparing the actual encoders. 4.\u0026nbsp; Screw up decoding.\u0026nbsp;People often forget that in addition to encoding, a test also involves decoding — a step which is equally possible to do wrong.\u0026nbsp; One common error caused by this is in tests of Theora on content whose resolution isn’t divisible by 16.\u0026nbsp; Decoding is often done with ffmpeg — which doesn’t crop the edges properly in some cases.\u0026nbsp; This isn’t really a big deal visually, but in a PSNR comparison, misaligning the entire frame by 4 or 8 pixels is a great way of completely invalidating the results. The greatest mistake of allAbove all, the biggest and most common mistake — and the one that leads to many of the problems mentioned here –\u0026nbsp; is the mistaken belief that one, or even a few tests can really represent all usage fairly.\u0026nbsp; Any comparison has to have some specific goal — to compare something in some particular case, whether it be “maximum offline compression ignoring encoding speed” or “real-time high-speed video streaming” or whatnot.\u0026nbsp; And even then, no comparison can represent all use-cases in that category alone.\u0026nbsp; An encoder comparison can only be honest if it’s aware of its limitations. ","permalink":"https://serge-m.github.io/posts/how-to-cheat-on-video-encoder/","summary":"Source:[Diary Of An x264 Developer](http://x264dev.multimedia.cx/archives/472\" target=\"_blank)Filed under:\u0026nbsp;[benchmark](http://x264dev.multimedia.cx/archives/category/benchmark\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in benchmark),[H.264](http://x264dev.multimedia.cx/archives/category/h264\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in H.264),[stupidity](http://x264dev.multimedia.cx/archives/category/stupidity\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in stupidity),[test sequences](http://x264dev.multimedia.cx/archives/category/test-sequences\" rel=\"category tag\" style=\"letter-spacing: 0px;\" title=\"View all posts in test sequences)\u0026nbsp;::Over the past few years, practically everyone and their dog has published some sort of encoder comparison.\u0026nbsp; Sometimes they’re actually intended to be something for the world to rely on, like the old Doom9 comparisons and the MSU comparisons.","title":"How to cheat on video encoder comparisons"},{"content":"I needed to launch multiview compression using codec from Intel MVC. Approximately a half a year ago I launched it normally. Yesterday it tried and I got such an error:\n$ ./sample_encode.exe h264 -i input.yuv -o output.h264 -w 1920 -h 1080 Return on error: error code -3, .\\src\\pipeline_encode.cpp 865 Return on error: error code 1, .\\src\\sample_encode.cpp 343 Frame number: 0  I started debug and found out the error appears in\nMFXVideoSession::mfxStatus Init(mfxIMPL impl, mfxVersion *ver) { return MFXInit(impl, ver, \u0026amp;m_session); }  Error code\n MFX_ERR_UNSUPPORTED = -3, /* undeveloped feature */  Of course, the cause was in missing dll.\nlibmfxsw32.dlllibmfxsw64.dll  After I added it to console\u0026rsquo;s directory everything became fine.\nPS I could not build sample_encode with Visual Studio 2008. So use 2010 instead.\n","permalink":"https://serge-m.github.io/posts/using-h264multiview-codec-from-intel/","summary":"I needed to launch multiview compression using codec from Intel MVC. Approximately a half a year ago I launched it normally. Yesterday it tried and I got such an error:\n$ ./sample_encode.exe h264 -i input.yuv -o output.h264 -w 1920 -h 1080 Return on error: error code -3, .\\src\\pipeline_encode.cpp 865 Return on error: error code 1, .\\src\\sample_encode.cpp 343 Frame number: 0  I started debug and found out the error appears in","title":"Using h264/multiview codec from Intel Media SDK"},{"content":"ls -w1 */  - show only subdirectories in current directory (*/) and display in 1 column (-w1) ls -c  - sort by date\n","permalink":"https://serge-m.github.io/posts/using-ls-command/","summary":"ls -w1 */  - show only subdirectories in current directory (*/) and display in 1 column (-w1) ls -c  - sort by date","title":"Using ls command"},{"content":"АлёнаC++: http://alenacpp.blogspot.ru/2005/08/c.html\nstatic_cast между указателями корректно, только если один из указателей - это указатель на void или если это приведение между объектами классов, где один класс является наследником другого. То есть для приведения к какому-либо типу от void*, который возвращает malloc, следует использовать static_cast.\nint * p = static_cast\u0026lt;int*\u0026gt;(malloc(100)); Если приведение не удалось, возникнет ошибка на этапе компиляции. Однако, если это приведение между указателями на объекты классов вниз по иерархии и оно не удалось, результат операции undefined. То есть, возможно такое приведение: static_cast\u0026lt;Derived*\u0026gt;(pBase), даже если pBase не указывает на Derived, но программа при этом будет вести себя странно.\n","permalink":"https://serge-m.github.io/posts/blog-post-2/","summary":"АлёнаC++: http://alenacpp.blogspot.ru/2005/08/c.html\nstatic_cast между указателями корректно, только если один из указателей - это указатель на void или если это приведение между объектами классов, где один класс является наследником другого. То есть для приведения к какому-либо типу от void*, который возвращает malloc, следует использовать static_cast.\nint * p = static_cast\u0026lt;int*\u0026gt;(malloc(100)); Если приведение не удалось, возникнет ошибка на этапе компиляции. Однако, если это приведение между указателями на объекты классов вниз по иерархии и оно не удалось, результат операции undefined.","title":"Некоторые заметки о приведении типов в СиПлюсПлюс"},{"content":"Errosion and dilation in AE is implemented in Minimax filter. See settings bellow: ","permalink":"https://serge-m.github.io/posts/math-morphology-in-adobe-after-effects/","summary":"Errosion and dilation in AE is implemented in Minimax filter. See settings bellow: ","title":"Math morphology in Adobe After Effects"},{"content":" I had pretty simple code with opencv:\n cv::Mat t;  t = cv::imread( \u0026ldquo;qq.bmp\u0026rdquo; ) ;  cv::imwrite( \u0026ldquo;q.bmp\u0026rdquo;, t );  cv::namedWindow( \u0026ldquo;asdads\u0026rdquo;, CV_WINDOW_AUTOSIZE );  cv::imshow( \u0026ldquo;asdads\u0026rdquo;, t );  cv::waitKey( -1 );  return 0;\nIt caused crash in debug configuration:\nOpenCV Error: Bad flag (parameter or structure field) (Unrecognized or unsupport ed array type) in unknown function, file ......\\src\\opencv\\modules\\core\\src\\ar ray.cpp, line 2482  In release configuration it was ok. Finally i found the solution. Both in debug and release configurations I used static libraries for release conguration  opencv_core231.lib opencv_imgproc231.lib opencv_highgui231.lib  And one should use opencv_core231d.lib opencv_imgproc231d.lib opencv_highgui231d.lib for debug.\nIt seems wrong libraries cause crashes only sometimes. I had used release version of static opencv libraries for my debu and release configurations for a long time an only now it caused problems\n","permalink":"https://serge-m.github.io/posts/opencv-and-opencv-error-bad-flag/","summary":"I had pretty simple code with opencv:\n cv::Mat t;  t = cv::imread( \u0026ldquo;qq.bmp\u0026rdquo; ) ;  cv::imwrite( \u0026ldquo;q.bmp\u0026rdquo;, t );  cv::namedWindow( \u0026ldquo;asdads\u0026rdquo;, CV_WINDOW_AUTOSIZE );  cv::imshow( \u0026ldquo;asdads\u0026rdquo;, t );  cv::waitKey( -1 );  return 0;\nIt caused crash in debug configuration:\nOpenCV Error: Bad flag (parameter or structure field) (Unrecognized or unsupport ed array type) in unknown function, file ......\\src\\opencv\\modules\\core\\src\\ar ray.cpp, line 2482  In release configuration it was ok.","title":"Opencv and \"OpenCV Error: Bad flag\""},{"content":"How to copy image from unsigned char buffer, resize and save to file. If you use single-channel image.\n#include \u0026lt;opencv/cv.h\u0026gt; #include \u0026lt;opencv/highgui.h\u0026gt; int coeff = 4; cv::Mat src( height, width, CV_8UC1, (void *) source_byte_beffer ) ); cv::Mat small(height/ coeff, width/ coeff, CV_8UC1 ); cv::Size s_half(width/ coeff, height/ coeff); cv::resize( src, small, s_half, 1, 1, cv::INTER_LINEAR ); // resize from src to small IplImage* writeImage; writeImage=cvCloneImage(\u0026amp;(IplImage)src); cvSaveImage(\u0026quot;src1.bmp\u0026quot;, writeImage); cvReleaseImage( \u0026amp;writeImage ); writeImage=cvCloneImage(\u0026amp;(IplImage)small); cvSaveImage(\u0026quot;little.bmp\u0026quot;, writeImage); cvReleaseImage( \u0026amp;writeImage ); //Another way of writtting: /*cvWrite(\u0026quot;src.bmp\u0026quot;, src ); cv::imwrite( \u0026quot;small.bmp\u0026quot;, small );*/ ","permalink":"https://serge-m.github.io/posts/opencv-copy-image-from-unsigned-char/","summary":"How to copy image from unsigned char buffer, resize and save to file. If you use single-channel image.\n#include \u0026lt;opencv/cv.h\u0026gt; #include \u0026lt;opencv/highgui.h\u0026gt; int coeff = 4; cv::Mat src( height, width, CV_8UC1, (void *) source_byte_beffer ) ); cv::Mat small(height/ coeff, width/ coeff, CV_8UC1 ); cv::Size s_half(width/ coeff, height/ coeff); cv::resize( src, small, s_half, 1, 1, cv::INTER_LINEAR ); // resize from src to small IplImage* writeImage; writeImage=cvCloneImage(\u0026amp;(IplImage)src); cvSaveImage(\u0026quot;src1.bmp\u0026quot;, writeImage); cvReleaseImage( \u0026amp;writeImage ); writeImage=cvCloneImage(\u0026amp;(IplImage)small); cvSaveImage(\u0026quot;little.","title":"OpenCV. Copy image from unsigned char buffer, resize and save to file"},{"content":"There was always a problem for me to copy only certain directories from one branch to another. For example we have following directory structure:\ntrunk - project1 - project2 -subdir1 -subdir2 -subdir3 We want to copy only subdir2 to a branch /branches/branch1 with saving all the structure of projects.branches/branch1  - project1 - project2 -subdir2\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;font-family: Times, Times New Roman, serif; font-size: 14px;\u0026quot;\u0026gt;  We want to copy only subdir2 to a branch /branches/branch1 with saving all the structure of projects.\nSuch a manipulation is required if you want to make a private branch for outsorcer and you don't wan him to see the other parts of the code. However you want to support reintegration of the branch back to the trunk.The solution is\u0026nbsp;svn cp --parents ./project2/subdir2 http://repo.url/branches/branch1/project2/subdir2orsvn cp --parents http:///trunk/project2/subdir2 /branches/branch1/project2/subdir2Subversion creates all intermediate directories. ","permalink":"https://serge-m.github.io/posts/svn-copy-certain-subdirectories-to/","summary":"There was always a problem for me to copy only certain directories from one branch to another. For example we have following directory structure:\ntrunk - project1 - project2 -subdir1 -subdir2 -subdir3 We want to copy only subdir2 to a branch /branches/branch1 with saving all the structure of projects.branches/branch1  - project1 - project2 -subdir2\u0026lt;/span\u0026gt;\u0026lt;span style=\u0026quot;font-family: Times, Times New Roman, serif; font-size: 14px;\u0026quot;\u0026gt;  We want to copy only subdir2 to a branch /branches/branch1 with saving all the structure of projects.","title":"SVN copy certain subdirectories to a branch"},{"content":"Trim 5 frames starting from 160-th frame and write to png sequence\nffmpeg -pix_fmt yuv420p -s 1920x1088 -r 1 -i input_video.yuv -r 1 -ss 160 -frames 5 output_sequence_%d.png  size of input video is 1920x1088, format YUV420 progressive.\nUPD: ffmpeg is renamed to avconv. Using it for trimming AVI video:\navconv -ss 00:58:00 -t 00:59:30 -i ./video.avi frame_%05d.png  UPD2: it seems ffmpeg is back.\n","permalink":"https://serge-m.github.io/posts/trim-frames-from-raw-yuv-video-using/","summary":"Trim 5 frames starting from 160-th frame and write to png sequence\nffmpeg -pix_fmt yuv420p -s 1920x1088 -r 1 -i input_video.yuv -r 1 -ss 160 -frames 5 output_sequence_%d.png  size of input video is 1920x1088, format YUV420 progressive.\nUPD: ffmpeg is renamed to avconv. Using it for trimming AVI video:\navconv -ss 00:58:00 -t 00:59:30 -i ./video.avi frame_%05d.png  UPD2: it seems ffmpeg is back.","title":"Trim frames from raw YUV video using FFMPEG"},{"content":"I processed depth maps using avisynth 2.5.8 and I found out it causes color degradation\nI have a depth map (from http://vision.middlebury.edu/stereo/data/scenes2001/data/imagehtml/sawtooth.html) And two scripts1)\u0026nbsp;just_show.avsv = ImageSource( \"depth.jpg\" ).Trim(0, -1) return v2) overlay.avs v = ImageSource( \"depth.jpg\" ).Trim(0, -1) return overlay( v, v, mode = \"blend\", opacity = 0.80)\u0026nbsp; They have slightly different colors. ![](http://4.bp.blogspot.com/-Lj-uQIVor4c/UcP-amr12PI/AAAAAAAAAZQ/RvXjjb2Vc14/s320/just_show.png)just show![](http://1.bp.blogspot.com/-O1GkOsIX8rU/UcP-avb8uEI/AAAAAAAAAZU/x9p_nPsHSwU/s320/overlay.png)overlayOverlay has chroma componentThe cause was in wrong color conversion in avisynth 2.5.8. I downloaded\u0026nbsp;AVS 2.6.0 Alpha 4 [130114] and the problem gone.\u0026nbsp;[http://sourceforge.net/projects/avisynth2/files/AviSynth_Alpha_Releases/AVS%202.6.0%20Alpha%204%20%5B130114%5D/](http://sourceforge.net/projects/avisynth2/files/AviSynth_Alpha_Releases/AVS%202.6.0%20Alpha%204%20%5B130114%5D/) ","permalink":"https://serge-m.github.io/posts/color-problems-in-avisynth-overlay/","summary":"I processed depth maps using avisynth 2.5.8 and I found out it causes color degradation\nI have a depth map (from http://vision.middlebury.edu/stereo/data/scenes2001/data/imagehtml/sawtooth.html) And two scripts1)\u0026nbsp;just_show.avsv = ImageSource( \"depth.jpg\" ).Trim(0, -1) return v2) overlay.avs v = ImageSource( \"depth.jpg\" ).Trim(0, -1) return overlay( v, v, mode = \"blend\", opacity = 0.80)\u0026nbsp; They have slightly different colors. ![](http://4.bp.blogspot.com/-Lj-uQIVor4c/UcP-amr12PI/AAAAAAAAAZQ/RvXjjb2Vc14/s320/just_show.png)just show![](http://1.bp.blogspot.com/-O1GkOsIX8rU/UcP-avb8uEI/AAAAAAAAAZU/x9p_nPsHSwU/s320/overlay.png)overlayOverlay has chroma componentThe cause was in wrong color conversion in avisynth 2.5.8. I downloaded\u0026nbsp;AVS 2.","title":"Color problems in avisynth-\u003eoverlay"},{"content":"in cygwin:\nPATH=$PATH:~/your_path or\nPATH=~/your_path:$PATH Add path windows\nPATH=%PATH%;C:\\Your_Path; ","permalink":"https://serge-m.github.io/posts/add-path-in-linuxcygwin-and-windows/","summary":"in cygwin:\nPATH=$PATH:~/your_path or\nPATH=~/your_path:$PATH Add path windows\nPATH=%PATH%;C:\\Your_Path; ","title":"Add path in linux/cygwin and windows"},{"content":"Never-Never-NEVER open file for reading until you think twice what mode (text/binary) of operation do you need!!\n","permalink":"https://serge-m.github.io/posts/never-never/","summary":"Never-Never-NEVER open file for reading until you think twice what mode (text/binary) of operation do you need!!","title":"Never never!"},{"content":"There are specific databases which were subjectively tested. Ans MOS was calculated for example http://www.ponomarenko.info/tid2008.htm you can download database and compare your metric to MOS by Spearman correlation\n","permalink":"https://serge-m.github.io/posts/how-to-compare-video-metrics/","summary":"There are specific databases which were subjectively tested. Ans MOS was calculated for example http://www.ponomarenko.info/tid2008.htm you can download database and compare your metric to MOS by Spearman correlation","title":"How to compare video metrics"},{"content":"See good explanation is in\n Title: Handbook of image and video processing \nAuthor(s): Bovik A. (ed.)\nSeries: Periodical:\nPublisher: Elsevier Academic Press\nYear: 2005\nEdition: 2ed.\nLanguage: English\nPages: 1429\nISBN: 9780121197926, 0121197921\n","permalink":"https://serge-m.github.io/posts/ssim-metric-for-video/","summary":"See good explanation is in\n Title: Handbook of image and video processing \nAuthor(s): Bovik A. (ed.)\nSeries: Periodical:\nPublisher: Elsevier Academic Press\nYear: 2005\nEdition: 2ed.\nLanguage: English\nPages: 1429\nISBN: 9780121197926, 0121197921","title":"SSIM metric for video"},{"content":"Нужно будет тут записать как работает метрика SSIM по человечески\n","permalink":"https://serge-m.github.io/posts/ssim/","summary":"Нужно будет тут записать как работает метрика SSIM по человечески","title":"SSIM по русски"},{"content":"To replace new lines in word use \u0026ldquo;^13\u0026rdquo; whild-char. \u0026ldquo;^p\u0026rdquo; doesn\u0026rsquo;t work :(\n","permalink":"https://serge-m.github.io/posts/replace-n-in-microsoft-word/","summary":"To replace new lines in word use \u0026ldquo;^13\u0026rdquo; whild-char. \u0026ldquo;^p\u0026rdquo; doesn\u0026rsquo;t work :(","title":"Replace \\n in Microsoft word"},{"content":"Assume we have Visual Stusio solution for console that makes some video processing. The console takes two input videos and generates the third video:\nConsole.exe --input1 video1.avi --input2 video2.avi --output result.aviWhile debugging we need to run console on several datasets. 1) dataset1 conststs of videos set1_video1.avi and set1_video2.avi 2) dataset2 conststs of videos set2_video1.avi and set2_video2.avi .... n) .... It seem rather convenient to distrubute videos between folders. One folder for each dataset. Insise the folder we should give template names to files (e.g. video1.avi and video2.avi) Therefore now we can set \u0026ldquo;Command Arguments\u0026rdquo; in \u0026ldquo;Project Properties-\u0026gt;Configuration properties-\u0026gt;Debugging\u0026rdquo; the same arguments for all cases and configurations, The thing should vary is \u0026ldquo;Working directory\u0026rdquo; in \u0026ldquo;Properties-\u0026gt;Configuration properties-\u0026gt;Debugging\u0026rdquo;.\nAnother usefull thing is setting \u0026ldquo;Environment\u0026rdquo;. Just write there PATH=%PATH%; and you don\u0026rsquo;t need to copy all dependency dll\u0026rsquo;s in every debug folder. ","permalink":"https://serge-m.github.io/posts/tips-for-debugging-of-video-processing/","summary":"Assume we have Visual Stusio solution for console that makes some video processing. The console takes two input videos and generates the third video:\nConsole.exe --input1 video1.avi --input2 video2.avi --output result.aviWhile debugging we need to run console on several datasets. 1) dataset1 conststs of videos set1_video1.avi and set1_video2.avi 2) dataset2 conststs of videos set2_video1.avi and set2_video2.avi .... n) .... It seem rather convenient to distrubute videos between folders. One folder for each dataset.","title":"Tips for debugging of video processing console"},{"content":"Bjontegaard metric (BD-PSNR) describes the distance between two RD-curved. I is useful to determine how big is the gain between \u0026ldquo;before\u0026rdquo; and \u0026ldquo;after\u0026rdquo; versions. Or determine what curve is better if they have complex form (for example, intersecting).\nI found matlab script, that calculates BD-PSNR, but it was not correct. The limits of integration were wrong. Difference between two curves can be calculated only in the area of thein projections intersection. Fixed script was verified by data provided the auther of the metric.\nDownload fixed matlab script for BD-PSNR calculation\ngithub\noriginal paper (doc)\ndata for verification (xls)\nOriginal publilcation: G. Bjontegaard, “Calculation of average PSNR differences between RD-curves (VCEG-M33),” in VCEG Meeting (ITU-T SG16 Q.6), Austin, Texas, USA, Apr. 2001\n","permalink":"https://serge-m.github.io/posts/bjontegaard-metric-matlab-script/","summary":"Bjontegaard metric (BD-PSNR) describes the distance between two RD-curved. I is useful to determine how big is the gain between \u0026ldquo;before\u0026rdquo; and \u0026ldquo;after\u0026rdquo; versions. Or determine what curve is better if they have complex form (for example, intersecting).\nI found matlab script, that calculates BD-PSNR, but it was not correct. The limits of integration were wrong. Difference between two curves can be calculated only in the area of thein projections intersection.","title":"Bjontegaard metric. Matlab script."},{"content":"I had a problem while running debug+openmp configuration of my console. Meanwhile in Release+openmp configuration everything is ok. Diagnostic message I see is something about wrong parallel configurations, system log and sxstrace.exe. The solution is following. Folder\nMicrosoft.VC90.DebugOpenMPwith files Microsoft.VC90.DebugOpenMP.manifest vcomp90d.dll\u0026nbsp;were missing. I knew this problem for the folder I specify in debug properties in visual studio: ![](http://3.bp.blogspot.com/-YWMVBmpI050/UZHxfnKVd5I/AAAAAAAAAYM/8syrhPsNbIc/s320/error_about_parallel_configurations_dialog1.png)In that folder I already had Microsoft.VC90.DebugOpenMP. But to avoid error I should place it in output directory (where my binary exe file is placed): ![](http://1.bp.blogspot.com/-rCC2c5cq7Lo/UZHyERiXNRI/AAAAAAAAAYU/wbQWL9p0MbE/s320/error_about_parallel_configurations_dialog2.png) Magick... ","permalink":"https://serge-m.github.io/posts/problems-with-debug-run-from-visual/","summary":"I had a problem while running debug+openmp configuration of my console. Meanwhile in Release+openmp configuration everything is ok. Diagnostic message I see is something about wrong parallel configurations, system log and sxstrace.exe. The solution is following. Folder\nMicrosoft.VC90.DebugOpenMPwith files Microsoft.VC90.DebugOpenMP.manifest vcomp90d.dll\u0026nbsp;were missing. I knew this problem for the folder I specify in debug properties in visual studio: ![](http://3.bp.blogspot.com/-YWMVBmpI050/UZHxfnKVd5I/AAAAAAAAAYM/8syrhPsNbIc/s320/error_about_parallel_configurations_dialog1.png)In that folder I already had Microsoft.VC90.DebugOpenMP. But to avoid error I should place it in output directory (where my binary exe file is placed): !","title":"Problems with debug run from Visual Studio with openMP"},{"content":"http://boost.teeks99.com/ - Another place with boost binaries. It includes build for version 1.49\nBefore I new only http://www.boostpro.com/download/\n","permalink":"https://serge-m.github.io/posts/another-place-with-boost-binaries/","summary":"http://boost.teeks99.com/ - Another place with boost binaries. It includes build for version 1.49\nBefore I new only http://www.boostpro.com/download/","title":"Another place with boost binaries"},{"content":"I got error message Error spawning cmd.exeduring compiling of Blender sources. The cause was in system PATH environment variable. Some buggy software I had installed and uninstalled before erased all contents of PATH. So I found out that Visual Studio 2008 has missing paths to\n%SystemRoot%\\system32; %SystemRoot%; %SystemRoot%\\System32\\Wbem;A added it and the problem gone.\u0026nbsp; ","permalink":"https://serge-m.github.io/posts/error-spawning-cmdexe/","summary":"I got error message Error spawning cmd.exeduring compiling of Blender sources. The cause was in system PATH environment variable. Some buggy software I had installed and uninstalled before erased all contents of PATH. So I found out that Visual Studio 2008 has missing paths to\n%SystemRoot%\\system32; %SystemRoot%; %SystemRoot%\\System32\\Wbem;A added it and the problem gone.\u0026nbsp; ","title":"Error spawning cmd.exe"},{"content":"http://blendersushi.blogspot.ru/\n one of blender\u0026rsquo;s developers  http://blender3d.org.ua/tutorial/%D0%92%D0%B8%D0%B4%D0%B5%D0%BE%D1%83%D1%80%D0%BE%D0%BA%D0%B8.html\n ukrainian blog, video tutorials in russian  ","permalink":"https://serge-m.github.io/posts/blogs-about-blender/","summary":"http://blendersushi.blogspot.ru/\n one of blender\u0026rsquo;s developers  http://blender3d.org.ua/tutorial/%D0%92%D0%B8%D0%B4%D0%B5%D0%BE%D1%83%D1%80%D0%BE%D0%BA%D0%B8.html\n ukrainian blog, video tutorials in russian  ","title":"Blogs about blender"},{"content":"Synapse Compositor Seems unsupported for a long time\nVery raw\nBlender Good stuff, but it is designed originally for 3D modeling. Video editing is performed in composing mode. Rendering is veeery sloooowww. Open source. Available for linux and windows. Source code\n","permalink":"https://serge-m.github.io/posts/overview-of-open-source-node-based/","summary":"Synapse Compositor Seems unsupported for a long time\nVery raw\nBlender Good stuff, but it is designed originally for 3D modeling. Video editing is performed in composing mode. Rendering is veeery sloooowww. Open source. Available for linux and windows. Source code","title":"Overview of open-source node-based video editors (composers)"},{"content":"So here we have two filters:\n Crop Reformat  To cut a part from an image we use Crop. It also has option \u0026ldquo;reformat\u0026rdquo;. Reformat cuts also borders behind area. If \u0026ldquo;Reformat\u0026rdquo; is disabled, black borders remain after crop.\n","permalink":"https://serge-m.github.io/posts/image-resize-in-nuke/","summary":"So here we have two filters:\n Crop Reformat  To cut a part from an image we use Crop. It also has option \u0026ldquo;reformat\u0026rdquo;. Reformat cuts also borders behind area. If \u0026ldquo;Reformat\u0026rdquo; is disabled, black borders remain after crop.","title":"Image resize in Nuke"},{"content":"How to visualize gradients with torch-lightning and tensorboard in your model class define a optimizer_step.\nclass Model(pl.LightningModule): # ... def optimizer_step( self, epoch: int, batch_idx: int, optimizer, optimizer_idx: int, second_order_closure = None, ) -\u0026gt; None: if self.trainer.use_tpu and XLA_AVAILABLE: xm.optimizer_step(optimizer) elif isinstance(optimizer, torch.optim.LBFGS): optimizer.step(second_order_closure) else: optimizer.step() #### Gradient reporting start ### if batch_idx % 500 == 0: for tag, param in self.model.named_parameters(): self.logger.experiment.add_histogram('{}_grad'.format(tag), param.grad.cpu().detach()) #### Gradient reporting end ### # clear gradients optimizer.zero_grad()  I have copied the code from the torch-lightning code. We have to add the gradient logging right before optimizer.zero_grad call.\nThen in the trainer definition we have to specify tensorboard logger:\nlogger = TensorBoardLogger(\u0026quot;lightning_logs\u0026quot;, name=name) # ... trainer = pl.Trainer( max_epochs=hparams.epochs, gpus=hparams.gpus, logger=logger, )  This is how the visualization look like: In the code of torch-lightning there is also a hook on_before_zero_grad for that: github search But I wasn\u0026rsquo;t able to understand how to use it.\n","permalink":"https://serge-m.github.io/posts/torch-lightning-library/","summary":"How to visualize gradients with torch-lightning and tensorboard in your model class define a optimizer_step.\nclass Model(pl.LightningModule): # ... def optimizer_step( self, epoch: int, batch_idx: int, optimizer, optimizer_idx: int, second_order_closure = None, ) -\u0026gt; None: if self.trainer.use_tpu and XLA_AVAILABLE: xm.optimizer_step(optimizer) elif isinstance(optimizer, torch.optim.LBFGS): optimizer.step(second_order_closure) else: optimizer.step() #### Gradient reporting start ### if batch_idx % 500 == 0: for tag, param in self.model.named_parameters(): self.logger.experiment.add_histogram('{}_grad'.format(tag), param.grad.cpu().detach()) #### Gradient reporting end ### # clear gradients optimizer.","title":"Torch-Lightning library (draft)"},{"content":"About camera models   OpenCV camera model and calibration\n  Difference between pinhole camera model and thin lens model. One more note, One more article Pinhole camera model\n  ","permalink":"https://serge-m.github.io/posts/camera-model-and-projective-geometry/","summary":"About camera models   OpenCV camera model and calibration\n  Difference between pinhole camera model and thin lens model. One more note, One more article Pinhole camera model\n  ","title":"Camera model and projective geometry"},{"content":"I usually prefer to keep (ana)conda deactivate in my system by default. One can chose such and option during conda installation. In order to activate base conda environment I run:\nsource ~/anaconda3/etc/profile.d/conda.sh  or\nsource ~/miniconda3/etc/profile.d/conda.sh  ","permalink":"https://serge-m.github.io/posts/conda-cheat-sheet/","summary":"I usually prefer to keep (ana)conda deactivate in my system by default. One can chose such and option during conda installation. In order to activate base conda environment I run:\nsource ~/anaconda3/etc/profile.d/conda.sh  or\nsource ~/miniconda3/etc/profile.d/conda.sh  ","title":"Conda cheat sheet"},{"content":"Draft  An Overview of ResNet and its Variants, 2017  ","permalink":"https://serge-m.github.io/posts/deep-learning-in-computer-vision/","summary":"Draft  An Overview of ResNet and its Variants, 2017  ","title":"Deep learning in computer vision (draft)"},{"content":"Arduino (ATMega328p) boards can be used to control multiple LEDs. To simplify the management of the pulse width modulation and use only a couple of arduino's pins for many LEDs I used PCA9685 controller.\nPCA9685 is connected to Arduino using I2C interface that requires only two data pins.\nArduino connects to PCA9685 using Adafruit_PWMServoDriver library:\n#include \u0026lt;Adafruit_PWMServoDriver.h\u0026gt; Adafruit_PWMServoDriver pwm = Adafruit_PWMServoDriver();  Initialization in setup():\npwm.begin(); pwm.setPWMFreq(1600); // This is the maximum PWM frequency  Then for each loop we will blink with 2 LEDs. That can be easily extended to more LEDs:\nInitialization in setup():\nvoid loop() { // set state 1 digitalWrite(LED_BUILTIN, HIGH); pwm.setPWM(0, 0, 3072); pwm.setPWM(1, 0, 4096); // wait delay(500); // set state 2 digitalWrite(LED_BUILTIN, LOW); pwm.setPWM(0, 0, 4096); pwm.setPWM(1, 0, 4095); // wait delay(500); }  pwm.setPWM(p, start, end); means that for the pin p the signal should transition from low to high at tick start (between 0..4095) and the signal should transition from high to low at tick end (between 0..4095).\nThere are special settings for full on and full off. Full on:\npwm.setPWM(pin, 4096, 0);  Full off:\npwm.setPWM(pin, 0, 4096);  Along with the other LED we will also control an LED that is built in the Arduino:\nvoid setup() { pinMode(LED_BUILTIN, OUTPUT); // ... } void loop() { digitalWrite(LED_BUILTIN, HIGH); // LED on // ... digitalWrite(LED_BUILTIN, LOW); // LED off // ... }  Here is the full listing:\n#include \u0026lt;Adafruit_PWMServoDriver.h\u0026gt; Adafruit_PWMServoDriver pwm = Adafruit_PWMServoDriver(); void setup() { pinMode(LED_BUILTIN, OUTPUT); pwm.begin(); pwm.setPWMFreq(1600); // This is the maximum PWM frequency } void loop() { // set state 1 digitalWrite(LED_BUILTIN, HIGH); pwm.setPWM(0, 0, 3072); pwm.setPWM(1, 0, 4096); // wait delay(500); // set state 2 digitalWrite(LED_BUILTIN, LOW); pwm.setPWM(0, 0, 4096); pwm.setPWM(1, 0, 4095); // wait delay(500); }  Results:\n One issue I had with that Adafruit_PWMServoDriver library is that it is blocking. If your PCA9685 is off or gets disconnected the whole program freezes. As far as I understand Adafruit_PWMServoDriver uses Wire as a backend and Wire is blocking. That is a known issues and there are some workarounds.\n","permalink":"https://serge-m.github.io/posts/blinking-multiple-leds-with-arduino-atmega328p-and-pca9685/","summary":"Arduino (ATMega328p) boards can be used to control multiple LEDs. To simplify the management of the pulse width modulation and use only a couple of arduino's pins for many LEDs I used PCA9685 controller.\nPCA9685 is connected to Arduino using I2C interface that requires only two data pins.\nArduino connects to PCA9685 using Adafruit_PWMServoDriver library:\n#include \u0026lt;Adafruit_PWMServoDriver.h\u0026gt; Adafruit_PWMServoDriver pwm = Adafruit_PWMServoDriver();  Initialization in setup():\npwm.begin(); pwm.setPWMFreq(1600); // This is the maximum PWM frequency  Then for each loop we will blink with 2 LEDs.","title":"Blinking multiple LEDs with Arduino (ATMega328p) and PCA9685"},{"content":"Parsing PWM signals For my robocar project I needed to understand the mechanism of pulse width modulation of the remote control.\nMy intention was to use Arduino as a proxy between RC-receiver and servos/ESC to be able to record the used input for imitation learning. Human driver (me) sends steering commands via the remote control (transmitter). RC receiver converts radio signal into PWM signal. Arduino captures and maybe filters the signal, saves it somehow and sends it to the servos/ESC.\nFor analysis of the PWM signal I have found a library PinChangeInterrupt:\n#include \u0026lt;PinChangeInterrupt.h\u0026gt;  the wires from the receiver are connected to the pins 10 and 11 of my arduino nano:\nconst int pin_pwm_in_steering = 11; const int pin_pwm_in_throttle = 10;  Let's create PwmListener class that listens to the interrupts on the given pin and returns the width of the last impulse.\nclass PwmListener { volatile unsigned int pwm_value_; // last pwm value volatile unsigned long prev_time_; // last time of the impulse public: const int pin; // pin to listen const int pin_as_pc_int; // pin index according to PC int public: PwmListener(int pin_, int default_value); // this function will be called on the interrupts void process(); unsigned long micros_since_last_signal() const; // return the latest pwm value unsigned int value() const; unsigned long prev_time_micros() const; };  Full code:\nclass PwmListener { volatile unsigned int pwm_value_; volatile unsigned long prev_time_; public: const int pin; const int pin_as_pc_int; public: PwmListener(int pin_, int default_value): pin(pin_), pwm_value_(default_value), prev_time_(0), pin_as_pc_int(digitalPinToPCINT(pin_)) {} void process() { uint8_t trigger = getPinChangeInterruptTrigger(pin_as_pc_int); if(trigger == RISING) prev_time_ = micros(); else if(trigger == FALLING) pwm_value_ = micros_since_last_signal(); else { // Wrong usage } } unsigned long micros_since_last_signal() const { return micros() - prev_time_; } unsigned int value() const { return pwm_value_; } unsigned long prev_time_micros() const { return prev_time_; } };  now we create two listeners and define dummy caller functions for them. The dummy functions are needed to pass them by pointer.\nPwmListener pwm_listener_steering (pin_pwm_in_steering, DEFAULT_PULSE_WIDTH); PwmListener pwm_listener_throttle (pin_pwm_in_throttle, DEFAULT_PULSE_WIDTH); void interrupt_steering() { pwm_listener_steering.process(); } void interrupt_throttle() { pwm_listener_throttle.process(); }  later in the setup() function we have to set up the pins:\nvoid setup() { // starting input pwm monitoring... pinMode(pwm_listener_steering.pin, INPUT); attachPinChangeInterrupt(pwm_listener_steering.pin_as_pc_int, \u0026amp;interrupt_steering, CHANGE); pinMode(pwm_listener_throttle.pin, INPUT); attachPinChangeInterrupt(pwm_listener_throttle.pin_as_pc_int, \u0026amp;interrupt_throttle, CHANGE); // setting up serial interface Serial.begin(115200); Serial.println(\u0026quot;info: waiting for output pwm controller...\u0026quot;); }  on every loop we send the values to the host computer:\nvoid loop() { const int steering = pwm_listener_steering.value(); const int throttle = pwm_listener_throttle.value(); // avoiding unnecessary duplication by filtering out small perturbations if ( abs(steering - old_steering) \u0026gt; 10 || abs(throttle - old_throttle) \u0026gt; 10 ) { sprintf(buf, \u0026quot;in: %d %d\\n\u0026quot;, steering, throttle); Serial.print(buf); old_steering = steering; old_throttle = throttle; } }  We have also enabled the serial interface to report the values to the host machine (connected via USB).\nOn the host machine we run python listener:\nimport serial ports = ['/dev/ttyUSB0', '/dev/ttyUSB1'] def main(): for port in ports: print(\u0026quot;connecting to port {}\u0026quot;.format(port)) try: with serial.Serial(port, 115200, timeout=0.01) as ser: print('connected') process(ser) return except serial.serialutil.SerialException as e: if e.errno == 2: continue raise raise FileNotFoundError(\u0026quot;Unable to open ports {}\u0026quot;.format(ports)) def process(ser): while(True): line = ser.readline() # read a '\\n' terminated line if line != b'': print(line) if __name__ == '__main__': main()   Analysis I have got the following graph (interactive):\n:file: /static/2019-12-pwm_visualization.htmlLet's consider steering series. It has three parts.\nactual signal. The values are around 1380. The change couple of times. That corresponds to what I actually did. Some strange signal values around 10000. I don't know what is it. Some rare noise in between. I also don't know why it's there.  We still can use that values with some filtering of the outliers.\nAnother interesting thing is that steering and throttle channels have different default values. For steering it is 1380 and for throttle it is 1540. I could adjust that values a bit, but anyway it is around 1400 for one and 1500 for another.\nAt the same time when I send PWM signal generated by Arduino I have to use 1400 as default for both to make it work properly. I don't know what causes that shift in my measurements.\n See also gist with the data robocar project robocar main repository wiki for robocar   ","permalink":"https://serge-m.github.io/posts/capture-pwm-signal-using-arduino/","summary":"Parsing PWM signals For my robocar project I needed to understand the mechanism of pulse width modulation of the remote control.\nMy intention was to use Arduino as a proxy between RC-receiver and servos/ESC to be able to record the used input for imitation learning. Human driver (me) sends steering commands via the remote control (transmitter). RC receiver converts radio signal into PWM signal. Arduino captures and maybe filters the signal, saves it somehow and sends it to the servos/ESC.","title":"Capture PWM signal using Arduino"},{"content":"Design pattern Mixin is often used in python. There are two main situations where mixins are used 1:\n You want to provide a lot of optional features for a class. You want to use one particular feature in a lot of different classes.  Order of mixins definition Order in which you use mixins defines the behaviour. Quote from 2:\nin Python the class hierarchy is defined right to left, so in this case the Mixin2 class is the base class, extended by Mixin1 and finally by BaseClass. This is usually fine because many times the mixin classes don\u0026rsquo;t override each other\u0026rsquo;s, or the base class' methods. But if you do override methods or properties in your mixins this can lead to unexpected results because the priority of how methods are resolved is from left to right.\n1 2 3 4 5 6 7 8 9 10 11  class BaseClass(object): def test(self): print (\u0026#34;BaseClass\u0026#34;) class Mixin1(object): def test(self): print (\u0026#34;Mixin1\u0026#34;) class Mixin2(object): def test(self): print( \u0026#34;Mixin2\u0026#34;)   Now this code\n1 2 3 4 5  class MyClass(BaseClass, Mixin1, Mixin2): pass MyClass().test()   prints\nBaseClass If you change the order of mixins:\n1 2 3 4  ass MyClass(Mixin2, Mixin1, BaseClass, ): pass MyClass().test()   you get\nMixin2 References  What is a mixin, and why are they useful? Mixins and Python  ","permalink":"https://serge-m.github.io/posts/mixins-in-python/","summary":"Design pattern Mixin is often used in python. There are two main situations where mixins are used 1:\n You want to provide a lot of optional features for a class. You want to use one particular feature in a lot of different classes.  Order of mixins definition Order in which you use mixins defines the behaviour. Quote from 2:\nin Python the class hierarchy is defined right to left, so in this case the Mixin2 class is the base class, extended by Mixin1 and finally by BaseClass.","title":"Mixin pattern in Python"},{"content":"There are many ways for showing slides beyond powerpoint.\nPowerpoint is not cross-platform solution. Google Presentations require internet connection.\nThere are solutions for making presentations with html and javascript:\n reveal.js \u0026ndash; more powerful, flexible formats, markdown support, plots a splugins remark many others : https://en.wikipedia.org/wiki/Web-based_slideshow  RevealJS example: [https://revealjs.com/#/]\nRemark Example Example of html code for 3-slides presentaiton from github of the project:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Title\u0026lt;/title\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;style\u0026gt; @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz); @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic); @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic); body { font-family: \u0026#39;Droid Serif\u0026#39;; } h1, h2, h3 { font-family: \u0026#39;Yanone Kaffeesatz\u0026#39;; font-weight: normal; } .remark-code, .remark-inline-code { font-family: \u0026#39;Ubuntu Mono\u0026#39;; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;textarea id=\u0026#34;source\u0026#34;\u0026gt; class: center, middle # Title --- # Agenda 1. Introduction 2. Deep-dive 3. ... --- # Introduction \u0026lt;/textarea\u0026gt; \u0026lt;script src=\u0026#34;https://remarkjs.com/downloads/remark-latest.min.js\u0026#34;\u0026gt; \u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var slideshow = remark.create(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   This is how the slide look like in browser:\nWell this example requires internet connection to download js and fonts, but you can put them in the same folder.\nExample presentations: sample, mocking in python\n","permalink":"https://serge-m.github.io/posts/presentations-in-browser/","summary":"There are many ways for showing slides beyond powerpoint.\nPowerpoint is not cross-platform solution. Google Presentations require internet connection.\nThere are solutions for making presentations with html and javascript:\n reveal.js \u0026ndash; more powerful, flexible formats, markdown support, plots a splugins remark many others : https://en.wikipedia.org/wiki/Web-based_slideshow  RevealJS example: [https://revealjs.com/#/]\nRemark Example Example of html code for 3-slides presentaiton from github of the project:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  \u0026lt;!","title":"Presentations in browser"}]