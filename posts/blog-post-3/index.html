<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>То, что вы хотели знать про оптический поток, но стеснялись спросить | sergem's personal public notebook</title><meta name=keywords content="opencv,russian,optical flow"><meta name=description content="Взято с http://habrahabr.ru/post/201406/
see also Deep Learning в вычислении оптического потока
Объяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.   Обработка изображений*,&nbsp;Алгоритмы*Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня)."><meta name=author content="SergeM"><link rel=canonical href=https://serge-m.github.io/posts/blog-post-3/><link href=/assets/css/stylesheet.min.6d98a2276d0cb41ef459267b3ff3ef02df70a8f16b70bbc52b20568702bc90cf.css integrity="sha256-bZiiJ20MtB70WSZ7P/PvAt9wqPFrcLvFKyBWhwK8kM8=" rel="preload stylesheet" as=style><link rel=icon href=https://serge-m.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://serge-m.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://serge-m.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://serge-m.github.io/apple-touch-icon.png><link rel=mask-icon href=https://serge-m.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.97.3"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-40853494-2","auto"),ga("send","pageview"))</script><meta property="og:title" content="То, что вы хотели знать про оптический поток, но стеснялись спросить"><meta property="og:description" content="Взято с http://habrahabr.ru/post/201406/
see also Deep Learning в вычислении оптического потока
Объяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.   Обработка изображений*,&nbsp;Алгоритмы*Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня)."><meta property="og:type" content="article"><meta property="og:url" content="https://serge-m.github.io/posts/blog-post-3/"><meta property="og:image" content="https://serge-m.github.io/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2013-11-12T11:17:00+00:00"><meta property="article:modified_time" content="2013-11-12T11:17:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://serge-m.github.io/papermod-cover.png"><meta name=twitter:title content="То, что вы хотели знать про оптический поток, но стеснялись спросить"><meta name=twitter:description content="Взято с http://habrahabr.ru/post/201406/
see also Deep Learning в вычислении оптического потока
Объяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.   Обработка изображений*,&nbsp;Алгоритмы*Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://serge-m.github.io/posts/"},{"@type":"ListItem","position":2,"name":"То, что вы хотели знать про оптический поток, но стеснялись спросить","item":"https://serge-m.github.io/posts/blog-post-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"То, что вы хотели знать про оптический поток, но стеснялись спросить","name":"То, что вы хотели знать про оптический поток, но стеснялись спросить","description":"Взято с http://habrahabr.ru/post/201406/\nsee also Deep Learning в вычислении оптического потока\nОбъяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.   Обработка изображений*,\u0026nbsp;Алгоритмы*Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня).","keywords":["opencv","russian","optical flow"],"articleBody":"Взято с http://habrahabr.ru/post/201406/\nsee also Deep Learning в вычислении оптического потока\nОбъяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.   Обработка изображений*, Алгоритмы*Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня). Тогда обратимся к готовым реализациям, например, в OpenCV. Там он реализован различными методами и совершенно непонятно, чем аббревиатура PyrLK лучше или хуже обозначения Farneback или чего-нибудь в этом роде, да и придется поразбираться со смыслом параметров, которых в некоторых реализациях очень много. Причем, что интересно, эти алгоритмы как-то работают, в отличие от того, что мы написали сами. В чем же секрет?  Что же такое оптический поток Оптический поток (ОП) – изображение видимого движения, представляющее собой сдвиг каждой точки между двумя изображениями. По сути, он представляет собой поле скоростей (т. к. сдвиг с точностью до масштаба эквивалентен мгновенной скорости). Суть ОП в том, что для каждой точки изображения ![](http://latex.codecogs.com/gif.latex?I_{1}(x,y)) находится такой сдвиг (dx, dy), чтобы исходной точке соответствовала точка на втором изображении ![](http://latex.codecogs.com/gif.latex?I_{2}(x\u0026plus;dx,y\u0026plus;dy)) . Как определить соответствие точек – отдельный вопрос. Для этого надо взять какую-то функцию точки, которая не изменяется в результате смещения. Обычно считается, что у точки сохраняется интенсивность (т. е. яркость или цвет для цветных изображений), но можно считать одинаковыми точки, у которых сохраняется величина градиента, гессиан, его величина или его определитель, лапласиан, другие характеристики. Очевидно, сохранение интенсивности дает сбои, если меняется освещенность или угол падения света. Тем не менее, если речь идет о видеопотоке, то, скорее всего, между двумя кадрами освещение сильно не изменится, хотя бы потому, что между ними проходит малый промежуток времени. Поэтому часто используют интенсивность в качестве функции, сохраняющейся у точки. По такому описанию можно перепутать ОП с поиском и сопоставлением характерных точек. Но это разные вещи, суть оптического потока в том, что он не ищет какие-то особенные точки, а по параметрам изображений пытается определить, куда сместилась произвольная точка.\nЕсть два варианта расчета оптического потока: плотный (dense) и выборочный (sparse). Sparse поток рассчитывает сдвиг отдельных заданных точек (например, точек, выделенных некоторым feature detector’ом), dense поток считает сдвиг всех точек изображения. Естественно, выборочный поток вычисляется быстрее, однако для некоторых алгоритмов разница не такая уж и большая, а для некоторых задач требуется нахождение потока во всех точках изображения.\nДля вырожденных случаев можно применять более простые методы определения сдвига. В частности, если все точки изображения имеют один и тот же сдвиг (изображение сдвинуто целиком), то можно применить метод фазовой корреляции: вычислить преобразование Фурье для обоих изображений, найти свертку их фаз и по ней определить сдвиг (см.[en.wikipedia.org/wiki/Phase_correlation](http://en.wikipedia.org/wiki/Phase_correlation\" style=“border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)). Также можно применять поблочное сравнение (block matching): находить сдвиг, минимизирующий норму разности изображений в окне. В чистом виде такой алгоритм будет работать долго и неустойчиво к поворотам и прочим искажениям. [Английская википедия](http://en.wikipedia.org/wiki/Optical_flow\" style=“border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) причисляет перечисленные алгоритмы к различным вариантам вычисления оптического потока, но мне это кажется не слишком корректным, так как эти алгоритмы могут быть применены и в других целях и не полностью решают данную задачу. Мы будем называть оптическим потоком методы, основанные на локальных характеристиках изображений (то, что в английской википедии называется differential methods).\nСтандартный подход (метод Лукаса-Канаде) Математическое описание алгоритма достаточно подробно приведено в [этой статье](http://habrahabr.ru/post/169055/\" style=\"border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), но в ней затрагиваются лишь теоретические аспекты. Рассмотрим математическую модель оптического потока, считая, что у точки в результате смещения не изменилась интенсивность.\nПусть – интенсивность в некоторой точке (x, y) на первом изображении (т. е. в момент времени t). На втором изображении эта точка сдвинулась на (dx, dy), при этом прошло время dt, тогда – это мы разложили по Тейлору функцию интенсивности до первого члена (позже будет упомянуто, почему только до первого), здесь – частные производные по координатам и времени, то есть по сути – изменение яркости в точке (x, y) между двумя кадрами.\nМы считаем, что у точки сохранилась интенсивность, значит Получаем одно уравнение с двумя неизвестными (dx и dy), значит его недостаточно для решения, то есть только на этом уравнении далеко не уедешь.\nСамое простое решение проблемы – алгоритм Лукаса-Канаде. У нас же на изображении объекты размером больше 1 пикселя, значит, скорее всего, в окрестности текущей точки у других точек будут примерно такие же сдвиги. Поэтому мы возьмем окно вокруг этой точки и минимизируем (по МНК) в нем суммарную погрешность с весовыми коэффициентами, распределенными по Гауссу, то есть так, чтобы наибольший вес имели пиксели, ближе всего находящиеся к исследуемому. После простейших преобразований, получаем уже систему из 2 уравнений с 2 неизвестными: Как известно, эта система имеет единственное решение не всегда (хотя и очень часто): если детерминант системы равен нулю, то решений либо нет, либо бесконечное число. Эта проблема известна как Aperture problem – неоднозначность сдвига при ограниченном поле зрения для периодических картинок. Она соответствует случаю, когда в поле зрения попадает фрагмент изображения, в котором присутствует некоторая цикличность; тут уж и человек не сможет однозначно определить, куда картинка сместилась. Проблема в том, что из-за шумов в таких неоднозначных ситуациях мы получим не нулевой детерминант, а очень маленький, который, скорее всего, приведет к очень большим значениям сдвига, особо не коррелирующим с действительностью. Так что на определенном этапе нужно просто проверять, не является ли детерминант системы достаточно маленьким, и, если что, не рассматривать такие точки или отмечать их как ошибочные.\nПочему не работает? Если мы остановимся на этом этапе и реализуем этот алгоритм, то он будет успешно работать. Но только если сдвиг между соседними изображениями будет очень маленький, порядка 1 пикселя, и то не всегда. (Для анализа качества генерировались синтетические последовательности с различным относительным сдвигом, причем этот сдвиг может выражаться нецелым числом пикселей, тогда результирующее изображение соответствующим образом интерполируется) Уже на сдвиге в 2 пикселя погрешность будет большая, а если 3 и более, то результат будет вообще неадекватным. В чем же дело? Тут нам устроила подставу математика. Она привила нам ощущение, что все функции вокруг непрерывные и много раз дифференцируемые. И вообще нас в институте приучили приближение функции в окрестности точки записывать с помощью формулы Тейлора, и мы везде бездумно радостно пользуемся этим. А теперь задумаемся, какой физический смысл производных в данном месте? Мы хотим с их помощью определить изменение значения функции в конечной окрестности точки, а производная дает представление о бесконечно малой окрестности. Для расширения этой окрестности можно было бы добавить более высокий порядок производных в разложение Тейлора, но это приведет к нелинейностям в системе, от чего ее станет существенно сложнее решать, а преимущества будут сомнительны, тем более что на практике мы имеем дело не с непрерывными многократно дифференцируемыми функциями, а с вообще непонятно какими дискретными функциями. Поэтому логичнее будет искать функцию g(x), для которой в нашем дискретном случае как можно точнее выполняется f(x) + g(x) = f(x+1), f(x) + 2g(x) = f(x+2), f(x) — g(x) = f(x-1), и т. д. Таким образом, нам в этом случае нужна не производная, а некоторая линейная функция, наиболее близко лежащая к точкам исходной функции. Простые математические выкладки приводят к решению , где . Если мы строили производную по одной соседней точке с каждой стороны, то нам повезло: в этом случае формула совпадает с формулой приближенного вычисления производных: g(x) = (f(x+1) – f(x-1)) / 2. Что характерно, в OpenCV при вычислении оптического потока Лукаса-Канаде используется именно такая формула, к этому мы еще вернемся потом. А вот если взять больше точек, то формула уже становится совсем не похожа на классические разностные схемы для первой производной.\nОчевидно, если мы строим эту функцию, например, по трем окрестным точкам слева и справа от исходной, то она никаким образом не зависит от точек, расположенных дальше, и, соответственно, при сдвиге более трех точек все равно у нас часто будут получаться неадекватные результаты. А еще, чем больше число точек, по которым мы строим эту функцию, тем больше среднее отклонение получаемой линии от используемых точек – опять же из-за того, что у нас не линейно меняющиеся изображения, а черт знает какие. На практике сдвиги более 2 пикселей уже дают неадекватно большую ошибку, сколько бы точек мы ни взяли.\nДругим слабым местом алгоритма является то, что мы опять же имеем дело не с гладкими непрерывными функциями, а с произвольными, да еще и дискретными. Поэтому на некоторых фрагментах изображения интенсивность может «скакать» вообще без явных закономерностей, например на границах объектов, или из-за шумов. В этом случае никакая функция g(x) не сможет достаточно точно описать изменения изображения в окрестности точки. Чтобы с этим побороться (хотя бы частично), предлагается исходное изображение размазать, причем полезно будет его размазать достаточно сильно, то есть лучше применять даже не всеми любимый gaussian blur (усреднение с весовыми коэффициентами), а прямо таки box filter (равномерное усреднение по окну), да еще и несколько раз подряд. Гладкость изображения для нас сейчас важнее, чем детализация.\nТем не менее, эти меры так же не спасут нас от ограничения детектируемого сдвига в 2-3 пикселя. И кстати, в OpenCV 1.0 присутствовала такая реализация оптического потока, и работала она только в идеальных условиях на очень маленьких сдвигах.\nЧто же делать? Итого, обычный Лукас-Канаде хорошо определяет маленькие сдвиги, такие, в рамках которых картинка похожа на свое линейное приближение. Чтобы с этим побороться, воспользуемся стандартным приемом CV – multi-scaling'ом: построим «пирамиду» изображений разного масштаба (почти всегда берется масштабирование в 2 раза по каждой оси, так проще считать) и пройдем по ним оптическим потоком от меньшего изображения к большему, тогда детектированный маленький сдвиг на маленьком изображении будет соответствовать большому сдвигу на большом изображении. На самом маленьком изображении мы обнаруживаем сдвиг не более 1-2 пикселей, а переходя от меньшего масштаба к большему, мы пользуемся результатом с предыдущего шага и уточняем значения сдвига. Собственно, в OpenCV его и реализует функция calcOptFlowPyrLK. Использование этого пирамидального алгоритма позволяет нам не заморачиваться вычислением линейной аппроксимации по многим точкам: проще взять больше уровней пирамиды, а на каждом уровне брать довольно грубое приближение этой функции. Поэтому в OpenCV и идет расчет всего по двум соседним точкам. И поэтому применительно к этой реализации алгоритма наши умозаключения про преимущество аппроксимирующей функции перед производной оказались бесполезными: для такого количества опорных точек производная и есть лучшая аппроксимирующая функция. А какие еще бывают? Этот алгоритм не является единственным вариантом вычисления оптического потока. В OpenCV кроме потока Лукаса-Канаде есть еще поток Farneback и SimpleFlow, также часто ссылаются на алгоритм Horn–Schunck. Метод Horn–Schunck носит несколько более глобальный характер, чем метод Лукаса-Канаде. Он опирается на предположение о том, что на всем изображении оптический поток будет достаточно гладким. От того же самого уравнения предлагается перейти к функционалу , то есть добавить требование на отсутствие резкого изменения сдвигов с весовым коэффициентом α. Минимизация этого функционала приводит нас к системе из двух уравнений: В этих уравнениях лапласиан предлагают посчитать приближенно: – разница со средним значением. Получаем систему уравнений, которую записываем для каждого пикселя и решаем общую систему итеративно: В данном алгоритме тоже предлагают использовать multi-scaling, причем рекомендуют масштабировать изображения не в 2 раза, а с коэффициентом 0.65\nЭтот алгоритм был реализован в первых версиях OpenCV, но в последствии от него отказались.\nFarneback предложил аппроксимировать изменение интенсивности в окрестности с помощью квадратичной формы: I = xAx + bx + c с симметричной матрицей A (по сути, рассматривая разложение по Тейлору до первого члена, мы брали линейную аппроксимацию I = bx + c, то есть сейчас мы как раз решили повысить точность приближения) Если изображение сдвинулось в пределах этой окрестности, то , подставляем в квадратичное разложение, раскрываем скобки, получаем .\nТеперь мы можем вычислить значения A, b, c на обеих картинках, и тогда эта система станет избыточной относительно d (особенно смущает первое уравнение), и вообще d можно получить из второго уравнения: . Приходится прибегать к следующей аппроксимации: . Обозначим еще для простоты , Тогда получим просто .\nДля компенсации шумов при вычислении, снова обратимся к тому предположению, что в окрестности исследуемой точки у всех точек более или менее одинаковый сдвиг. Поэтому опять же проинтегрируем погрешностьпо окну с гауссовскими весовыми коэффициентами w, и найдем вектор d, минимизирующий эту суммарную погрешность. Тогда мы получим оптимальное значение и соответствующую минимальную ошибку . То есть нам надо для каждой точки посчитать , усреднить по окну, инвертировать матрицу и получить результат. Соответственно эти произведения можно посчитать для всей картинки и использовать заранее рассчитанные значения для разных точек, то есть это как раз тот случай, когда имеет смысл считать dense поток.\nКак обычно, у этого алгоритма есть некоторое количество модификаций и усовершенствований, в первую очередь позволяющих использовать известную априорную информацию – заданную начальную аппроксимацию потока – и, опять же, multi-scaling.\nВ основе метода SimpleFlow лежит следующая идея: если мы все равно не умеем определять сдвиг больше чем размер окна, по которому мы искали производные, то зачем вообще заморачиваться с вычислением производных? Давайте просто в окне найдем наиболее похожую точку! А для разрешения неоднозначностей и для компенсации шумов учтем, что поток непрерывный и в окрестности данной точки все точки имеют почти одинаковый сдвиг. А проблему с размером окна опять же решим за счет multi-scaling’а.\nБолее строго, алгоритм звучит так: для всех точек в окне находится функция «энергии», отвечающая (с обратной логарифмической зависимостью) за вероятность перехода исходной точки в эту точку: . Далее, считается свертка этой энергии с гауссовым окном и находятся значения (dx,dy), минимизирующие эту функцию. Чтобы получить субпиксельную точность, рассматривается малая окрестность найденной оптимальной точки (dx,dy) и в ней ищется пик функции энергии как пик параболоида. И, как было упомянуто выше, эта процедура выполняется для пирамиды масштабированных изображений. Еще у них в алгоритме предложены хитрые методы ускорения расчетов, но это уже кому интересно разберутся сами. Для нас же важно, что за счет этого данный алгоритм является (теоретически) достаточно быстрым при неплохой точности. И у него нет такой проблемы, как у предыдущих, что чем больше сдвиг, тем хуже он детектируется.\nА если брать не интенсивность? Выше было сказано, что соответствие между точками может определяться разными величинами, так почему же мы рассматриваем только интенсивность? А потому, что любую другую величину можно свести к ней: мы просто фильтруем изображения соответствующим фильтром и на вход описанных выше алгоритмов подаем отфильтрованные изображения. Соответственно, если вы хотите использовать оптический поток, то сначала подумайте, в ваших условиях какая характеристика изображения будет наиболее стабильной, и проведите соответствующую фильтрацию, чтобы на входе алгоритма оказалась не интенсивность, а эта характеристика. Практика Давайте опробуем на практике алгоритмы, которые нам предлагает OpenCV. Здесь можно проводить множество различных исследований каждого алгоритма, варьируя параметры, изменяя входные последовательности – с разными сдвигами, поворотами, проективными преобразованиями, сегментами, с разными шумами и т. д. Это все заняло бы уйму времени и по размеру отчета превзошло бы настоящую статью, поэтому здесь предлагаю ограничиться простым случаем параллельного сдвига изображения на фиксированное расстояние и наложение небольших шумов. Это позволит понять в общих чертах, как запускать алгоритмы и кто из них круче.\nПодробно синтаксис процедур описан на странице с [мануалом](http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html\" style=“border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), здесь я приведу выжимку-перевод с моими комментариями.\nКлассический Лукас-Канаде реализован с пирамидой в процедуре calcOpticalFlowPyrLK. Алгоритм рассчитывает sparse-поток, то есть для заданного набора точек на первом изображении оценивает их положение на втором. Входные параметры достаточно очевидны: два входных изображения, входной и выходной наборы точек, status – выходной вектор, показывающий, найдена ли успешно соответствующая точка, err – выходной вектор оцененных погрешностей соответствующих точек, WinSize – размер окна, по которому происходит гауссово усреднение, я брал 21х21 и работало хорошо, maxLevel – количество слоев в пирамиде минус один, т. е. номер последнего слоя, я брал 5, criteria – условие выхода из итеративного процесса определения сдвига (минимизация погрешности производится итеративно) – этот параметр я оставлял по умолчанию, flags – дополнительные флаги, например можно использовать начальное приближение потока или выбрать метод оценки погрешности, minEigThreshold – пороговое значение градиента, ниже которого матрица считается вырожденной, я оставлял по умолчанию. Начиная с OpenCV 2.4.1, при вычислении потока можно использовать заранее вычисленную пирамиду отмасштабированных изображений.\nРезультат работы – успешно и стабильно обнаруживаются как малые, так и большие сдвиги, устойчив к довольно большим шумам, время работы – порядка 10 мс для 400 точек c 5-слойной пирамидой (на core i7 950).\nКстати, этот алгоритм реализован так же на Gpu (CUDA), причем как dense, так и sparse версии.\nПоток Farneback реализуется процедурой calcOpticalFlowFarneback, рассчитывается dense-поток, то есть сдвиг каждой точки. Параметры: входные изображения, выходной поток в формате двухканальной матрицы float’ов, pyr_scale определяет отношение масштабов между слоями пирамиды, levels – количество уровней в пирамиде, winsize – размер окна, по которому производится усреднение, iterations – количество итераций на каждом уровне, poly_n – размер полинома, по которому оцениваются значения A и b, poly_sigma – сигма гауссовского размытия при сглаживании производных, рекомендованные значения параметров указаны в мануале, flags – дополнительные флаги, например можно использовать начальное приближение потока или по-другому усреднять по окну.\nЭтот алгоритм куда менее стабилен (по моим наблюдениям), легче промахивается на довольно равномерных картинках (видимо, проблема в отсутствии фильтрации неудачных точек), плохо определяет большие сдвиги. У меня отрабатывал за 600 мс на изображении 512х512.\nПоток SimpleFlow реализует процедура calcOpticalFlowSF (рассчитывается опять же dense поток), и у нее есть множество загадочных параметров без дефолтных значений, и вообще на данный момент на странице информация предоставлена весьма лаконично. Попробуем разобраться. Первые 3 – входные изображения и выходное двухканальное; layers – количество слоев в пирамиде, то есть сколько раз масштабируем исходное изображение; averaging_block_size – размер окна, в котором мы считали функцию энергии пикселей; max_flow – максимальный сдвиг, который мы хотим уметь определять на каждом шаге, по сути он определяется размером окна (хотя не совсем понятно, почему он int). На этом можно остановиться, а можно задать еще несколько параметров, смысл некоторых из них от меня ускользает.\nНа сайте предлагают посмотреть [пример](https://github.com/Itseez/opencv/blob/master/samples/cpp/simpleflow_demo.cpp\" style=“border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) его использования, в котором он запускается с такими параметрами: calcOpticalFlowSF(frame1, frame2, flow, 3, 2, 4, 4.1, 25.5, 18, 55.0, 25.5, 0.35, 18, 55.0, 25.5, 10);\nУ меня алгоритм работает значительно медленнее других, порядка 9-12 секунд на картинку 512х512. Результат работы кажется более правдоподобным, чем Farneback, по крайней мере лучше определяется сдвиг на равномерных картинках, заметно лучше срабатывает с большими сдвигами.\nВыводы Если вы хотите использовать где-то оптический поток, сначала подумайте, нужен ли он вам: часто можно обойтись более простыми методами. Браться реализовывать поток самостоятельно стоит только несколько раз подумав: каждый алгоритм имеет множество хитростей, тонкостей и оптимизаций; что бы вы ни сделали, скорее всего, в OpenCV оно же работает лучше (естественно, при условии, что оно там есть). Тем более что они там вовсю используют логические и хардварные оптимизации типа использования SSE инструкций, многопоточность, возможности вычисления с CUDA или OpenCL и т. д. Если вам достаточно посчитать сдвиг некоторого набора точек (т. е. sparse поток), то можете смело использовать функцию calcOpticalFlowPyrLK, оно работает хорошо, надежно и достаточно быстро. Для вычисления dense-потока хорошо использовать функцию calcOpticalFlowSF, но она работает очень медленно. Если быстродействие критично, то calcOpticalFlowFarneback, но надо еще удостовериться, что результаты его работы вас устроят. Литература [docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html\" style=\"border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) Pyramidal Implementation of the Lucas Kanade Feature Tracker. Description of the algorithm — Jean-Yves Bouguet Two-Frame Motion Estimation Based on Polynomial Expansion — Gunnar Farneback SimpleFlow: A Non-iterative, Sublinear Optical Flow Algorithm — Michael Tao, Jiamin Bai, Pushmeet Kohli, and Sylvain Paris Horn-Schunck Optical Flow with a Multi-Scale Strategy — Enric Meinhardt-Llopis, Javier Sanchez [en.wikipedia.org/wiki/Optical_flow](http://en.wikipedia.org/wiki/Optical_flow\" style=\"border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) [оптический поток](http://habrahabr.ru/search/?q=%5B%D0%BE%D0%BF%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9%20%D0%BF%D0%BE%D1%82%D0%BE%D0%BA%5D\u0026target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), [optical flow](http://habrahabr.ru/search/?q=%5Boptical%20flow%5D\u0026target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), [computer vision](http://habrahabr.ru/search/?q=%5Bcomputer%20vision%5D\u0026target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), [openCV](http://habrahabr.ru/search/?q=%5BopenCV%5D\u0026target_type=posts\" rel=\"tag\" style=\"border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) ","wordCount":"3244","inLanguage":"en","datePublished":"2013-11-12T11:17:00Z","dateModified":"2013-11-12T11:17:00Z","author":{"@type":"Person","name":"SergeM"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://serge-m.github.io/posts/blog-post-3/"},"publisher":{"@type":"Organization","name":"sergem's personal public notebook","logo":{"@type":"ImageObject","url":"https://serge-m.github.io/favicon.ico"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://serge-m.github.io/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://serge-m.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://serge-m.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://serge-m.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://serge-m.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://serge-m.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://serge-m.github.io/posts/>Posts</a></div><h1 class=post-title>То, что вы хотели знать про оптический поток, но стеснялись спросить</h1><div class=post-meta>November 12, 2013&nbsp;·&nbsp;SergeM</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=# aria-label="Что же такое оптический поток">Что же такое оптический поток</a></li><li><a href=# aria-label="Стандартный подход (метод Лукаса-Канаде)">Стандартный подход (метод Лукаса-Канаде)</a><ul><li><a href=# aria-label="Почему не работает?">Почему не работает?</a></li><li><a href=# aria-label="Что же делать?">Что же делать?</a></li></ul></li><li><a href=# aria-label="А какие еще бывают?">А какие еще бывают?</a></li><li><a href=# aria-label="А если брать не интенсивность?">А если брать не интенсивность?</a></li><li><a href=# aria-label=Практика>Практика</a></li><li><a href=# aria-label=Выводы>Выводы</a></li><li><a href=# aria-label=Литература>Литература</a></li></ul></div></details></div><div class=post-content><p>Взято с <a href=http://habrahabr.ru/post/201406/>http://habrahabr.ru/post/201406/</a></p><p>see also <a href=https://habr.com/ru/company/ods/blog/446726/>Deep Learning в вычислении оптического потока</a></p><div>Объяснение оптического потока из OpenCV для тех, кто не в теме и не очень хочет разобраться.<div></div><div><div class=hubs style="background-color:#fff;background-image:url(http://habrahabr.ru/images/posts/hub.icon.png);background-position:0 0;background-repeat:no-repeat;border:0;color:#999;font-family:Verdana,sans-serif;font-size:11px;margin:0 0 15px;outline:0;padding:2px 0 2px 25px;vertical-align:baseline"><a class=hub href=http://habrahabr.ru/hub/image_processing/ style=border:0;color:#999;margin:0;outline:0;padding:0;vertical-align:baseline title="Вы не подписаны на этот хаб">Обработка изображений</a><span class=profiled_hub style=border:0;cursor:help;margin:0;outline:0;padding:0;vertical-align:baseline title="Профильный хаб">*</span>,&nbsp;<a class=hub href=http://habrahabr.ru/hub/algorithms/ style=border:0;color:#999;margin:0;outline:0;padding:0;vertical-align:baseline title="Вы не подписаны на этот хаб">Алгоритмы</a><span class=profiled_hub style=border:0;cursor:help;margin:0;outline:0;padding:0;vertical-align:baseline title="Профильный хаб">*</span></div><div class="content html_format" style="background-color:#fff;border:0;font-family:Verdana,sans-serif;font-size:13px;line-height:20px;margin:0 0 10px;outline:0;overflow:hidden;padding:0;vertical-align:baseline"><img align=right src=http://habr.habrastorage.org/post_images/6e7/202/488/6e7202488554327be02a3b211df4647a.jpg style="border:0;margin:5px 0 5px 30px;max-width:100%;outline:0;padding:0;vertical-align:middle">
Оптический поток (Optical flow) – технология, использующаяся в различных областях computer vision для определения сдвигов, сегментации, выделения объектов, компрессии видео. Однако если мы захотим его по-быстрому реализовать в своем проекте, прочитав про него на википедии или где-нибудь еще, то, скорее всего, очень быстро наткнемся на то, что он работает очень плохо и сбоит при определении сдвигов уже порядка 1-2 пикселей (по крайней мере так было у меня). Тогда обратимся к готовым реализациям, например, в OpenCV. Там он реализован различными методами и совершенно непонятно, чем аббревиатура PyrLK лучше или хуже обозначения Farneback или чего-нибудь в этом роде, да и придется поразбираться со смыслом параметров, которых в некоторых реализациях очень много. Причем, что интересно, эти алгоритмы как-то работают, в отличие от того, что мы написали сами. В чем же секрет?
<a href="https://www.blogger.com/blogger.g?blogID=636453477220885924" name=habracut style=border:0;color:#6da3bd;margin:0;outline:0;padding:0;vertical-align:baseline></a><h4 style=border:0;color:#999;font-size:16px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>Что же такое оптический поток</h4>Оптический поток (ОП) – изображение видимого движения, представляющее собой сдвиг каждой точки между двумя изображениями. По сути, он представляет собой поле скоростей (т. к. сдвиг с точностью до масштаба эквивалентен мгновенной скорости). Суть ОП в том, что для каждой точки изображения&nbsp;
![](http://latex.codecogs.com/gif.latex?I_{1}(x,y))
&nbsp;находится такой сдвиг (dx, dy), чтобы исходной точке соответствовала точка на втором изображении&nbsp;
![](http://latex.codecogs.com/gif.latex?I_{2}(x&amp;plus;dx,y&amp;plus;dy))
. Как определить соответствие точек – отдельный вопрос. Для этого надо взять какую-то функцию точки, которая не изменяется в результате смещения. Обычно считается, что у точки сохраняется интенсивность (т. е. яркость или цвет для цветных изображений), но можно считать одинаковыми точки, у которых сохраняется величина градиента, гессиан, его величина или его определитель, лапласиан, другие характеристики. Очевидно, сохранение интенсивности дает сбои, если меняется освещенность или угол падения света. Тем не менее, если речь идет о видеопотоке, то, скорее всего, между двумя кадрами освещение сильно не изменится, хотя бы потому, что между ними проходит малый промежуток времени. Поэтому часто используют интенсивность в качестве функции, сохраняющейся у точки.<p>По такому описанию можно перепутать ОП с поиском и сопоставлением характерных точек. Но это разные вещи, суть оптического потока в том, что он не ищет какие-то особенные точки, а по параметрам изображений пытается определить, куда сместилась произвольная точка.</p><p>Есть два варианта расчета оптического потока: плотный (dense) и выборочный (sparse). Sparse поток рассчитывает сдвиг отдельных заданных точек (например, точек, выделенных некоторым feature detector&rsquo;ом), dense поток считает сдвиг всех точек изображения. Естественно, выборочный поток вычисляется быстрее, однако для некоторых алгоритмов разница не такая уж и большая, а для некоторых задач требуется нахождение потока во всех точках изображения.</p><p>Для вырожденных случаев можно применять более простые методы определения сдвига. В частности, если все точки изображения имеют один и тот же сдвиг (изображение сдвинуто целиком), то можно применить метод фазовой корреляции: вычислить преобразование Фурье для обоих изображений, найти свертку их фаз и по ней определить сдвиг (см.[en.wikipedia.org/wiki/Phase_correlation](<a href=http://en.wikipedia.org/wiki/Phase_correlation%22>http://en.wikipedia.org/wiki/Phase_correlation"</a> style=&ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)). Также можно применять поблочное сравнение (block matching): находить сдвиг, минимизирующий норму разности изображений в окне. В чистом виде такой алгоритм будет работать долго и неустойчиво к поворотам и прочим искажениям. [Английская википедия](<a href=http://en.wikipedia.org/wiki/Optical_flow%22>http://en.wikipedia.org/wiki/Optical_flow"</a> style=&ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) причисляет перечисленные алгоритмы к различным вариантам вычисления оптического потока, но мне это кажется не слишком корректным, так как эти алгоритмы могут быть применены и в других целях и не полностью решают данную задачу. Мы будем называть оптическим потоком методы, основанные на локальных характеристиках изображений (то, что в английской википедии называется differential methods).</p><h4 style=border:0;color:#999;font-size:16px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>Стандартный подход (метод Лукаса-Канаде)</h4>Математическое описание алгоритма достаточно подробно приведено в&nbsp;[этой статье](http://habrahabr.ru/post/169055/" style="border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), но в ней затрагиваются лишь теоретические аспекты.<p>Рассмотрим математическую модель оптического потока, считая, что у точки в результате смещения не изменилась интенсивность.</p><p>Пусть <img src=http://habr.habrastorage.org/post_images/fd3/055/cea/fd3055ceaa3ac999993187efe591cc42.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle> – интенсивность в некоторой точке (x, y) на первом изображении (т. е. в момент времени t). На втором изображении эта точка сдвинулась на (dx, dy), при этом прошло время dt, тогда <img src=http://habr.habrastorage.org/post_images/d80/ad7/750/d80ad77501aec2e45d1b199cabe7db55.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle> – это мы разложили по Тейлору функцию интенсивности до первого члена (позже будет упомянуто, почему только до первого), здесь 
<img loading=lazy src=http://latex.codecogs.com/gif.latex?I_%7bx%7d,I_%7by%7d,I_%7bt%7d alt>
 – частные производные по координатам и времени, то есть по сути 
<img loading=lazy src=http://latex.codecogs.com/gif.latex?I_%7bt%7ddt alt>
 – изменение яркости в точке (x, y) между двумя кадрами.</p><p>Мы считаем, что у точки сохранилась интенсивность, значит 
<img loading=lazy src="http://latex.codecogs.com/gif.latex?I_%7b1%7d=I_%7b2%7d%5cRightarrow" alt>
 <img src=http://habr.habrastorage.org/post_images/240/a53/130/240a531308dcc6d61a797f29f0d09b7d.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>
Получаем одно уравнение с двумя неизвестными (dx и dy), значит его недостаточно для решения, то есть только на этом уравнении далеко не уедешь.</p><p>Самое простое решение проблемы – алгоритм Лукаса-Канаде. У нас же на изображении объекты размером больше 1 пикселя, значит, скорее всего, в окрестности текущей точки у других точек будут примерно такие же сдвиги. Поэтому мы возьмем окно вокруг этой точки и минимизируем (по МНК) в нем суммарную погрешность с весовыми коэффициентами, распределенными по Гауссу, то есть так, чтобы наибольший вес имели пиксели, ближе всего находящиеся к исследуемому. После простейших преобразований, получаем уже систему из 2 уравнений с 2 неизвестными: <img src=http://habr.habrastorage.org/post_images/0ce/761/7f0/0ce7617f00672f62bc3b8819af66f72d.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>
<img src=http://habr.habrastorage.org/post_images/f56/644/5aa/f566445aa7c1524c856f73396a28dea5.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>
Как известно, эта система имеет единственное решение не всегда (хотя и очень часто): если детерминант системы равен нулю, то решений либо нет, либо бесконечное число. Эта проблема известна как Aperture problem – неоднозначность сдвига при ограниченном поле зрения для периодических картинок. Она соответствует случаю, когда в поле зрения попадает фрагмент изображения, в котором присутствует некоторая цикличность; тут уж и человек не сможет однозначно определить, куда картинка сместилась. Проблема в том, что из-за шумов в таких неоднозначных ситуациях мы получим не нулевой детерминант, а очень маленький, который, скорее всего, приведет к очень большим значениям сдвига, особо не коррелирующим с действительностью. Так что на определенном этапе нужно просто проверять, не является ли детерминант системы достаточно маленьким, и, если что, не рассматривать такие точки или отмечать их как ошибочные.</p><h5 style=border:0;color:#999;font-size:14px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>Почему не работает?</h5>Если мы остановимся на этом этапе и реализуем этот алгоритм, то он будет успешно работать. Но только если сдвиг между соседними изображениями будет очень маленький, порядка 1 пикселя, и то не всегда. (Для анализа качества генерировались синтетические последовательности с различным относительным сдвигом, причем этот сдвиг может выражаться нецелым числом пикселей, тогда результирующее изображение соответствующим образом интерполируется) Уже на сдвиге в 2 пикселя погрешность будет большая, а если 3 и более, то результат будет вообще неадекватным. В чем же дело?<p>Тут нам устроила подставу математика. Она привила нам ощущение, что все функции вокруг непрерывные и много раз дифференцируемые. И вообще нас в институте приучили приближение функции в окрестности точки записывать с помощью формулы Тейлора, и мы везде <s style=border:0;margin:0;outline:0;padding:0;vertical-align:baseline>бездумно</s> радостно пользуемся этим. А теперь задумаемся, какой физический смысл производных в данном месте? Мы хотим с их помощью определить изменение значения функции в конечной окрестности точки, а производная дает представление о бесконечно малой окрестности. Для расширения этой окрестности можно было бы добавить более высокий порядок производных в разложение Тейлора, но это приведет к нелинейностям в системе, от чего ее станет существенно сложнее решать, а преимущества будут сомнительны, тем более что на практике мы имеем дело не с непрерывными многократно дифференцируемыми функциями, а с вообще непонятно какими дискретными функциями. Поэтому логичнее будет искать функцию g(x), для которой в нашем дискретном случае как можно точнее выполняется f(x) + g(x) = f(x+1), f(x) + 2g(x) = f(x+2), f(x) — g(x) = f(x-1), и т. д. Таким образом, нам в этом случае нужна не производная, а некоторая линейная функция, наиболее близко лежащая к точкам исходной функции. Простые математические выкладки приводят к решению <img src=http://habr.habrastorage.org/post_images/873/fc4/f2e/873fc4f2ef7f7bc8cc586734f2e1eeb6.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>, где <img src=http://habr.habrastorage.org/post_images/5e3/400/fb2/5e3400fb2e1b01aad1ad4c61fc1f286a.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>. Если мы строили производную по одной соседней точке с каждой стороны, то нам повезло: в этом случае формула совпадает с формулой приближенного вычисления производных: g(x) = (f(x+1) – f(x-1)) / 2. Что характерно, в OpenCV при вычислении оптического потока Лукаса-Канаде используется именно такая формула, к этому мы еще вернемся потом. А вот если взять больше точек, то формула уже становится совсем не похожа на классические разностные схемы для первой производной.</p><p>Очевидно, если мы строим эту функцию, например, по трем окрестным точкам слева и справа от исходной, то она никаким образом не зависит от точек, расположенных дальше, и, соответственно, при сдвиге более трех точек все равно у нас часто будут получаться неадекватные результаты. А еще, чем больше число точек, по которым мы строим эту функцию, тем больше среднее отклонение получаемой линии от используемых точек – опять же из-за того, что у нас не линейно меняющиеся изображения, а черт знает какие. На практике сдвиги более 2 пикселей уже дают неадекватно большую ошибку, сколько бы точек мы ни взяли.</p><p>Другим слабым местом алгоритма является то, что мы опять же имеем дело не с гладкими непрерывными функциями, а с произвольными, да еще и дискретными. Поэтому на некоторых фрагментах изображения интенсивность может «скакать» вообще без явных закономерностей, например на границах объектов, или из-за шумов. В этом случае никакая функция g(x) не сможет достаточно точно описать изменения изображения в окрестности точки. Чтобы с этим побороться (хотя бы частично), предлагается исходное изображение размазать, причем полезно будет его размазать достаточно сильно, то есть лучше применять даже не всеми любимый gaussian blur (усреднение с весовыми коэффициентами), а прямо таки box filter (равномерное усреднение по окну), да еще и несколько раз подряд. Гладкость изображения для нас сейчас важнее, чем детализация.</p><p>Тем не менее, эти меры так же не спасут нас от ограничения детектируемого сдвига в 2-3 пикселя. И кстати, в OpenCV 1.0 присутствовала такая реализация оптического потока, и работала она только в идеальных условиях на очень маленьких сдвигах.</p><h5 style=border:0;color:#999;font-size:14px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>Что же делать?</h5>Итого, обычный Лукас-Канаде хорошо определяет маленькие сдвиги, такие, в рамках которых картинка похожа на свое линейное приближение. Чтобы с этим побороться, воспользуемся стандартным приемом CV – multi-scaling'ом: построим «пирамиду» изображений разного масштаба (почти всегда берется масштабирование в 2 раза по каждой оси, так проще считать) и пройдем по ним оптическим потоком от меньшего изображения к большему, тогда детектированный маленький сдвиг на маленьком изображении будет соответствовать большому сдвигу на большом изображении. На самом маленьком изображении мы обнаруживаем сдвиг не более 1-2 пикселей, а переходя от меньшего масштаба к большему, мы пользуемся результатом с предыдущего шага и уточняем значения сдвига. Собственно, в OpenCV его и реализует функция calcOptFlowPyrLK. Использование этого пирамидального алгоритма позволяет нам не заморачиваться вычислением линейной аппроксимации по многим точкам: проще взять больше уровней пирамиды, а на каждом уровне брать довольно грубое приближение этой функции. Поэтому в OpenCV и идет расчет всего по двум соседним точкам. И поэтому применительно к этой реализации алгоритма наши умозаключения про преимущество аппроксимирующей функции перед производной оказались бесполезными: для такого количества опорных точек производная и есть лучшая аппроксимирующая функция.<h4 style=border:0;color:#999;font-size:16px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>А какие еще бывают?</h4>Этот алгоритм не является единственным вариантом вычисления оптического потока. В OpenCV кроме потока Лукаса-Канаде есть еще поток Farneback и SimpleFlow, также часто ссылаются на алгоритм Horn–Schunck.<p>Метод <b style=border:0;margin:0;outline:0;padding:0;vertical-align:baseline>Horn–Schunck</b> носит несколько более глобальный характер, чем метод Лукаса-Канаде. Он опирается на предположение о том, что на всем изображении оптический поток будет достаточно гладким. От того же самого уравнения <img src=http://habr.habrastorage.org/post_images/240/a53/130/240a531308dcc6d61a797f29f0d09b7d.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle> предлагается перейти к функционалу <img src=http://habr.habrastorage.org/post_images/408/f58/8d3/408f588d3cb05bc7382f837ecc8ecf20.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>, то есть добавить требование на отсутствие резкого изменения сдвигов с весовым коэффициентом α. Минимизация этого функционала приводит нас к системе из двух уравнений:
<img src=http://habr.habrastorage.org/post_images/17e/fbc/79a/17efbc79aea908239ef99a47018bce6e.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>
<img src=http://habr.habrastorage.org/post_images/f38/e8c/b3a/f38e8cb3adc643694b4e08559e55d798.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle></p><p>В этих уравнениях лапласиан предлагают посчитать приближенно: <img src=http://habr.habrastorage.org/post_images/e82/489/c96/e82489c96f4090512dfa656998a04c2f.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle> – разница со средним значением. Получаем систему уравнений, которую записываем для каждого пикселя и решаем общую систему итеративно:
<img src=http://habr.habrastorage.org/post_images/a34/68c/2e9/a3468c2e91b7c7e4f44cdd51b13f170a.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>
<img src=http://habr.habrastorage.org/post_images/152/655/864/152655864a0daca7ecaf8baba9077b7a.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle></p><p>В данном алгоритме тоже предлагают использовать multi-scaling, причем рекомендуют масштабировать изображения не в 2 раза, а с коэффициентом 0.65</p><p>Этот алгоритм был реализован в первых версиях OpenCV, но в последствии от него отказались.</p><p><b style=border:0;margin:0;outline:0;padding:0;vertical-align:baseline>Farneback </b>предложил аппроксимировать изменение интенсивности в окрестности с помощью квадратичной формы: I = xAx + bx + c с симметричной матрицей A (по сути, рассматривая разложение по Тейлору до первого члена, мы брали линейную аппроксимацию I = bx + c, то есть сейчас мы как раз решили повысить точность приближения) Если изображение сдвинулось в пределах этой окрестности, то <img src=http://habr.habrastorage.org/post_images/cef/f63/791/ceff6379139b144d6d2c290118c7c8d5.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>, подставляем в квадратичное разложение, раскрываем скобки, получаем
<img src=http://habr.habrastorage.org/post_images/090/fa5/838/090fa58384ab43497db73237eb3a988e.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>
<img src=http://habr.habrastorage.org/post_images/030/eba/ae7/030ebaae7c37805e0be02defef7b4244.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>
<img src=http://habr.habrastorage.org/post_images/9d4/449/ebb/9d4449ebb384df15ef0853676772cdc9.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>.</p><p>Теперь мы можем вычислить значения A, b, c на обеих картинках, и тогда эта система станет избыточной относительно d (особенно смущает первое уравнение), и вообще d можно получить из второго уравнения: <img src=http://habr.habrastorage.org/post_images/ef8/7cf/c7d/ef87cfc7d6261d21a899cf835ab7df3a.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>. Приходится прибегать к следующей аппроксимации: <img src=http://habr.habrastorage.org/post_images/203/301/bf3/203301bf395673df883b82488e29ca14.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>. Обозначим еще для простоты <img src=http://habr.habrastorage.org/post_images/b37/4b0/d2f/b374b0d2f529418ef08c5def9fe659b7.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>, Тогда получим просто <img src=http://habr.habrastorage.org/post_images/b43/4e2/d0c/b434e2d0c3ae8e29e4249dc5c9aed27d.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>.</p><p>Для компенсации шумов при вычислении, снова обратимся к тому предположению, что в окрестности исследуемой точки у всех точек более или менее одинаковый сдвиг. Поэтому опять же проинтегрируем погрешность<img src=http://habr.habrastorage.org/post_images/a7e/62d/483/a7e62d48343e482228cf72e68de75445.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle> по окну с гауссовскими весовыми коэффициентами <i style=border:0;margin:0;outline:0;padding:0;vertical-align:baseline>w</i>, и найдем вектор d, минимизирующий эту суммарную погрешность. Тогда мы получим оптимальное значение <img src=http://habr.habrastorage.org/post_images/771/272/648/77127264820430245399b7c058413ed9.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle> и соответствующую минимальную ошибку <img src=http://habr.habrastorage.org/post_images/202/f32/3c0/202f323c016171111b818ae08b97ef6f.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>. То есть нам надо для каждой точки посчитать <img src=http://habr.habrastorage.org/post_images/98d/eec/6dd/98deec6ddd703117053247522a2da8af.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>, усреднить по окну, инвертировать матрицу и получить результат. Соответственно эти произведения можно посчитать для всей картинки и использовать заранее рассчитанные значения для разных точек, то есть это как раз тот случай, когда имеет смысл считать dense поток.</p><p>Как обычно, у этого алгоритма есть некоторое количество модификаций и усовершенствований, в первую очередь позволяющих использовать известную априорную информацию – заданную начальную аппроксимацию потока – и, опять же, multi-scaling.</p><p>В основе метода <b style=border:0;margin:0;outline:0;padding:0;vertical-align:baseline>SimpleFlow</b> лежит следующая идея: если мы все равно не умеем определять сдвиг больше чем размер окна, по которому мы искали производные, то зачем вообще заморачиваться с вычислением производных? Давайте просто в окне найдем наиболее похожую точку! А для разрешения неоднозначностей и для компенсации шумов учтем, что поток непрерывный и в окрестности данной точки все точки имеют почти одинаковый сдвиг. А проблему с размером окна опять же решим за счет multi-scaling&rsquo;а.</p><p>Более строго, алгоритм звучит так: для всех точек в окне находится функция «энергии», отвечающая (с обратной логарифмической зависимостью) за вероятность перехода исходной точки в эту точку: <img src=http://habr.habrastorage.org/post_images/95a/5bb/1f2/95a5bb1f2de9ef34b6c376a4e85c3003.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle>. Далее, считается свертка этой энергии с гауссовым окном <img src=http://habr.habrastorage.org/post_images/829/bd8/f54/829bd8f541609108d50d8bd269f146de.gif style=border:0;margin:0;max-width:100%;outline:0;padding:0;vertical-align:middle> и находятся значения (dx,dy), минимизирующие эту функцию. Чтобы получить субпиксельную точность, рассматривается малая окрестность найденной оптимальной точки (dx,dy) и в ней ищется пик функции энергии как пик параболоида. И, как было упомянуто выше, эта процедура выполняется для пирамиды масштабированных изображений. Еще у них в алгоритме предложены хитрые методы ускорения расчетов, но это уже кому интересно разберутся сами. Для нас же важно, что за счет этого данный алгоритм является (теоретически) достаточно быстрым при неплохой точности. И у него нет такой проблемы, как у предыдущих, что чем больше сдвиг, тем хуже он детектируется.</p><h4 style=border:0;color:#999;font-size:16px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>А если брать не интенсивность?</h4>Выше было сказано, что соответствие между точками может определяться разными величинами, так почему же мы рассматриваем только интенсивность? А потому, что любую другую величину можно свести к ней: мы просто фильтруем изображения соответствующим фильтром и на вход описанных выше алгоритмов подаем отфильтрованные изображения. Соответственно, если вы хотите использовать оптический поток, то сначала подумайте, в ваших условиях какая характеристика изображения будет наиболее стабильной, и проведите соответствующую фильтрацию, чтобы на входе алгоритма оказалась не интенсивность, а эта характеристика.<h4 style=border:0;color:#999;font-size:16px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>Практика</h4>Давайте опробуем на практике алгоритмы, которые нам предлагает OpenCV.<p>Здесь можно проводить множество различных исследований каждого алгоритма, варьируя параметры, изменяя входные последовательности – с разными сдвигами, поворотами, проективными преобразованиями, сегментами, с разными шумами и т. д. Это все заняло бы уйму времени и по размеру отчета превзошло бы настоящую статью, поэтому здесь предлагаю ограничиться простым случаем параллельного сдвига изображения на фиксированное расстояние и наложение небольших шумов. Это позволит понять в общих чертах, как запускать алгоритмы и кто из них круче.</p><p>Подробно синтаксис процедур описан на странице с [мануалом](<a href=http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html%22>http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html"</a> style=&ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;), здесь я приведу выжимку-перевод с моими комментариями.</p><p>Классический Лукас-Канаде реализован с пирамидой в процедуре calcOpticalFlowPyrLK. Алгоритм рассчитывает sparse-поток, то есть для заданного набора точек на первом изображении оценивает их положение на втором. Входные параметры достаточно очевидны: два входных изображения, входной и выходной наборы точек, status – выходной вектор, показывающий, найдена ли успешно соответствующая точка, err – выходной вектор оцененных погрешностей соответствующих точек, WinSize – размер окна, по которому происходит гауссово усреднение, я брал 21х21 и работало хорошо, maxLevel – количество слоев в пирамиде минус один, т. е. номер последнего слоя, я брал 5, criteria – условие выхода из итеративного процесса определения сдвига (минимизация погрешности производится итеративно) – этот параметр я оставлял по умолчанию, flags – дополнительные флаги, например можно использовать начальное приближение потока или выбрать метод оценки погрешности, minEigThreshold – пороговое значение градиента, ниже которого матрица считается вырожденной, я оставлял по умолчанию. Начиная с OpenCV 2.4.1, при вычислении потока можно использовать заранее вычисленную пирамиду отмасштабированных изображений.</p><p>Результат работы – успешно и стабильно обнаруживаются как малые, так и большие сдвиги, устойчив к довольно большим шумам, время работы – порядка 10 мс для 400 точек c 5-слойной пирамидой (на core i7 950).</p><p>Кстати, этот алгоритм реализован так же на Gpu (CUDA), причем как dense, так и sparse версии.</p><p>Поток Farneback реализуется процедурой calcOpticalFlowFarneback, рассчитывается dense-поток, то есть сдвиг каждой точки. Параметры: входные изображения, выходной поток в формате двухканальной матрицы float&rsquo;ов, pyr_scale определяет отношение масштабов между слоями пирамиды, levels – количество уровней в пирамиде, winsize – размер окна, по которому производится усреднение, iterations – количество итераций на каждом уровне, poly_n – размер полинома, по которому оцениваются значения A и b, poly_sigma – сигма гауссовского размытия при сглаживании производных, рекомендованные значения параметров указаны в мануале, flags – дополнительные флаги, например можно использовать начальное приближение потока или по-другому усреднять по окну.</p><p>Этот алгоритм куда менее стабилен (по моим наблюдениям), легче промахивается на довольно равномерных картинках (видимо, проблема в отсутствии фильтрации неудачных точек), плохо определяет большие сдвиги. У меня отрабатывал за 600 мс на изображении 512х512.</p><p>Поток SimpleFlow реализует процедура calcOpticalFlowSF (рассчитывается опять же dense поток), и у нее есть множество загадочных параметров без дефолтных значений, и вообще на данный момент на странице информация предоставлена весьма лаконично. Попробуем разобраться. Первые 3 – входные изображения и выходное двухканальное; layers – количество слоев в пирамиде, то есть сколько раз масштабируем исходное изображение; averaging_block_size – размер окна, в котором мы считали функцию энергии пикселей; max_flow – максимальный сдвиг, который мы хотим уметь определять на каждом шаге, по сути он определяется размером окна (хотя не совсем понятно, почему он int). На этом можно остановиться, а можно задать еще несколько параметров, смысл некоторых из них от меня ускользает.</p><p>На сайте предлагают посмотреть [пример](<a href=https://github.com/Itseez/opencv/blob/master/samples/cpp/simpleflow_demo.cpp%22>https://github.com/Itseez/opencv/blob/master/samples/cpp/simpleflow_demo.cpp"</a> style=&ldquo;border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;) его использования, в котором он запускается с такими параметрами: calcOpticalFlowSF(frame1, frame2, flow, 3, 2, 4, 4.1, 25.5, 18, 55.0, 25.5, 0.35, 18, 55.0, 25.5, 10);</p><p>У меня алгоритм работает значительно медленнее других, порядка 9-12 секунд на картинку 512х512. Результат работы кажется более правдоподобным, чем Farneback, по крайней мере лучше определяется сдвиг на равномерных картинках, заметно лучше срабатывает с большими сдвигами.</p><h4 style=border:0;color:#999;font-size:16px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>Выводы</h4>Если вы хотите использовать где-то оптический поток, сначала подумайте, нужен ли он вам: часто можно обойтись более простыми методами. Браться реализовывать поток самостоятельно стоит только несколько раз подумав: каждый алгоритм имеет множество хитростей, тонкостей и оптимизаций; что бы вы ни сделали, скорее всего, в OpenCV оно же работает лучше (естественно, при условии, что оно там есть). Тем более что они там вовсю используют логические и хардварные оптимизации типа использования SSE инструкций, многопоточность, возможности вычисления с CUDA или OpenCL и т. д. Если вам достаточно посчитать сдвиг некоторого набора точек (т. е. sparse поток), то можете смело использовать функцию calcOpticalFlowPyrLK, оно работает хорошо, надежно и достаточно быстро. Для вычисления dense-потока хорошо использовать функцию calcOpticalFlowSF, но она работает очень медленно. Если быстродействие критично, то calcOpticalFlowFarneback, но надо еще удостовериться, что результаты его работы вас устроят.<h4 style=border:0;color:#999;font-size:16px;font-weight:400;margin:0;outline:0;padding:0;vertical-align:baseline>Литература</h4>[docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/modules/video/doc/motion_analysis_and_object_tracking.html" style="border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)
Pyramidal Implementation of the Lucas Kanade Feature Tracker. Description of the algorithm — Jean-Yves Bouguet
Two-Frame Motion Estimation Based on Polynomial Expansion — Gunnar Farneback
SimpleFlow: A Non-iterative, Sublinear Optical Flow Algorithm — Michael Tao, Jiamin Bai, Pushmeet Kohli, and Sylvain Paris
Horn-Schunck Optical Flow with a Multi-Scale Strategy — Enric Meinhardt-Llopis, Javier Sanchez
[en.wikipedia.org/wiki/Optical_flow](http://en.wikipedia.org/wiki/Optical_flow" style="border: 0px; color: #990099; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)<div class=clear style=border:0;clear:both;margin:0;outline:0;padding:0;vertical-align:baseline></div></div><ul class=tags style="background-attachment:scroll;background-color:#fff;background-image:url(http://habrahabr.ru/images/bg-tags2.gif);background-position:0;background-repeat:no-repeat;border:0;font-family:Verdana,sans-serif;font-size:10px;list-style:none;margin:0 0 15px;outline:0;padding:2px 0 2px 20px;vertical-align:baseline"><li style=border:0;color:#999;display:inline;margin:0;outline:0;padding:0;vertical-align:baseline>[оптический поток](http://habrahabr.ru/search/?q=%5B%D0%BE%D0%BF%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9%20%D0%BF%D0%BE%D1%82%D0%BE%D0%BA%5D&target_type=posts" rel="tag" style="border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)</li><li style=border:0;color:#999;display:inline;margin:0;outline:0;padding:0;vertical-align:baseline>,&nbsp;[optical flow](http://habrahabr.ru/search/?q=%5Boptical%20flow%5D&target_type=posts" rel="tag" style="border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)</li><li style=border:0;color:#999;display:inline;margin:0;outline:0;padding:0;vertical-align:baseline>,&nbsp;[computer vision](http://habrahabr.ru/search/?q=%5Bcomputer%20vision%5D&target_type=posts" rel="tag" style="border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)</li><li style=border:0;color:#999;display:inline;margin:0;outline:0;padding:0;vertical-align:baseline>,&nbsp;[openCV](http://habrahabr.ru/search/?q=%5BopenCV%5D&target_type=posts" rel="tag" style="border: 0px; color: #666666; margin: 0px; outline: 0px; padding: 0px; vertical-align: baseline;)</li></ul><div class=infopanel_wrapper style=background-color:#fff;border:0;font-family:Verdana,sans-serif;font-size:12px;margin:0;outline:0;padding:0;vertical-align:baseline><div class=infopanel id=infopanel_post_201406 style="border-bottom-left-radius:5px;border-bottom-right-radius:5px;border-top-left-radius:5px;border-top-right-radius:5px;border:1px solid #e5e5e5;display:inline-block;font-family:Arial,sans-serif;font-size:11px;margin:0;outline:0;padding:0 10px;vertical-align:middle"><div class=voting style="border:0;float:left;margin:6px 26px 6px 0;outline:0;padding:0 20px;position:relative;vertical-align:baseline"></div></div></div></div></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://serge-m.github.io/tags/opencv/>opencv</a></li><li><a href=https://serge-m.github.io/tags/russian/>russian</a></li><li><a href=https://serge-m.github.io/tags/optical-flow/>optical flow</a></li></ul><nav class=paginav><a class=prev href=https://serge-m.github.io/posts/move-time-indicator-in-composition/><span class=title>« Prev Page</span><br><span>Moving time indicator in composition timeline in After Effects CS6 using scripts</span></a>
<a class=next href=https://serge-m.github.io/posts/opencv-tutorials-in-russian/><span class=title>Next Page »</span><br><span>OpenCV tutorials (Russian)</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share То, что вы хотели знать про оптический поток, но стеснялись спросить on twitter" href="https://twitter.com/intent/tweet/?text=%d0%a2%d0%be%2c%20%d1%87%d1%82%d0%be%20%d0%b2%d1%8b%20%d1%85%d0%be%d1%82%d0%b5%d0%bb%d0%b8%20%d0%b7%d0%bd%d0%b0%d1%82%d1%8c%20%d0%bf%d1%80%d0%be%20%d0%be%d0%bf%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9%20%d0%bf%d0%be%d1%82%d0%be%d0%ba%2c%20%d0%bd%d0%be%20%d1%81%d1%82%d0%b5%d1%81%d0%bd%d1%8f%d0%bb%d0%b8%d1%81%d1%8c%20%d1%81%d0%bf%d1%80%d0%be%d1%81%d0%b8%d1%82%d1%8c&url=https%3a%2f%2fserge-m.github.io%2fposts%2fblog-post-3%2f&hashtags=opencv%2crussian%2copticalflow"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share То, что вы хотели знать про оптический поток, но стеснялись спросить on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fserge-m.github.io%2fposts%2fblog-post-3%2f&title=%d0%a2%d0%be%2c%20%d1%87%d1%82%d0%be%20%d0%b2%d1%8b%20%d1%85%d0%be%d1%82%d0%b5%d0%bb%d0%b8%20%d0%b7%d0%bd%d0%b0%d1%82%d1%8c%20%d0%bf%d1%80%d0%be%20%d0%be%d0%bf%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9%20%d0%bf%d0%be%d1%82%d0%be%d0%ba%2c%20%d0%bd%d0%be%20%d1%81%d1%82%d0%b5%d1%81%d0%bd%d1%8f%d0%bb%d0%b8%d1%81%d1%8c%20%d1%81%d0%bf%d1%80%d0%be%d1%81%d0%b8%d1%82%d1%8c&summary=%d0%a2%d0%be%2c%20%d1%87%d1%82%d0%be%20%d0%b2%d1%8b%20%d1%85%d0%be%d1%82%d0%b5%d0%bb%d0%b8%20%d0%b7%d0%bd%d0%b0%d1%82%d1%8c%20%d0%bf%d1%80%d0%be%20%d0%be%d0%bf%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9%20%d0%bf%d0%be%d1%82%d0%be%d0%ba%2c%20%d0%bd%d0%be%20%d1%81%d1%82%d0%b5%d1%81%d0%bd%d1%8f%d0%bb%d0%b8%d1%81%d1%8c%20%d1%81%d0%bf%d1%80%d0%be%d1%81%d0%b8%d1%82%d1%8c&source=https%3a%2f%2fserge-m.github.io%2fposts%2fblog-post-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share То, что вы хотели знать про оптический поток, но стеснялись спросить on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fserge-m.github.io%2fposts%2fblog-post-3%2f&title=%d0%a2%d0%be%2c%20%d1%87%d1%82%d0%be%20%d0%b2%d1%8b%20%d1%85%d0%be%d1%82%d0%b5%d0%bb%d0%b8%20%d0%b7%d0%bd%d0%b0%d1%82%d1%8c%20%d0%bf%d1%80%d0%be%20%d0%be%d0%bf%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9%20%d0%bf%d0%be%d1%82%d0%be%d0%ba%2c%20%d0%bd%d0%be%20%d1%81%d1%82%d0%b5%d1%81%d0%bd%d1%8f%d0%bb%d0%b8%d1%81%d1%8c%20%d1%81%d0%bf%d1%80%d0%be%d1%81%d0%b8%d1%82%d1%8c"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share То, что вы хотели знать про оптический поток, но стеснялись спросить on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fserge-m.github.io%2fposts%2fblog-post-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share То, что вы хотели знать про оптический поток, но стеснялись спросить on whatsapp" href="https://api.whatsapp.com/send?text=%d0%a2%d0%be%2c%20%d1%87%d1%82%d0%be%20%d0%b2%d1%8b%20%d1%85%d0%be%d1%82%d0%b5%d0%bb%d0%b8%20%d0%b7%d0%bd%d0%b0%d1%82%d1%8c%20%d0%bf%d1%80%d0%be%20%d0%be%d0%bf%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9%20%d0%bf%d0%be%d1%82%d0%be%d0%ba%2c%20%d0%bd%d0%be%20%d1%81%d1%82%d0%b5%d1%81%d0%bd%d1%8f%d0%bb%d0%b8%d1%81%d1%8c%20%d1%81%d0%bf%d1%80%d0%be%d1%81%d0%b8%d1%82%d1%8c%20-%20https%3a%2f%2fserge-m.github.io%2fposts%2fblog-post-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share То, что вы хотели знать про оптический поток, но стеснялись спросить on telegram" href="https://telegram.me/share/url?text=%d0%a2%d0%be%2c%20%d1%87%d1%82%d0%be%20%d0%b2%d1%8b%20%d1%85%d0%be%d1%82%d0%b5%d0%bb%d0%b8%20%d0%b7%d0%bd%d0%b0%d1%82%d1%8c%20%d0%bf%d1%80%d0%be%20%d0%be%d0%bf%d1%82%d0%b8%d1%87%d0%b5%d1%81%d0%ba%d0%b8%d0%b9%20%d0%bf%d0%be%d1%82%d0%be%d0%ba%2c%20%d0%bd%d0%be%20%d1%81%d1%82%d0%b5%d1%81%d0%bd%d1%8f%d0%bb%d0%b8%d1%81%d1%8c%20%d1%81%d0%bf%d1%80%d0%be%d1%81%d0%b8%d1%82%d1%8c&url=https%3a%2f%2fserge-m.github.io%2fposts%2fblog-post-3%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://serge-m.github.io/>sergem's personal public notebook</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>