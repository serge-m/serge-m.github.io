<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Effect of techniques from Fast.ai | sergem's personal public notebook</title>
<meta name=keywords content="pytorch,deep learning,computer vision,neural networks,fast.ai,fastai,AdamW,learning rate,LRfinder,pytorch-nn-tools,resnet,one cycle">
<meta name=description content="fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.
Here I want to reproduce some of the techniques in order to understand what is the effect they bring.">
<meta name=author content="SergeM">
<link rel=canonical href=https://serge-m.github.io/posts/effect-of-techniques-from-fastai/>
<link href=/assets/css/stylesheet.min.6d98a2276d0cb41ef459267b3ff3ef02df70a8f16b70bbc52b20568702bc90cf.css integrity="sha256-bZiiJ20MtB70WSZ7P/PvAt9wqPFrcLvFKyBWhwK8kM8=" rel="preload stylesheet" as=style>
<link rel=icon href=https://serge-m.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://serge-m.github.io/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://serge-m.github.io/favicon-32x32.png>
<link rel=apple-touch-icon href=https://serge-m.github.io/apple-touch-icon.png>
<link rel=mask-icon href=https://serge-m.github.io/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.1">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-40853494-2','auto'),ga('send','pageview'))</script><meta property="og:title" content="Effect of techniques from Fast.ai">
<meta property="og:description" content="fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.
Here I want to reproduce some of the techniques in order to understand what is the effect they bring.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://serge-m.github.io/posts/effect-of-techniques-from-fastai/"><meta property="og:image" content="https://serge-m.github.io/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-11-15T10:00:00+00:00">
<meta property="article:modified_time" content="2020-11-15T10:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://serge-m.github.io/papermod-cover.png">
<meta name=twitter:title content="Effect of techniques from Fast.ai">
<meta name=twitter:description content="fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.
Here I want to reproduce some of the techniques in order to understand what is the effect they bring.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://serge-m.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Effect of techniques from Fast.ai","item":"https://serge-m.github.io/posts/effect-of-techniques-from-fastai/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Effect of techniques from Fast.ai","name":"Effect of techniques from Fast.ai","description":"fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.\nHere I want to reproduce some of the techniques in order to understand what is the effect they bring.","keywords":["pytorch","deep learning","computer vision","neural networks","fast.ai","fastai","AdamW","learning rate","LRfinder","pytorch-nn-tools","resnet","one cycle"],"articleBody":"fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.\nHere I want to reproduce some of the techniques in order to understand what is the effect they bring.\nI don’t want to use fastai library here for two reasons. First, for better understanding of the processes it is better to implement them by yourself.\nSecond, I think the library has certain disadvantages and it is better to stick to pytorch as close as possible. Fastai introduces too many layers of indirection between the user and pytorch. It is very convenient for the beginners or for standard use cases. But as soon as you need something more advance you basically have to digg through ALL the layers of fastai to get what you want. I started building another library that incorporates techniques from fast ai, but doesn’t force the user to stick to the library. Hopefully. Thus I am going to use that library for my tests.\nTask I am going to train an image classifier based on Resnet18 from pytorch. The classifier has to distinguish between the dog breeds from an excellent Imagewoof dataset. The dataset is designed by Jeremy Howard again to for learning purposes. the dataset is small enough to to fast experimentation. But it also large enough and sophisticated enough to make results and insights applicable for real datasets like Imagenet.\nBaseline I am going to start from SGD with 40 epochs, LR=0.1 for the first 30 epochs and 0.01 for epochs 30-40.\nInstalling dependencies:\n1  pip install pytorch-nn-tools==0.3.3 torch_lr_finder==0.2.1   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  import os import torch import torch.nn.parallel import torch.utils.data import torchvision import torchvision.datasets as datasets import torchvision.models as models import torchvision.transforms as transforms import numpy as np import matplotlib.pyplot as plt import torch.nn.functional as F from collections import defaultdict from typing import Dict, List, Callable, Union from pathlib import Path import json import time import datetime from fastprogress.fastprogress import master_bar, progress_bar from time import sleep from pytorch_nn_tools.visual import UnNormalize_, imagenet_stats from pytorch_nn_tools.train.metrics.processor import mod_name_train, mod_name_val, Marker from pytorch_nn_tools.train.metrics.processor import MetricAggregator, MetricLogger, MetricType from pytorch_nn_tools.train.progress import ProgressTracker from pytorch_nn_tools.convert import map_dict from pytorch_nn_tools.train.metrics.history_condition import HistoryCondition from pytorch_nn_tools.train.checkpoint import CheckpointSaver   Functions for generating datasets\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  def _train_dataset(path): normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], ) train_dir = os.path.join(path, 'train') train_dataset = datasets.ImageFolder( train_dir, transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize, ])) return train_dataset def _val_dataset(path): normalize = transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], ) val_dir = os.path.join(path, 'val') dataset = datasets.ImageFolder(val_dir, transforms.Compose( [transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ])) return dataset   Settings and data loaders:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  batch_size_train = 128 batch_size_val = 16 device = 'cuda' data_path = \"data/imagewoof2-320/\" train_dataloader = torch.utils.data.DataLoader( dataset=_train_dataset(data_path), batch_size=batch_size_train, shuffle=True, num_workers=num_workers, ) val_dataloader = torch.utils.data.DataLoader( dataset=_val_dataset(data_path), batch_size=batch_size_val, shuffle=False, num_workers=num_workers, )   Accuracy function from pytorch-lightning:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  def accuracy(output, target, topk=(1,)): \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\" with torch.no_grad(): maxk = max(topk) batch_size = target.size(0) _, pred = output.topk(maxk, 1, True, True) pred = pred.t() correct = pred.eq(target.view(1, -1).expand_as(pred)) res = [] for k in topk: correct_k = correct[:k].view(-1).float().sum(0, keepdim=True) res.append(correct_k.mul_(100.0 / batch_size)) return res   Some helper functions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  def to_device(batch, device): return batch.to(device) from fastprogress.fastprogress import master_bar, progress_bar class PBars: def __init__(self): self._main = None self._second = None def main(self, it, **kwargs): self._main = master_bar(it, **kwargs) return self._main def secondary(self, it, **kwargs): if self._main is None: raise RuntimeError(\"Cannot instantiate secondary progress bar. The main progress bar is not set.\") self._second = progress_bar(it, parent=self._main, **kwargs) return self._second def main_comment(self, comment): self._main.main_bar.comment = comment def now_as_str(): now = datetime.datetime.now() return now.strftime(\"%Y%m%d_%H%M%s_%f\") class DummyLogger: def debug(self, *args): print(*args)   TrainerIO is responsible for logging and checkpointing. That is the class that does IO on behalf of a trainer. It can 1) instantiate metric logger. 2) load last checkpoint and 3) save checkpoints\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  class TrainerIO: def __init__(self, log_dir: Union[Path, str], experiment_name: str, checkpoint_condition: Callable[[MetricType], bool]): self.log_dir = Path(log_dir) self.experiment_name = experiment_name self.path_experiment = self.log_dir.joinpath(experiment_name) self.path_checkpoints = self.path_experiment.joinpath(\"checkpoints\") self.checkpoint_saver = CheckpointSaver(self.path_checkpoints, logger=DummyLogger()) self.checkpoint_condition = checkpoint_condition def create_metric_logger(self): path_logs = self.path_experiment.joinpath(f\"{self.experiment_name}_{now_as_str()}\") metric_logger = MetricLogger(path_logs) return metric_logger def load_last(self, start_epoch: int, end_epoch: int, model, optimizer, scheduler) - int: last = self.checkpoint_saver.find_last(start_epoch, end_epoch) if last is not None: print(f\"found pretrained results for epoch {last}. Loading...\") self.checkpoint_saver.load(model, optimizer, scheduler, last) return last + 1 else: return start_epoch def save_checkpoint(self, metrics: MetricType, model, optimizer, scheduler, epoch): if self.checkpoint_condition(metrics): self.checkpoint_saver.save(model, optimizer, scheduler, epoch)   Now goes the trainer. In fastai and pytorch-lightning frameworks the trainers are implemented inside the library. The user has to use callbacks of inheritance in order to patch the standard trainer loop. That means the trainer has to be overly generic. For each case the user has to know how which callback to use and how the result of that callback influences the training logic. That look like a leaking abstraction to me. In that case the library doesn’t really abstract anything. The user has to know all the internals in order to do even moderately difficult things with it.\nI have a hypothesis that it is possible to inverse that dependency. I want the user to write a training loop on it’s own. I want my library to provide helper functions for that in order to make it super easy.\nThe biggest part of the training loop is IO: checkpointing, logging, data loading. There is very little amount of logic. I hypothesize that it may has no sense to extract it to a library. Let the user write it, but make it simple.\nIt’s not a finalized version btw. Some of the parts may be included into pytorch-nn-tools in the future.\nThe trainer has main function fit(...) that calls train_epoch(...) and validate_epoch(...).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  class Trainer: def __init__(self, device, trainer_io: TrainerIO, continue_training: bool = False): self.device = device self.continue_training = continue_training self.trainer_io = trainer_io self.pbars = PBars() def fit(self, model, optimizer, scheduler, start_epoch, end_epoch): metric_logger = self.trainer_io.create_metric_logger() model = model.to(self.device) if self.continue_training: start_epoch = self.trainer_io.load_last(start_epoch, end_epoch, model, optimizer, scheduler) progr_train = ProgressTracker() for epoch in self.pbars.main(range(start_epoch, end_epoch)): metric_aggregator = MetricAggregator() self.train_epoch( train_dataloader, progr_train, model, optimizer, scheduler, metric_proc=mod_name_train+metric_aggregator+metric_logger, pbars=self.pbars, report_step=10, tb_writer=metric_logger.writer ) self.validate_epoch( val_dataloader, model, metric_proc=mod_name_val+metric_aggregator+metric_logger, pbars=self.pbars, ) aggregated = map_dict(metric_aggregator.aggregate(), key_fn=lambda key: f\"avg.{key}\") metric_logger({ **aggregated, **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())}, Marker.EPOCH: epoch, }) self.pbars.main_comment(f\"{aggregated}\") self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch) scheduler.step() # for epoch-based scheduler metric_logger.close() def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, pbars, report_step=1, tb_writer=None): model.train() for batch_idx, (images, target) in enumerate(progr.track(pbars.secondary(data_loader))): optimizer.zero_grad() images = to_device(images, self.device) target = to_device(target, self.device) output = model(images) loss = F.cross_entropy(output, target) loss.backward() optimizer.step() if progr.cnt_total_iter % report_step == 0: with torch.no_grad(): acc1, acc5 = accuracy(output, target, topk=(1, 5)) metric_proc({ 'loss': loss, 'acc1': acc1, 'acc5': acc5, Marker.ITERATION: progr.cnt_total_iter, **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())}, }) # scheduler.step() # for batch based scheduler def validate_epoch(self, data_loader, model, metric_proc, pbars): model.eval() with torch.no_grad(): for images, target in pbars.secondary(data_loader): images = to_device(images, self.device) target = to_device(target, self.device) output = model(images) loss = F.cross_entropy(output, target) acc1, acc5 = accuracy(output, target, topk=(1, 5)) metric_proc(dict(loss=loss, acc1=acc1, acc5=acc5))   Finally the main part\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  recommended_lr = 0.1 num_epochs = 40 model = models.resnet18(pretrained=False) optimizer = torch.optim.SGD([ { 'name': 'main_model', 'params': model.parameters(), 'lr': 0.1, 'momentum': 0.9, 'weight_decay': 1e-4, } ]) scheduler = torch.optim.lr_scheduler.LambdaLR( optimizer, lambda epoch: 0.1 ** (epoch // 30) ) trainer_io = TrainerIO( log_dir=\"./logs/\", experiment_name=f\"experiment1_onecycle_lr{recommended_lr}_adamw\", checkpoint_condition=HistoryCondition( 'avg.val.acc1', lambda hist: len(hist) == 1 or hist[-1]  max(hist[:-1]) ) ) trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False) trainer.fit( model, optimizer, scheduler, start_epoch=0, end_epoch=num_epochs )   In order to run the trainer the user has to define TrainerIO, model, scheduler and optimizer. All those parts are interconnected and essential for the process.\nWe achieve accuracy for top 1 about 69% and for top 5 about 96 %. That is in line with the values that other people get for that dataset: Imagewoof Leaderboard\ntop1 accuracy\ntop5 accuracy\nLR Finder One of the greatest things I found in fastai is learning rate finder. It is a technique that helps to set up the initial (base) learning rate for the models.\nThe idea is to do the iteration of the training for gradually increasing learning rate. The learning rate value where we see the fastest descent of the loss is a good chioce.\nI am not going to write that LR finder myself and I use the existing module pytorch-lr-finder, github. That module works with standard pytorch concepts. If I would like to use the LRFinder from fast ai or pytorch lightning, I would be forced to use their trainers, optimizers etc.\nLet’s create a new model and optimizer. We start with a very low learning rate: 1e-8. The lr_finder will do 50 iterations up until lr=10.:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  model = models.resnet18(pretrained=False) optimizer = torch.optim.AdamW([ { 'name': 'main_model', 'params': model.parameters(), 'lr': 1e-8, 'weight_decay': 1e-4, } ]) from torch_lr_finder import LRFinder criterion = torch.nn.CrossEntropyLoss() lr_finder = LRFinder(model, optimizer, criterion, device=device) lr_finder.range_test(train_dataloader, val_loader=None, end_lr=10, num_iter=50, step_mode=\"exp\") _, recommended_lr = lr_finder.plot(log_lr=False) lr_finder.reset()   My recommended LR is about 0.02. I can launch with the same optimizer, lr schedule, but with the new base LR:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  recommended_lr = 0.02 model = models.resnet18(pretrained=False) optimizer = torch.optim.SGD([ { 'name': 'main_model', 'params': model.parameters(), 'lr': recommended_lr, 'momentum': 0.9, 'weight_decay': 1e-4, } ]) scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=recommended_lr, epochs=num_epochs, steps_per_epoch=len(train_dataloader) )   Here also we have to patch our trainer. Scheduler step now happens for each training batch, not per epoch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  class Trainer: # ..... def fit(self, model, optimizer, scheduler, start_epoch, end_epoch): # .... for epoch in self.pbars.main(range(start_epoch, end_epoch)): # ..... self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch) # not needed any more # scheduler.step()  metric_logger.close() def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, pbars, report_step=1, tb_writer=None): model.train() for batch_idx, (images, target) in enumerate(progr.track(pbars.secondary(data_loader))): # ... # now we do scheduler step for every batch scheduler.step() def validate_epoch(self, data_loader, model, metric_proc, pbars): # stays the same # ...   Let’s run:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  trainer_io = TrainerIO( log_dir=\"./logs/\", experiment_name=f\"experiment_base_lr{recommended_lr}\", checkpoint_condition=HistoryCondition( 'avg.val.acc1', lambda hist: len(hist) == 1 or hist[-1]  max(hist[:-1]) ) ) trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False) trainer.fit( model, optimizer, scheduler, start_epoch=0, end_epoch=num_epochs )   train loss. lr=0.1 vs lr=0.02 found by LRFinder\nTop 1 Accuracy improved significantly from 0.69 to 0.78!\ntop 1 accuracy on validation. lr=0.1 vs lr=0.02 found by LRFinder\nOne Cycle learning rate Another technique is to have a special LR schedule. We start small, ramp it up quickly to the maximum and then gradually decrease.\nHere is how the LR graphs look like:\nlearning rate schedules. Gray line is a “one cycle”\nWe don’t get accuracy increase here. But we don’t need to hand craft the learning rate schedule any more. That is positive.\nHopefully one cycle brings some numerical stability.\nAccuracy top 1 for one cycle. Gray line is a “one cycle”\nAdamW Instead of SGD let’s use Adam optimizer. More specifically AdamW version of it. It was also advertised in fastai as a good practice.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  model = models.resnet18(pretrained=False) optimizer = torch.optim.AdamW([ { 'name': 'main_model', 'params': model.parameters(), 'lr': recommended_lr, 'weight_decay': 1e-4, } ]) scheduler = torch.optim.lr_scheduler.OneCycleLR( optimizer, max_lr=recommended_lr, epochs=num_epochs, steps_per_epoch=len(train_dataloader) ) trainer_io = TrainerIO( log_dir=\"./logs/\", experiment_name=f\"experiment1_onecycle_lr{recommended_lr}_adamw\", checkpoint_condition=HistoryCondition( 'avg.val.acc1', lambda hist: len(hist) == 1 or hist[-1]  max(hist[:-1]) ) ) trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False) trainer.fit( model, optimizer, scheduler, start_epoch=0, end_epoch=num_epochs )   That helps a lot. We go from 79.5% to 82.6% of accuracy:\nAccuracy top 1 for AdamW. Dark red line is a AdamW\nTrain loss for AdamW. Dark red line is a AdamW\nConclusion We used a fraction of tricks from fastai to improve the performance of our training. They all seem to be useful. Only OneCycle scheduler didn’t bring an evident benefits in the accuracy. However it brings some simplicity for developer.\nThe numbers are still pretty far from the values on the leaderboard, but we didn’t use all the tricks of course.\nAlso here we verified the viability of pytorch-nn-tools library.\n","wordCount":"2352","inLanguage":"en","datePublished":"2020-11-15T10:00:00Z","dateModified":"2020-11-15T10:00:00Z","author":{"@type":"Person","name":"SergeM"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://serge-m.github.io/posts/effect-of-techniques-from-fastai/"},"publisher":{"@type":"Organization","name":"sergem's personal public notebook","logo":{"@type":"ImageObject","url":"https://serge-m.github.io/favicon.ico"}}}</script>
</head>
<body id=top>
<script>window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://serge-m.github.io/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches>
</span>
</div>
<ul id=menu onscroll=menu_on_scroll()>
<li>
<a href=https://serge-m.github.io/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://serge-m.github.io/categories/ title=Categories>
<span>Categories</span>
</a>
</li>
<li>
<a href=https://serge-m.github.io/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://serge-m.github.io/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://serge-m.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://serge-m.github.io/posts/>Posts</a></div>
<h1 class=post-title>
Effect of techniques from Fast.ai
</h1>
<div class=post-meta>November 15, 2020&nbsp;·&nbsp;SergeM
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<div class=details>Table of Contents</div>
</summary>
<div class=inner><ul>
<li>
<a href=#task aria-label=Task>Task</a></li>
<li>
<a href=#baseline aria-label=Baseline>Baseline</a></li>
<li>
<a href=#lr-finder aria-label="LR Finder">LR Finder</a></li>
<li>
<a href=#one-cycle-learning-rate aria-label="One Cycle learning rate">One Cycle learning rate</a></li>
<li>
<a href=#adamw aria-label=AdamW>AdamW</a></li>
<li>
<a href=#conclusion aria-label=Conclusion>Conclusion</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p><code>fast.ai</code> is a brilliant <a href=https://github.com/fastai/fastai>library</a> and a <a href=https://course.fast.ai/>course</a>
by Jeremy Howard an co. They use pytorch as a base and explain
deep learning from the foundations to a very decent level.
In his course Jeremy Howard demonstrates a lot of interesting techniques
that he finds in papers and that do NN training faster/better/cheaper.</p>
<p>Here I want to reproduce some of the techniques in order to understand what is the effect they bring.</p>
<p>I don&rsquo;t want to use <a href=https://github.com/fastai/fastai>fastai library</a> here for two reasons.
First, for better understanding of the processes it
is better to implement them by yourself.<br>
Second, I think the library has certain disadvantages and it is better to
stick to pytorch as close as possible.
Fastai introduces too many layers of indirection between the user and pytorch.
It is very convenient for the beginners or for standard use cases.
But as soon as you need something more advance you basically have
to digg through ALL the layers of fastai to get what you want.
I started building another <a href=https://pypi.org/project/pytorch-nn-tools/>library</a>
that incorporates techniques from fast ai, but doesn&rsquo;t force the user to
stick to the library. Hopefully. Thus I am going to use that library for my tests.</p>
<h2 id=task>Task<a hidden class=anchor aria-hidden=true href=#task>#</a></h2>
<p>I am going to train an image classifier
based on Resnet18 from pytorch.
The classifier has to distinguish between the dog breeds from
an excellent <a href=https://github.com/fastai/imagenette#imagewoof>Imagewoof</a> dataset. The dataset
is designed by Jeremy Howard again to
for learning purposes. the dataset is small enough to
to fast experimentation. But it also large enough and sophisticated enough
to make results and insights applicable for real datasets like Imagenet.</p>
<h2 id=baseline>Baseline<a hidden class=anchor aria-hidden=true href=#baseline>#</a></h2>
<p>I am going to start from SGD with 40 epochs, LR=0.1 for the first 30 epochs and 0.01 for epochs 30-40.</p>
<p>Installing dependencies:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-sh data-lang=sh>pip install pytorch-nn-tools<span class=o>==</span>0.3.3 <span class=nv>torch_lr_finder</span><span class=o>==</span>0.2.1
</code></pre></td></tr></table>
</div>
</div><div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>os</span>

<span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>import</span> <span class=nn>torch.nn.parallel</span>
<span class=kn>import</span> <span class=nn>torch.utils.data</span>
<span class=kn>import</span> <span class=nn>torchvision</span>
<span class=kn>import</span> <span class=nn>torchvision.datasets</span> <span class=k>as</span> <span class=nn>datasets</span>
<span class=kn>import</span> <span class=nn>torchvision.models</span> <span class=k>as</span> <span class=nn>models</span>
<span class=kn>import</span> <span class=nn>torchvision.transforms</span> <span class=k>as</span> <span class=nn>transforms</span>
<span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>

<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
<span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>defaultdict</span>
<span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>Dict</span><span class=p>,</span> <span class=n>List</span><span class=p>,</span> <span class=n>Callable</span><span class=p>,</span> <span class=n>Union</span>
<span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <span class=n>Path</span>
<span class=kn>import</span> <span class=nn>json</span>
<span class=kn>import</span> <span class=nn>time</span>
<span class=kn>import</span> <span class=nn>datetime</span>


<span class=kn>from</span> <span class=nn>fastprogress.fastprogress</span> <span class=kn>import</span> <span class=n>master_bar</span><span class=p>,</span> <span class=n>progress_bar</span>
<span class=kn>from</span> <span class=nn>time</span> <span class=kn>import</span> <span class=n>sleep</span>

<span class=kn>from</span> <span class=nn>pytorch_nn_tools.visual</span> <span class=kn>import</span> <span class=n>UnNormalize_</span><span class=p>,</span> <span class=n>imagenet_stats</span>
<span class=kn>from</span> <span class=nn>pytorch_nn_tools.train.metrics.processor</span> <span class=kn>import</span> <span class=n>mod_name_train</span><span class=p>,</span> <span class=n>mod_name_val</span><span class=p>,</span> <span class=n>Marker</span>
<span class=kn>from</span> <span class=nn>pytorch_nn_tools.train.metrics.processor</span> <span class=kn>import</span> <span class=n>MetricAggregator</span><span class=p>,</span> <span class=n>MetricLogger</span><span class=p>,</span> <span class=n>MetricType</span>
<span class=kn>from</span> <span class=nn>pytorch_nn_tools.train.progress</span> <span class=kn>import</span> <span class=n>ProgressTracker</span>
<span class=kn>from</span> <span class=nn>pytorch_nn_tools.convert</span> <span class=kn>import</span> <span class=n>map_dict</span>
<span class=kn>from</span> <span class=nn>pytorch_nn_tools.train.metrics.history_condition</span> <span class=kn>import</span> <span class=n>HistoryCondition</span>
<span class=kn>from</span> <span class=nn>pytorch_nn_tools.train.checkpoint</span> <span class=kn>import</span> <span class=n>CheckpointSaver</span>
</code></pre></td></tr></table>
</div>
</div><p>Functions for generating datasets</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>_train_dataset</span><span class=p>(</span><span class=n>path</span><span class=p>):</span>
    <span class=n>normalize</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>(</span>
        <span class=n>mean</span><span class=o>=</span><span class=p>[</span><span class=mf>0.485</span><span class=p>,</span> <span class=mf>0.456</span><span class=p>,</span> <span class=mf>0.406</span><span class=p>],</span>
        <span class=n>std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.224</span><span class=p>,</span> <span class=mf>0.225</span><span class=p>],</span>
    <span class=p>)</span>

    <span class=n>train_dir</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>path</span><span class=p>,</span> <span class=s1>&#39;train&#39;</span><span class=p>)</span>
    <span class=n>train_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>ImageFolder</span><span class=p>(</span>
        <span class=n>train_dir</span><span class=p>,</span>
        <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
            <span class=n>transforms</span><span class=o>.</span><span class=n>RandomResizedCrop</span><span class=p>(</span><span class=mi>224</span><span class=p>),</span>
            <span class=n>transforms</span><span class=o>.</span><span class=n>RandomHorizontalFlip</span><span class=p>(),</span>
            <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>
            <span class=n>normalize</span><span class=p>,</span>
        <span class=p>]))</span>
    <span class=k>return</span> <span class=n>train_dataset</span>


<span class=k>def</span> <span class=nf>_val_dataset</span><span class=p>(</span><span class=n>path</span><span class=p>):</span>
    <span class=n>normalize</span> <span class=o>=</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>(</span>
        <span class=n>mean</span><span class=o>=</span><span class=p>[</span><span class=mf>0.485</span><span class=p>,</span> <span class=mf>0.456</span><span class=p>,</span> <span class=mf>0.406</span><span class=p>],</span>
        <span class=n>std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.224</span><span class=p>,</span> <span class=mf>0.225</span><span class=p>],</span>
    <span class=p>)</span>
    <span class=n>val_dir</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>path</span><span class=p>,</span> <span class=s1>&#39;val&#39;</span><span class=p>)</span>
    <span class=n>dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>ImageFolder</span><span class=p>(</span><span class=n>val_dir</span><span class=p>,</span> <span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>(</span>
        <span class=p>[</span><span class=n>transforms</span><span class=o>.</span><span class=n>Resize</span><span class=p>(</span><span class=mi>256</span><span class=p>),</span> <span class=n>transforms</span><span class=o>.</span><span class=n>CenterCrop</span><span class=p>(</span><span class=mi>224</span><span class=p>),</span> <span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span> <span class=n>normalize</span><span class=p>,</span> <span class=p>]))</span>
    <span class=k>return</span> <span class=n>dataset</span>

</code></pre></td></tr></table>
</div>
</div><p>Settings and data loaders:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>batch_size_train</span> <span class=o>=</span> <span class=mi>128</span>
<span class=n>batch_size_val</span> <span class=o>=</span> <span class=mi>16</span>
<span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span>


<span class=n>data_path</span> <span class=o>=</span> <span class=s2>&#34;data/imagewoof2-320/&#34;</span>

<span class=n>train_dataloader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
        <span class=n>dataset</span><span class=o>=</span><span class=n>_train_dataset</span><span class=p>(</span><span class=n>data_path</span><span class=p>),</span>
        <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size_train</span><span class=p>,</span>
        <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> 
        <span class=n>num_workers</span><span class=o>=</span><span class=n>num_workers</span><span class=p>,</span>
    <span class=p>)</span>
    
<span class=n>val_dataloader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span>
        <span class=n>dataset</span><span class=o>=</span><span class=n>_val_dataset</span><span class=p>(</span><span class=n>data_path</span><span class=p>),</span>
        <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size_val</span><span class=p>,</span>
        <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
        <span class=n>num_workers</span><span class=o>=</span><span class=n>num_workers</span><span class=p>,</span>
    <span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><p>Accuracy function from pytorch-lightning:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>accuracy</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>,</span> <span class=n>topk</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,)):</span>
    <span class=s2>&#34;&#34;&#34;Computes the accuracy over the k top predictions for the specified values of k&#34;&#34;&#34;</span>
    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
        <span class=n>maxk</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>topk</span><span class=p>)</span>
        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>target</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>

        <span class=n>_</span><span class=p>,</span> <span class=n>pred</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>maxk</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=kc>True</span><span class=p>,</span> <span class=kc>True</span><span class=p>)</span>
        <span class=n>pred</span> <span class=o>=</span> <span class=n>pred</span><span class=o>.</span><span class=n>t</span><span class=p>()</span>
        <span class=n>correct</span> <span class=o>=</span> <span class=n>pred</span><span class=o>.</span><span class=n>eq</span><span class=p>(</span><span class=n>target</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>expand_as</span><span class=p>(</span><span class=n>pred</span><span class=p>))</span>

        <span class=n>res</span> <span class=o>=</span> <span class=p>[]</span>
        <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>topk</span><span class=p>:</span>
            <span class=n>correct_k</span> <span class=o>=</span> <span class=n>correct</span><span class=p>[:</span><span class=n>k</span><span class=p>]</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
            <span class=n>res</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>correct_k</span><span class=o>.</span><span class=n>mul_</span><span class=p>(</span><span class=mf>100.0</span> <span class=o>/</span> <span class=n>batch_size</span><span class=p>))</span>
        <span class=k>return</span> <span class=n>res</span>
</code></pre></td></tr></table>
</div>
</div><p>Some helper functions:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>to_device</span><span class=p>(</span><span class=n>batch</span><span class=p>,</span> <span class=n>device</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>batch</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>

<span class=kn>from</span> <span class=nn>fastprogress.fastprogress</span> <span class=kn>import</span> <span class=n>master_bar</span><span class=p>,</span> <span class=n>progress_bar</span>

<span class=k>class</span> <span class=nc>PBars</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_main</span> <span class=o>=</span> <span class=kc>None</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_second</span> <span class=o>=</span> <span class=kc>None</span>
        
    <span class=k>def</span> <span class=nf>main</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>it</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_main</span> <span class=o>=</span> <span class=n>master_bar</span><span class=p>(</span><span class=n>it</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_main</span>
    
    <span class=k>def</span> <span class=nf>secondary</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>it</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>_main</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
            <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;Cannot instantiate secondary progress bar. The main progress bar is not set.&#34;</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_second</span> <span class=o>=</span> <span class=n>progress_bar</span><span class=p>(</span><span class=n>it</span><span class=p>,</span> <span class=n>parent</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_main</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_second</span>
        
    <span class=k>def</span> <span class=nf>main_comment</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>comment</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>_main</span><span class=o>.</span><span class=n>main_bar</span><span class=o>.</span><span class=n>comment</span> <span class=o>=</span> <span class=n>comment</span>
        
<span class=k>def</span> <span class=nf>now_as_str</span><span class=p>():</span>
    <span class=n>now</span> <span class=o>=</span> <span class=n>datetime</span><span class=o>.</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span>
    <span class=k>return</span> <span class=n>now</span><span class=o>.</span><span class=n>strftime</span><span class=p>(</span><span class=s2>&#34;%Y%m</span><span class=si>%d</span><span class=s2>_%H%M</span><span class=si>%s</span><span class=s2>_</span><span class=si>%f</span><span class=s2>&#34;</span><span class=p>)</span>

<span class=k>class</span> <span class=nc>DummyLogger</span><span class=p>:</span>
    <span class=k>def</span> <span class=nf>debug</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>args</span><span class=p>):</span>
        <span class=nb>print</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>)</span>

</code></pre></td></tr></table>
</div>
</div><p>TrainerIO is responsible for logging and checkpointing.
That is the class that does IO on behalf of a trainer.
It can 1) instantiate metric logger. 2) load last checkpoint and 3) save checkpoints</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>TrainerIO</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>log_dir</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=n>Path</span><span class=p>,</span> <span class=nb>str</span><span class=p>],</span> <span class=n>experiment_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>checkpoint_condition</span><span class=p>:</span> <span class=n>Callable</span><span class=p>[[</span><span class=n>MetricType</span><span class=p>],</span> <span class=nb>bool</span><span class=p>]):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>log_dir</span> <span class=o>=</span> <span class=n>Path</span><span class=p>(</span><span class=n>log_dir</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>experiment_name</span> <span class=o>=</span> <span class=n>experiment_name</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>path_experiment</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>log_dir</span><span class=o>.</span><span class=n>joinpath</span><span class=p>(</span><span class=n>experiment_name</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>path_checkpoints</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>path_experiment</span><span class=o>.</span><span class=n>joinpath</span><span class=p>(</span><span class=s2>&#34;checkpoints&#34;</span><span class=p>)</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>checkpoint_saver</span> <span class=o>=</span> <span class=n>CheckpointSaver</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>path_checkpoints</span><span class=p>,</span> <span class=n>logger</span><span class=o>=</span><span class=n>DummyLogger</span><span class=p>())</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>checkpoint_condition</span> <span class=o>=</span> <span class=n>checkpoint_condition</span>
    
    <span class=k>def</span> <span class=nf>create_metric_logger</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=n>path_logs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>path_experiment</span><span class=o>.</span><span class=n>joinpath</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>experiment_name</span><span class=si>}</span><span class=s2>_</span><span class=si>{</span><span class=n>now_as_str</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
        <span class=n>metric_logger</span> <span class=o>=</span> <span class=n>MetricLogger</span><span class=p>(</span><span class=n>path_logs</span><span class=p>)</span>
        <span class=k>return</span> <span class=n>metric_logger</span>
    
    <span class=k>def</span> <span class=nf>load_last</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>start_epoch</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>end_epoch</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
        <span class=n>last</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>checkpoint_saver</span><span class=o>.</span><span class=n>find_last</span><span class=p>(</span><span class=n>start_epoch</span><span class=p>,</span> <span class=n>end_epoch</span><span class=p>)</span>
        <span class=k>if</span> <span class=n>last</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;found pretrained results for epoch </span><span class=si>{</span><span class=n>last</span><span class=si>}</span><span class=s2>. Loading...&#34;</span><span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>checkpoint_saver</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>last</span><span class=p>)</span>
            <span class=k>return</span> <span class=n>last</span> <span class=o>+</span> <span class=mi>1</span>
        <span class=k>else</span><span class=p>:</span>
            <span class=k>return</span> <span class=n>start_epoch</span>
    
    <span class=k>def</span> <span class=nf>save_checkpoint</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>metrics</span><span class=p>:</span> <span class=n>MetricType</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>epoch</span><span class=p>):</span>
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>checkpoint_condition</span><span class=p>(</span><span class=n>metrics</span><span class=p>):</span> 
            <span class=bp>self</span><span class=o>.</span><span class=n>checkpoint_saver</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>epoch</span><span class=p>)</span>
    
</code></pre></td></tr></table>
</div>
</div><p>Now goes the trainer. In fastai and pytorch-lightning frameworks
the trainers are implemented inside the library. The user has to use callbacks of inheritance
in order to patch the standard trainer loop.
That means the trainer has to be overly generic. For each case the user has to know how
which callback to use and how the result of that callback influences the training logic.
That look like a leaking abstraction to me. In that case the library doesn&rsquo;t really abstract anything.
The user has to know all the internals in order to do even moderately difficult things with it.</p>
<p>I have a hypothesis that it is possible to inverse that dependency. I want the user to write a training
loop on it&rsquo;s own. I want my library to provide helper functions for that in order to make it super easy.</p>
<p>The biggest part of the training loop is IO: checkpointing, logging, data loading. There is
very little amount of logic. I hypothesize that it may has no sense to extract it to a library. Let
the user write it, but make it simple.</p>
<p>It&rsquo;s not a finalized version btw. Some of the parts may be included into <code>pytorch-nn-tools</code>
in the future.</p>
<p>The trainer has main function <code>fit(...)</code> that calls <code>train_epoch(...)</code> and <code>validate_epoch(...)</code>.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span><span class=lnt>84
</span><span class=lnt>85
</span><span class=lnt>86
</span><span class=lnt>87
</span><span class=lnt>88
</span><span class=lnt>89
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>Trainer</span><span class=p>:</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>device</span><span class=p>,</span> <span class=n>trainer_io</span><span class=p>:</span> <span class=n>TrainerIO</span><span class=p>,</span>
                <span class=n>continue_training</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>):</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>continue_training</span> <span class=o>=</span> <span class=n>continue_training</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>trainer_io</span> <span class=o>=</span> <span class=n>trainer_io</span>
        <span class=bp>self</span><span class=o>.</span><span class=n>pbars</span> <span class=o>=</span> <span class=n>PBars</span><span class=p>()</span>
        
        
    <span class=k>def</span> <span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>start_epoch</span><span class=p>,</span> <span class=n>end_epoch</span><span class=p>):</span>
        <span class=n>metric_logger</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>trainer_io</span><span class=o>.</span><span class=n>create_metric_logger</span><span class=p>()</span>
        
        <span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
        
        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>continue_training</span><span class=p>:</span>
            <span class=n>start_epoch</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>trainer_io</span><span class=o>.</span><span class=n>load_last</span><span class=p>(</span><span class=n>start_epoch</span><span class=p>,</span> <span class=n>end_epoch</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>)</span>

        <span class=n>progr_train</span> <span class=o>=</span> <span class=n>ProgressTracker</span><span class=p>()</span>
        
        <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>pbars</span><span class=o>.</span><span class=n>main</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>start_epoch</span><span class=p>,</span> <span class=n>end_epoch</span><span class=p>)):</span>
            <span class=n>metric_aggregator</span> <span class=o>=</span> <span class=n>MetricAggregator</span><span class=p>()</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>train_epoch</span><span class=p>(</span>
                <span class=n>train_dataloader</span><span class=p>,</span> <span class=n>progr_train</span><span class=p>,</span>
                <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span>  
                <span class=n>metric_proc</span><span class=o>=</span><span class=n>mod_name_train</span><span class=o>+</span><span class=n>metric_aggregator</span><span class=o>+</span><span class=n>metric_logger</span><span class=p>,</span>
                <span class=n>pbars</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>pbars</span><span class=p>,</span>
                <span class=n>report_step</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
                <span class=n>tb_writer</span><span class=o>=</span><span class=n>metric_logger</span><span class=o>.</span><span class=n>writer</span>
            <span class=p>)</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>validate_epoch</span><span class=p>(</span>
                <span class=n>val_dataloader</span><span class=p>,</span>
                <span class=n>model</span><span class=p>,</span>  
                <span class=n>metric_proc</span><span class=o>=</span><span class=n>mod_name_val</span><span class=o>+</span><span class=n>metric_aggregator</span><span class=o>+</span><span class=n>metric_logger</span><span class=p>,</span>
                <span class=n>pbars</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>pbars</span><span class=p>,</span>
            <span class=p>)</span>
            
            <span class=n>aggregated</span> <span class=o>=</span> <span class=n>map_dict</span><span class=p>(</span><span class=n>metric_aggregator</span><span class=o>.</span><span class=n>aggregate</span><span class=p>(),</span> <span class=n>key_fn</span><span class=o>=</span><span class=k>lambda</span> <span class=n>key</span><span class=p>:</span> <span class=sa>f</span><span class=s2>&#34;avg.</span><span class=si>{</span><span class=n>key</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
            <span class=n>metric_logger</span><span class=p>({</span>
                <span class=o>**</span><span class=n>aggregated</span><span class=p>,</span> 
                <span class=o>**</span><span class=p>{</span><span class=sa>f</span><span class=s2>&#34;lr_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>:</span> <span class=n>lr</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>lr</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>scheduler</span><span class=o>.</span><span class=n>get_last_lr</span><span class=p>())},</span>
                <span class=n>Marker</span><span class=o>.</span><span class=n>EPOCH</span><span class=p>:</span> <span class=n>epoch</span><span class=p>,</span>
            <span class=p>})</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>pbars</span><span class=o>.</span><span class=n>main_comment</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>aggregated</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
                        
            <span class=bp>self</span><span class=o>.</span><span class=n>trainer_io</span><span class=o>.</span><span class=n>save_checkpoint</span><span class=p>(</span><span class=n>aggregated</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>epoch</span><span class=p>)</span>
            
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>  <span class=c1># for epoch-based scheduler</span>
            
        <span class=n>metric_logger</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
            
    <span class=k>def</span> <span class=nf>train_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>data_loader</span><span class=p>,</span> <span class=n>progr</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>metric_proc</span><span class=p>,</span> <span class=n>pbars</span><span class=p>,</span> <span class=n>report_step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
                   <span class=n>tb_writer</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
                
        <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>progr</span><span class=o>.</span><span class=n>track</span><span class=p>(</span><span class=n>pbars</span><span class=o>.</span><span class=n>secondary</span><span class=p>(</span><span class=n>data_loader</span><span class=p>))):</span>
            <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
            
            <span class=n>images</span> <span class=o>=</span> <span class=n>to_device</span><span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
            <span class=n>target</span> <span class=o>=</span> <span class=n>to_device</span><span class=p>(</span><span class=n>target</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
            
            <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
            <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
            <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
            <span class=k>if</span> <span class=n>progr</span><span class=o>.</span><span class=n>cnt_total_iter</span> <span class=o>%</span> <span class=n>report_step</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
                <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
                    <span class=n>acc1</span><span class=p>,</span> <span class=n>acc5</span> <span class=o>=</span> <span class=n>accuracy</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>,</span> <span class=n>topk</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>

                <span class=n>metric_proc</span><span class=p>({</span>
                    <span class=s1>&#39;loss&#39;</span><span class=p>:</span> <span class=n>loss</span><span class=p>,</span> <span class=s1>&#39;acc1&#39;</span><span class=p>:</span> <span class=n>acc1</span><span class=p>,</span> <span class=s1>&#39;acc5&#39;</span><span class=p>:</span> <span class=n>acc5</span><span class=p>,</span> <span class=n>Marker</span><span class=o>.</span><span class=n>ITERATION</span><span class=p>:</span> <span class=n>progr</span><span class=o>.</span><span class=n>cnt_total_iter</span><span class=p>,</span>
                    <span class=o>**</span><span class=p>{</span><span class=sa>f</span><span class=s2>&#34;lr_</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>:</span> <span class=n>lr</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>lr</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>scheduler</span><span class=o>.</span><span class=n>get_last_lr</span><span class=p>())},</span>
                <span class=p>})</span>
                
<span class=c1>#             scheduler.step()  # for batch based scheduler</span>

    <span class=k>def</span> <span class=nf>validate_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>data_loader</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>metric_proc</span><span class=p>,</span> <span class=n>pbars</span><span class=p>):</span>
        <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
        
        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
            <span class=k>for</span> <span class=n>images</span><span class=p>,</span> <span class=n>target</span> <span class=ow>in</span> <span class=n>pbars</span><span class=o>.</span><span class=n>secondary</span><span class=p>(</span><span class=n>data_loader</span><span class=p>):</span>
                <span class=n>images</span> <span class=o>=</span> <span class=n>to_device</span><span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
                <span class=n>target</span> <span class=o>=</span> <span class=n>to_device</span><span class=p>(</span><span class=n>target</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

                <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>
                <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span>
                <span class=n>acc1</span><span class=p>,</span> <span class=n>acc5</span> <span class=o>=</span> <span class=n>accuracy</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>target</span><span class=p>,</span> <span class=n>topk</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
                
                <span class=n>metric_proc</span><span class=p>(</span><span class=nb>dict</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=n>loss</span><span class=p>,</span> <span class=n>acc1</span><span class=o>=</span><span class=n>acc1</span><span class=p>,</span> <span class=n>acc5</span><span class=o>=</span><span class=n>acc5</span><span class=p>))</span>

</code></pre></td></tr></table>
</div>
</div><p>Finally the main part</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>recommended_lr</span> <span class=o>=</span> <span class=mf>0.1</span>
<span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>40</span>

<span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>pretrained</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>        

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>([</span>
    <span class=p>{</span>
        <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;main_model&#39;</span><span class=p>,</span>
        <span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
        <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>,</span>
        <span class=s1>&#39;momentum&#39;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span>
        <span class=s1>&#39;weight_decay&#39;</span><span class=p>:</span> <span class=mf>1e-4</span><span class=p>,</span>
    <span class=p>}</span>
<span class=p>])</span>

<span class=n>scheduler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>LambdaLR</span><span class=p>(</span>
    <span class=n>optimizer</span><span class=p>,</span>
    <span class=k>lambda</span> <span class=n>epoch</span><span class=p>:</span> <span class=mf>0.1</span> <span class=o>**</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>//</span> <span class=mi>30</span><span class=p>)</span>
<span class=p>)</span>

<span class=n>trainer_io</span> <span class=o>=</span> <span class=n>TrainerIO</span><span class=p>(</span>
    <span class=n>log_dir</span><span class=o>=</span><span class=s2>&#34;./logs/&#34;</span><span class=p>,</span> <span class=n>experiment_name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;experiment1_onecycle_lr</span><span class=si>{</span><span class=n>recommended_lr</span><span class=si>}</span><span class=s2>_adamw&#34;</span><span class=p>,</span> 
    <span class=n>checkpoint_condition</span><span class=o>=</span><span class=n>HistoryCondition</span><span class=p>(</span>
        <span class=s1>&#39;avg.val.acc1&#39;</span><span class=p>,</span> 
        <span class=k>lambda</span> <span class=n>hist</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>hist</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>or</span> <span class=n>hist</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=nb>max</span><span class=p>(</span><span class=n>hist</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
    <span class=p>)</span>
<span class=p>)</span>

<span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span><span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>trainer_io</span><span class=o>=</span><span class=n>trainer_io</span><span class=p>,</span> <span class=n>continue_training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span>
    <span class=n>start_epoch</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>end_epoch</span><span class=o>=</span><span class=n>num_epochs</span>
<span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><p>In order to run the trainer the user has to define TrainerIO, model, scheduler and optimizer.
All those parts are interconnected and essential for the process.</p>
<p>We achieve accuracy for top 1 about 69% and for top 5 about 96 %. That is in line with the
values that other people get for that dataset: <a href=https://github.com/fastai/imagenette#imagewoof-leaderboard>Imagewoof Leaderboard</a></p>
<p><img loading=lazy src=/media/2020-11-14/lr_base_avg.val.acc1.svg alt>
<em>top1 accuracy</em></p>
<p><img loading=lazy src=/media/2020-11-14/lr_base_avg.val.acc5.svg alt>
<em>top5 accuracy</em></p>
<h2 id=lr-finder>LR Finder<a hidden class=anchor aria-hidden=true href=#lr-finder>#</a></h2>
<p>One of the greatest things I found in fastai is learning rate finder. It is a technique that
helps to set up the initial (base) learning rate for the models.</p>
<p>The idea is to do the iteration of the training for gradually increasing learning rate.
The learning rate value where we see the fastest descent of the loss is a good chioce.</p>
<p>I am not going to write that LR finder myself and
I use the existing module <a href=https://pypi.org/project/torch-lr-finder/>pytorch-lr-finder</a>,
<a href=https://github.com/davidtvs/pytorch-lr-finder>github</a>.
That module works with standard pytorch concepts.
If I would like to use the LRFinder from fast ai or pytorch lightning, I would be forced to
use their trainers, optimizers etc.</p>
<p>Let&rsquo;s create a new model and optimizer. We start with a very low learning rate: 1e-8. The lr_finder will do 50 iterations
up until lr=10.:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>pretrained</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>        

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>([</span>
    <span class=p>{</span>
        <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;main_model&#39;</span><span class=p>,</span>
        <span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
        <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=mf>1e-8</span><span class=p>,</span>
        <span class=s1>&#39;weight_decay&#39;</span><span class=p>:</span> <span class=mf>1e-4</span><span class=p>,</span>
    <span class=p>}</span>
<span class=p>])</span>

<span class=kn>from</span> <span class=nn>torch_lr_finder</span> <span class=kn>import</span> <span class=n>LRFinder</span>

<span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
<span class=n>lr_finder</span> <span class=o>=</span> <span class=n>LRFinder</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>criterion</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
<span class=n>lr_finder</span><span class=o>.</span><span class=n>range_test</span><span class=p>(</span><span class=n>train_dataloader</span><span class=p>,</span> <span class=n>val_loader</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>end_lr</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>num_iter</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>step_mode</span><span class=o>=</span><span class=s2>&#34;exp&#34;</span><span class=p>)</span>
<span class=n>_</span><span class=p>,</span> <span class=n>recommended_lr</span> <span class=o>=</span> <span class=n>lr_finder</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>log_lr</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
<span class=n>lr_finder</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</code></pre></td></tr></table>
</div>
</div><p><img loading=lazy src=/media/2020-11-14/lr_finder.png alt>
</p>
<p>My recommended LR is about 0.02. I can launch with the same optimizer, lr schedule, but with the new base LR:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>recommended_lr</span> <span class=o>=</span> <span class=mf>0.02</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>pretrained</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>        

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>([</span>
    <span class=p>{</span>
        <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;main_model&#39;</span><span class=p>,</span>
        <span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
        <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=n>recommended_lr</span><span class=p>,</span>
        <span class=s1>&#39;momentum&#39;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span>
        <span class=s1>&#39;weight_decay&#39;</span><span class=p>:</span> <span class=mf>1e-4</span><span class=p>,</span>
    <span class=p>}</span>
<span class=p>])</span>

<span class=n>scheduler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>OneCycleLR</span><span class=p>(</span>
    <span class=n>optimizer</span><span class=p>,</span>
    <span class=n>max_lr</span><span class=o>=</span><span class=n>recommended_lr</span><span class=p>,</span>
    <span class=n>epochs</span><span class=o>=</span><span class=n>num_epochs</span><span class=p>,</span>
    <span class=n>steps_per_epoch</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataloader</span><span class=p>)</span>
<span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><p>Here also we have to patch our trainer. Scheduler step now happens for each training batch, not per epoch.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>class</span> <span class=nc>Trainer</span><span class=p>:</span>
    <span class=c1># .....</span>
        
    <span class=k>def</span> <span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>start_epoch</span><span class=p>,</span> <span class=n>end_epoch</span><span class=p>):</span>
        <span class=c1># ....</span>
        
        <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>pbars</span><span class=o>.</span><span class=n>main</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>start_epoch</span><span class=p>,</span> <span class=n>end_epoch</span><span class=p>)):</span>
            <span class=c1># .....</span>
            <span class=bp>self</span><span class=o>.</span><span class=n>trainer_io</span><span class=o>.</span><span class=n>save_checkpoint</span><span class=p>(</span><span class=n>aggregated</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>epoch</span><span class=p>)</span>
            
            <span class=c1># not needed any more</span>
            <span class=c1># scheduler.step() </span>
            
        <span class=n>metric_logger</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
            
    <span class=k>def</span> <span class=nf>train_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>data_loader</span><span class=p>,</span> <span class=n>progr</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span> <span class=n>metric_proc</span><span class=p>,</span> <span class=n>pbars</span><span class=p>,</span> <span class=n>report_step</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
                   <span class=n>tb_writer</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
        <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
                
        <span class=k>for</span> <span class=n>batch_idx</span><span class=p>,</span> <span class=p>(</span><span class=n>images</span><span class=p>,</span> <span class=n>target</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>progr</span><span class=o>.</span><span class=n>track</span><span class=p>(</span><span class=n>pbars</span><span class=o>.</span><span class=n>secondary</span><span class=p>(</span><span class=n>data_loader</span><span class=p>))):</span>
            <span class=c1># ...</span>
                 
            <span class=c1># now we do scheduler step for every batch</span>
            <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span> 

    <span class=k>def</span> <span class=nf>validate_epoch</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>data_loader</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>metric_proc</span><span class=p>,</span> <span class=n>pbars</span><span class=p>):</span>
        <span class=c1># stays the same</span>
        <span class=c1># ...</span>
</code></pre></td></tr></table>
</div>
</div><p>Let&rsquo;s run:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>trainer_io</span> <span class=o>=</span> <span class=n>TrainerIO</span><span class=p>(</span>
    <span class=n>log_dir</span><span class=o>=</span><span class=s2>&#34;./logs/&#34;</span><span class=p>,</span> <span class=n>experiment_name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;experiment_base_lr</span><span class=si>{</span><span class=n>recommended_lr</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span> 
    <span class=n>checkpoint_condition</span><span class=o>=</span><span class=n>HistoryCondition</span><span class=p>(</span>
        <span class=s1>&#39;avg.val.acc1&#39;</span><span class=p>,</span> 
        <span class=k>lambda</span> <span class=n>hist</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>hist</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>or</span> <span class=n>hist</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=nb>max</span><span class=p>(</span><span class=n>hist</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
    <span class=p>)</span>
<span class=p>)</span>

<span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span><span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>trainer_io</span><span class=o>=</span><span class=n>trainer_io</span><span class=p>,</span> <span class=n>continue_training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span>
    <span class=n>start_epoch</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>end_epoch</span><span class=o>=</span><span class=n>num_epochs</span>
<span class=p>)</span>

</code></pre></td></tr></table>
</div>
</div><p><img loading=lazy src=/media/2020-11-14/lr_base_vs_finder_avg.train.loss.svg alt="training loss after lr finder">
<em>train loss. lr=0.1 vs lr=0.02 found by LRFinder</em></p>
<p>Top 1 Accuracy improved significantly from 0.69 to 0.78!</p>
<p><img loading=lazy src=/media/2020-11-14/lr_base_vs_finder_avg.val.acc1.svg alt="top 1 accuracy after lr finder">
<em>top 1 accuracy on validation. lr=0.1 vs lr=0.02 found by LRFinder</em></p>
<h2 id=one-cycle-learning-rate>One Cycle learning rate<a hidden class=anchor aria-hidden=true href=#one-cycle-learning-rate>#</a></h2>
<p>Another technique is to have a special LR schedule. We start small, ramp it up quickly to the maximum and then
gradually decrease.</p>
<p>Here is how the LR graphs look like:</p>
<p><img loading=lazy src=/media/2020-11-14/lr_one_cycle.svg alt="learning rate schedules">
<em>learning rate schedules. Gray line is a &ldquo;one cycle&rdquo;</em></p>
<p>We don&rsquo;t get accuracy increase here. But we don&rsquo;t need to hand craft the learning rate schedule any more.
That is positive.</p>
<p>Hopefully one cycle brings some numerical stability.</p>
<p><img loading=lazy src=/media/2020-11-14/one_cycle_avg.val.acc1.svg alt="accuracy top 1 for one cycle">
<em>Accuracy top 1 for one cycle. Gray line is a &ldquo;one cycle&rdquo;</em></p>
<h2 id=adamw>AdamW<a hidden class=anchor aria-hidden=true href=#adamw>#</a></h2>
<p>Instead of SGD let&rsquo;s use Adam optimizer. More specifically AdamW version of it. It was also advertised
in fastai as a good practice.</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>model</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>resnet18</span><span class=p>(</span><span class=n>pretrained</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>        

<span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>([</span>
    <span class=p>{</span>
        <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=s1>&#39;main_model&#39;</span><span class=p>,</span>
        <span class=s1>&#39;params&#39;</span><span class=p>:</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span>
        <span class=s1>&#39;lr&#39;</span><span class=p>:</span> <span class=n>recommended_lr</span><span class=p>,</span>
        <span class=s1>&#39;weight_decay&#39;</span><span class=p>:</span> <span class=mf>1e-4</span><span class=p>,</span>
    <span class=p>}</span>
<span class=p>])</span>


<span class=n>scheduler</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>OneCycleLR</span><span class=p>(</span>
    <span class=n>optimizer</span><span class=p>,</span>
    <span class=n>max_lr</span><span class=o>=</span><span class=n>recommended_lr</span><span class=p>,</span>
    <span class=n>epochs</span><span class=o>=</span><span class=n>num_epochs</span><span class=p>,</span>
    <span class=n>steps_per_epoch</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>train_dataloader</span><span class=p>)</span>
<span class=p>)</span>

<span class=n>trainer_io</span> <span class=o>=</span> <span class=n>TrainerIO</span><span class=p>(</span>
    <span class=n>log_dir</span><span class=o>=</span><span class=s2>&#34;./logs/&#34;</span><span class=p>,</span> <span class=n>experiment_name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&#34;experiment1_onecycle_lr</span><span class=si>{</span><span class=n>recommended_lr</span><span class=si>}</span><span class=s2>_adamw&#34;</span><span class=p>,</span> 
    <span class=n>checkpoint_condition</span><span class=o>=</span><span class=n>HistoryCondition</span><span class=p>(</span>
        <span class=s1>&#39;avg.val.acc1&#39;</span><span class=p>,</span> 
        <span class=k>lambda</span> <span class=n>hist</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>hist</span><span class=p>)</span> <span class=o>==</span> <span class=mi>1</span> <span class=ow>or</span> <span class=n>hist</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>&gt;</span> <span class=nb>max</span><span class=p>(</span><span class=n>hist</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
    <span class=p>)</span>
<span class=p>)</span>

<span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span><span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> <span class=n>trainer_io</span><span class=o>=</span><span class=n>trainer_io</span><span class=p>,</span> <span class=n>continue_training</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>

<span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span>
    <span class=n>model</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>,</span> <span class=n>scheduler</span><span class=p>,</span>
    <span class=n>start_epoch</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>end_epoch</span><span class=o>=</span><span class=n>num_epochs</span>
<span class=p>)</span>

</code></pre></td></tr></table>
</div>
</div><p>That helps a lot. We go from 79.5% to 82.6% of accuracy:</p>
<p><img loading=lazy src=/media/2020-11-14/adamw_avg.val.acc1.svg alt="accuracy top 1 for one cycle">
<em>Accuracy top 1 for AdamW. Dark red line is a AdamW</em></p>
<p><img loading=lazy src=/media/2020-11-14/adamw_avg.train.loss.svg alt="train loss for one cycle">
<em>Train loss for AdamW. Dark red line is a AdamW</em></p>
<h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2>
<p>We used a fraction of tricks from fastai to improve the performance of our training. They all seem to be useful.
Only OneCycle scheduler didn&rsquo;t bring an evident benefits in the accuracy. However it brings some simplicity
for developer.<br>
The numbers are still pretty far from the values on the leaderboard, but we didn&rsquo;t use all the tricks of course.</p>
<p>Also here we verified the viability of pytorch-nn-tools library.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://serge-m.github.io/tags/pytorch/>pytorch</a></li>
<li><a href=https://serge-m.github.io/tags/deep-learning/>deep learning</a></li>
<li><a href=https://serge-m.github.io/tags/computer-vision/>computer vision</a></li>
<li><a href=https://serge-m.github.io/tags/neural-networks/>neural networks</a></li>
<li><a href=https://serge-m.github.io/tags/fast.ai/>fast.ai</a></li>
<li><a href=https://serge-m.github.io/tags/fastai/>fastai</a></li>
<li><a href=https://serge-m.github.io/tags/adamw/>AdamW</a></li>
<li><a href=https://serge-m.github.io/tags/learning-rate/>learning rate</a></li>
<li><a href=https://serge-m.github.io/tags/lrfinder/>LRfinder</a></li>
<li><a href=https://serge-m.github.io/tags/pytorch-nn-tools/>pytorch-nn-tools</a></li>
<li><a href=https://serge-m.github.io/tags/resnet/>resnet</a></li>
<li><a href=https://serge-m.github.io/tags/one-cycle/>one cycle</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://serge-m.github.io/posts/symbolic-math-and-python/>
<span class=title>« Prev Page</span>
<br>
<span>Symbolic math and python</span>
</a>
<a class=next href=https://serge-m.github.io/posts/point-cloud-processing/>
<span class=title>Next Page »</span>
<br>
<span>Point cloud processing</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Effect of techniques from Fast.ai on twitter" href="https://twitter.com/intent/tweet/?text=Effect%20of%20techniques%20from%20Fast.ai&url=https%3a%2f%2fserge-m.github.io%2fposts%2feffect-of-techniques-from-fastai%2f&hashtags=pytorch%2cdeeplearning%2ccomputervision%2cneuralnetworks%2cfast.ai%2cfastai%2cAdamW%2clearningrate%2cLRfinder%2cpytorch-nn-tools%2cresnet%2conecycle"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Effect of techniques from Fast.ai on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fserge-m.github.io%2fposts%2feffect-of-techniques-from-fastai%2f&title=Effect%20of%20techniques%20from%20Fast.ai&summary=Effect%20of%20techniques%20from%20Fast.ai&source=https%3a%2f%2fserge-m.github.io%2fposts%2feffect-of-techniques-from-fastai%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Effect of techniques from Fast.ai on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fserge-m.github.io%2fposts%2feffect-of-techniques-from-fastai%2f&title=Effect%20of%20techniques%20from%20Fast.ai"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Effect of techniques from Fast.ai on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fserge-m.github.io%2fposts%2feffect-of-techniques-from-fastai%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Effect of techniques from Fast.ai on whatsapp" href="https://api.whatsapp.com/send?text=Effect%20of%20techniques%20from%20Fast.ai%20-%20https%3a%2f%2fserge-m.github.io%2fposts%2feffect-of-techniques-from-fastai%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Effect of techniques from Fast.ai on telegram" href="https://telegram.me/share/url?text=Effect%20of%20techniques%20from%20Fast.ai&url=https%3a%2f%2fserge-m.github.io%2fposts%2feffect-of-techniques-from-fastai%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://serge-m.github.io/>sergem's personal public notebook</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)">
<button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script defer src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
</body>
</html>