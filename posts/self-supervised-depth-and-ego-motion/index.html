<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Self-supervised depth and ego motion estimation | sergem&#39;s personal public notebook</title>
<meta name="keywords" content="machine learning, computer vision, DNN, CNN, deep learning, depth, ego motion." />
<meta name="description" content="3D Packing for Self-Supervised Monocular Depth Estimation by Vitor Guizilini, pdf at arxiv, 2020
Learning
Depth estimator fD : I → D Ego motion estimator: fx : (It, IS) → xt → S  Depth Estimator They predict an inverse depth and use a packnet architecture.
Inverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have more information">
<meta name="author" content="SergeM">
<link rel="canonical" href="https://serge-m.github.io/posts/self-supervised-depth-and-ego-motion/" />
<link href="/assets/css/stylesheet.min.4a7c1baa41934a41353b5c02dd9ecd335d4e6b47f3fa5a187a77574ac682b8ac.css" integrity="sha256-SnwbqkGTSkE1O1wC3Z7NM11Oa0fz&#43;loYendXSsaCuKw=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://serge-m.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://serge-m.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://serge-m.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://serge-m.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://serge-m.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.82.0" />

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-40853494-2', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="Self-supervised depth and ego motion estimation" />
<meta property="og:description" content="3D Packing for Self-Supervised Monocular Depth Estimation by Vitor Guizilini, pdf at arxiv, 2020
Learning
Depth estimator fD : I → D Ego motion estimator: fx : (It, IS) → xt → S  Depth Estimator They predict an inverse depth and use a packnet architecture.
Inverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have more information" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://serge-m.github.io/posts/self-supervised-depth-and-ego-motion/" /><meta property="og:image" content="https://serge-m.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-08-23T19:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-08-23T19:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://serge-m.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Self-supervised depth and ego motion estimation"/>
<meta name="twitter:description" content="3D Packing for Self-Supervised Monocular Depth Estimation by Vitor Guizilini, pdf at arxiv, 2020
Learning
Depth estimator fD : I → D Ego motion estimator: fx : (It, IS) → xt → S  Depth Estimator They predict an inverse depth and use a packnet architecture.
Inverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have more information"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://serge-m.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Self-supervised depth and ego motion estimation",
      "item": "https://serge-m.github.io/posts/self-supervised-depth-and-ego-motion/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Self-supervised depth and ego motion estimation",
  "name": "Self-supervised depth and ego motion estimation",
  "description": "3D Packing for Self-Supervised Monocular Depth Estimation by Vitor Guizilini, pdf at arxiv, 2020\nLearning\nDepth estimator fD : I → D Ego motion estimator: fx : (It, IS) → xt → S  Depth Estimator They predict an inverse depth and use a packnet architecture.\nInverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have more information",
  "keywords": [
    "machine learning", "computer vision", "DNN", "CNN", "deep learning", "depth", "ego motion."
  ],
  "articleBody": "3D Packing for Self-Supervised Monocular Depth Estimation by Vitor Guizilini, pdf at arxiv, 2020\nLearning\nDepth estimator fD : I → D Ego motion estimator: fx : (It, IS) → xt → S  Depth Estimator They predict an inverse depth and use a packnet architecture.\nInverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have more information\nProbably they assume that the scene is rigid and there is no moving objects. It is likely to give some errors for the moving objects. How do they deal with the moving objects?\n Ego motion estimator They use a rather simple CNN from SfMLearner.\n Loss function Loss function consists of three parts. 1. appearance loss 2. depth smoothness loss 3. velocity scaling loss\n  SfMLearner paper: Unsupervised Learning of Depth and Ego-Motion from Video by Berkley and google\npdf\nOne of the previous works, that became a foundation for 3d-pack.\nOfficial website.\nGithub: https://github.com/tinghuiz/SfMLearner.\nImplementation in pytorch: SfmLearner-Pytorch\n  Depth from Videos in the Wild Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras by Ariel Gordon et al, 2019.\nhttps://research.google/pubs/pub48440/\npdf\ngithub\nHere they learn not only depth and R/t but also intrinsics of the camera\n struct2depth Depth Prediction Without the Sensors: Leveraging Structure for UnsupervisedLearning from Monocular Videos\nPaper by google\nsite\ncode\nbased on vid2depth\n vid2depth another paper by google.\ngithub\nbased on sfm learner\n monodepth2 - Digging Into Self-Supervised Monocular Depth Estimation arxiv\ngithub\nsome method that 3D Packing use as a competitor.\n RealMonoDepth: Self-Supervised Monocular Depth Estimation for General Scenes self-supervised from stereo and mono.\nby deepai: https://deepai.org/publication/realmonodepth-self-supervised-monocular-depth-estimation-for-general-scenes.\nThey claim to be better than monodepth2 and mode generalized than depth_from_video_in_the_wild \"Depth from Videos in the Wild\". However they require camera calibration and median depth to be estimated prior to processing with an external tool. (COLMAP). Not really \"in the wild\".\nThey were able to train on multiple scenes with different depth ranges. The method still requires a static scene for the training. For example they use data from Mannequin Challenge to train their models. Then the network can be used on dynamic scenes with people.\nThe code is not available so far.\n ",
  "wordCount" : "370",
  "inLanguage": "en",
  "datePublished": "2020-08-23T19:00:00Z",
  "dateModified": "2020-08-23T19:00:00Z",
  "author":{
    "@type": "Person",
    "name": "SergeM"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://serge-m.github.io/posts/self-supervised-depth-and-ego-motion/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "sergem's personal public notebook",
    "logo": {
      "@type": "ImageObject",
      "url": "https://serge-m.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://serge-m.github.io/" accesskey="h" title="Home (Alt + H)">Home</a>
            <span class="logo-switches">
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://serge-m.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://serge-m.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://serge-m.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Self-supervised depth and ego motion estimation
    </h1>
    <div class="post-meta">August 23, 2020&nbsp;·&nbsp;SergeM
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#" aria-label="3D Packing for Self-Supervised Monocular Depth Estimation">3D Packing for Self-Supervised Monocular Depth Estimation</a><ul>
                        
                <li>
                    <a href="#" aria-label="Depth Estimator">Depth Estimator</a></li>
                <li>
                    <a href="#" aria-label="Ego motion estimator">Ego motion estimator</a></li>
                <li>
                    <a href="#" aria-label="Loss function">Loss function</a></li></ul>
                </li>
                <li>
                    <a href="#" aria-label="SfMLearner">SfMLearner</a></li>
                <li>
                    <a href="#" aria-label="Depth from Videos in the Wild">Depth from Videos in the Wild</a></li>
                <li>
                    <a href="#" aria-label="struct2depth">struct2depth</a></li>
                <li>
                    <a href="#" aria-label="vid2depth">vid2depth</a></li>
                <li>
                    <a href="#" aria-label="monodepth2 - Digging Into Self-Supervised Monocular Depth Estimation">monodepth2 - Digging Into Self-Supervised Monocular Depth Estimation</a></li>
                <li>
                    <a href="#" aria-label="RealMonoDepth: Self-Supervised Monocular Depth Estimation for General Scenes">RealMonoDepth: Self-Supervised Monocular Depth Estimation for General Scenes</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><div class="document">


<div class="section" id="d-packing-for-self-supervised-monocular-depth-estimation">
<h2>3D Packing for Self-Supervised Monocular Depth Estimation</h2>
<p>by Vitor Guizilini, <a class="reference external" href="https://arxiv.org/pdf/1905.02693.pdf">pdf at arxiv</a>, 2020</p>
<p>Learning</p>
<ol class="arabic simple">
<li>Depth estimator <span class="formula"><i>f</i><sub><i>D</i></sub> : <i>I</i> → <i>D</i></span></li>
<li>Ego motion estimator: <span class="formula"><i>f</i><sub><i>x</i></sub> : (<i>I</i><sub><i>t</i></sub>, <i>I</i><sub><i>S</i></sub>) → <i>x</i><sub><i>t</i> → <i>S</i></sub></span></li>
</ol>
<div class="section" id="depth-estimator">
<h3>Depth Estimator</h3>
<p>They predict an inverse depth and use a packnet architecture.</p>
<p>Inverse depth probably has more stable results. Points far away from camera have small inverse depth that with low precision. The nearer points have  more information</p>
<p>Probably they assume that the scene is rigid and there is no moving objects. It is likely to give some errors for the moving objects. How do they deal with the moving objects?</p>
</div>
<div class="section" id="ego-motion-estimator">
<h3>Ego motion estimator</h3>
<p>They use a rather simple CNN from SfMLearner.</p>
</div>
<div class="section" id="loss-function">
<h3>Loss function</h3>
<p>Loss function consists of three parts.
1. appearance loss
2. depth smoothness loss
3. velocity scaling loss</p>
</div>
</div>
<div class="section" id="sfmlearner">
<h2>SfMLearner</h2>
<p>paper: Unsupervised Learning of Depth and Ego-Motion from Video
by Berkley and google</p>
<p><a class="reference external" href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf">pdf</a></p>
<img alt="/media/2020-08-23/sfm-teaser.png" src="/media/2020-08-23/sfm-teaser.png" style="width: 600px;" />
<p>One of the previous works, that became a foundation for 3d-pack.</p>
<p><a class="reference external" href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/">Official website</a>.</p>
<p>Github: <a class="reference external" href="https://github.com/tinghuiz/SfMLearner">https://github.com/tinghuiz/SfMLearner</a>.</p>
<p>Implementation in pytorch:
<a class="reference external" href="https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/models/PoseExpNet.py">SfmLearner-Pytorch</a></p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/RTFatijYcaU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div></div>
<div class="section" id="depth-from-videos-in-the-wild">
<h2>Depth from Videos in the Wild</h2>
<p>Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras
by Ariel Gordon et al, 2019.</p>
<p><a class="reference external" href="https://research.google/pubs/pub48440/">https://research.google/pubs/pub48440/</a></p>
<p><a class="reference external" href="https://research.google/pubs/pub48440.pdf">pdf</a></p>
<p><a class="reference external" href="https://github.com/google-research/google-research/tree/master/depth_from_video_in_the_wild">github</a></p>
<p>Here they learn not only depth and R/t but also intrinsics of the camera</p>
</div>
<div class="section" id="struct2depth">
<h2>struct2depth</h2>
<p>Depth Prediction Without the Sensors: Leveraging Structure for UnsupervisedLearning from Monocular Videos</p>
<p>Paper by google</p>
<p><a class="reference external" href="https://sites.google.com/view/struct2depth">site</a></p>
<p><a class="reference external" href="https://github.com/tensorflow/models/tree/archive/research/struct2depth">code</a></p>
<p>based on vid2depth</p>
</div>
<div class="section" id="vid2depth">
<h2>vid2depth</h2>
<p>another paper by google.</p>
<p><a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/vid2depth">github</a></p>
<p>based on sfm learner</p>
</div>
<div class="section" id="monodepth2-digging-into-self-supervised-monocular-depth-estimation">
<h2>monodepth2 - Digging Into Self-Supervised Monocular Depth Estimation</h2>
<p><a class="reference external" href="https://arxiv.org/pdf/1806.01260.pdf">arxiv</a></p>
<p><a class="reference external" href="https://github.com/nianticlabs/monodepth2">github</a></p>
<p>some method that 3D Packing use as a competitor.</p>
</div>
<div class="section" id="realmonodepth-self-supervised-monocular-depth-estimation-for-general-scenes">
<h2>RealMonoDepth: Self-Supervised Monocular Depth Estimation for General Scenes</h2>
<p>self-supervised from stereo and mono.</p>
<p>by deepai:
<a class="reference external" href="https://deepai.org/publication/realmonodepth-self-supervised-monocular-depth-estimation-for-general-scenes">https://deepai.org/publication/realmonodepth-self-supervised-monocular-depth-estimation-for-general-scenes</a>.</p>
<p>They claim to be better than monodepth2 and mode generalized than depth_from_video_in_the_wild &quot;Depth from Videos in the Wild&quot;.
However they require camera calibration and median depth to be estimated prior to processing with an external tool.
(<a class="reference external" href="https://colmap.github.io/">COLMAP</a>). Not really &quot;in the wild&quot;.</p>
<p>They were able to train on multiple scenes with different depth ranges.
The method still requires a static scene for the training. For example they use data from Mannequin Challenge
to train their models. Then the network can be used on dynamic scenes with people.</p>
<p>The code is not available so far.</p>
</div>
</div>
</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://serge-m.github.io/tags/machine-learning/">machine learning</a></li>
      <li><a href="https://serge-m.github.io/tags/computer-vision/">computer vision</a></li>
      <li><a href="https://serge-m.github.io/tags/dnn/">DNN</a></li>
      <li><a href="https://serge-m.github.io/tags/cnn/">CNN</a></li>
      <li><a href="https://serge-m.github.io/tags/deep-learning/">deep learning</a></li>
      <li><a href="https://serge-m.github.io/tags/depth/">depth</a></li>
    </ul>
    <nav class="paginav">
      <a class="prev" href="https://serge-m.github.io/posts/python-multiprocessing/">
        <span class="title">« Prev Page</span>
        <br>
        <span>Python - Multiprocessing</span>
      </a>
      <a class="next" href="https://serge-m.github.io/posts/which-backbone-to-choose/">
        <span class="title">Next Page »</span>
        <br>
        <span>Which pretrained backbone to choose</span>
      </a>
    </nav>
<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Self-supervised depth and ego motion estimation on twitter"
        href="https://twitter.com/intent/tweet/?text=Self-supervised%20depth%20and%20ego%20motion%20estimation&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fself-supervised-depth-and-ego-motion%2f&amp;hashtags=machinelearning%2ccomputervision%2cDNN%2cCNN%2cdeeplearning%2cdepth%2cegomotion.">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Self-supervised depth and ego motion estimation on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fself-supervised-depth-and-ego-motion%2f&amp;title=Self-supervised%20depth%20and%20ego%20motion%20estimation&amp;summary=Self-supervised%20depth%20and%20ego%20motion%20estimation&amp;source=https%3a%2f%2fserge-m.github.io%2fposts%2fself-supervised-depth-and-ego-motion%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Self-supervised depth and ego motion estimation on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fserge-m.github.io%2fposts%2fself-supervised-depth-and-ego-motion%2f&title=Self-supervised%20depth%20and%20ego%20motion%20estimation">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Self-supervised depth and ego motion estimation on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fserge-m.github.io%2fposts%2fself-supervised-depth-and-ego-motion%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Self-supervised depth and ego motion estimation on whatsapp"
        href="https://api.whatsapp.com/send?text=Self-supervised%20depth%20and%20ego%20motion%20estimation%20-%20https%3a%2f%2fserge-m.github.io%2fposts%2fself-supervised-depth-and-ego-motion%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Self-supervised depth and ego motion estimation on telegram"
        href="https://telegram.me/share/url?text=Self-supervised%20depth%20and%20ego%20motion%20estimation&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fself-supervised-depth-and-ego-motion%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://serge-m.github.io/">sergem&#39;s personal public notebook</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>
<script defer src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>

<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>
