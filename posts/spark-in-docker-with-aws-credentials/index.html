<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Spark in Docker with AWS credentials | sergem's personal public notebook</title><meta name=keywords content="spark,python,aws"><meta name=description content="Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.
Docker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark
Examples Running SparkPi sample program (one of the examples from the docs of Spark):
docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:"><meta name=author content="SergeM"><link rel=canonical href=https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/><link href=/assets/css/stylesheet.min.6d98a2276d0cb41ef459267b3ff3ef02df70a8f16b70bbc52b20568702bc90cf.css integrity="sha256-bZiiJ20MtB70WSZ7P/PvAt9wqPFrcLvFKyBWhwK8kM8=" rel="preload stylesheet" as=style><link rel=icon href=https://serge-m.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://serge-m.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://serge-m.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://serge-m.github.io/apple-touch-icon.png><link rel=mask-icon href=https://serge-m.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.113.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-40853494-2","auto"),ga("send","pageview"))</script><meta property="og:title" content="Spark in Docker with AWS credentials"><meta property="og:description" content="Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.
Docker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark
Examples Running SparkPi sample program (one of the examples from the docs of Spark):
docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:"><meta property="og:type" content="article"><meta property="og:url" content="https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/"><meta property="og:image" content="https://serge-m.github.io/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-07-29T22:07:00+00:00"><meta property="article:modified_time" content="2018-07-29T22:07:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://serge-m.github.io/papermod-cover.png"><meta name=twitter:title content="Spark in Docker with AWS credentials"><meta name=twitter:description content="Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.
Docker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark
Examples Running SparkPi sample program (one of the examples from the docs of Spark):
docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://serge-m.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Spark in Docker with AWS credentials","item":"https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Spark in Docker with AWS credentials","name":"Spark in Docker with AWS credentials","description":"Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.\nDocker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark\nExamples Running SparkPi sample program (one of the examples from the docs of Spark):\ndocker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:","keywords":["spark","python","aws"],"articleBody":"Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.\nDocker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark\nExamples Running SparkPi sample program (one of the examples from the docs of Spark):\ndocker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:\necho -e \"import pyspark\\n\\nprint(pyspark.SparkContext().parallelize(range(0, 10)).count())\" \u003e count.py docker run --rm -it -p 4040:4040 -v $(pwd)/count.py:/count.py gettyimages/spark bin/spark-submit /count.py Here we create a file with a python program outside of the docker. During docker run we map this file to the file inside the docker container with path /count.py and the we execute bin/spark-submit command that executes our code.\nYou can also run PySpark in interactive mode:\n$ docker run --rm -it -p 4040:4040 gettyimages/spark bin/pyspark Python 3.5.3 (default, Jan 19 2017, 14:11:04) [GCC 6.3.0 20170118] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. 2018-07-29 20:03:59 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.3.1 /_/ Using Python version 3.5.3 (default, Jan 19 2017 14:11:04) SparkSession available as 'spark'. \u003e\u003e\u003e sc \u003e\u003e\u003e Now you can enter commands and evaluate your code in interactive mode.\nRunning a cluster with docker-compose One can use docker-compose.yaml file from https://github.com/gettyimages/docker-spark.git to run a cluster locally.\ndocker-compose.yaml file looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 master: image: gettyimages/spark command: bin/spark-class org.apache.spark.deploy.master.Master -h master hostname: master environment: MASTER: spark://master:7077 SPARK_CONF_DIR: /conf SPARK_PUBLIC_DNS: localhost expose: - 7001 - 7002 - 7003 - 7004 - 7005 - 7006 - 7077 - 6066 ports: - 4040:4040 - 6066:6066 - 7077:7077 - 8080:8080 volumes: - ./conf/master:/conf - ./data:/tmp/data worker: image: gettyimages/spark command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077 hostname: worker environment: SPARK_CONF_DIR: /conf SPARK_WORKER_CORES: 2 SPARK_WORKER_MEMORY: 1g SPARK_WORKER_PORT: 8881 SPARK_WORKER_WEBUI_PORT: 8081 SPARK_PUBLIC_DNS: localhost links: - master expose: - 7012 - 7013 - 7014 - 7015 - 7016 - 8881 ports: - 8081:8081 volumes: - ./conf/worker:/conf - ./data:/tmp/data Run it with command.\ndocker-compose up It uses configs for master and worker nodes from conf directory.\nAccessing S3 from local Spark I want to do experiments locally on spark but my data is stored in the cloud - AWS S3. If I deploy spark on EMR credentials are automatically passed to spark from AWS. But locally it is not the case. In the simple case one can use environment variables to pass AWS credentials:\ndocker run --rm -it -e \"AWS_ACCESS_KEY_ID=YOURKEY\" -e \"AWS_SECRET_ACCESS_KEY=YOURSECRET\" -p 4040:4040 gettyimages/spark bin/spark-shell Loading credentials from ~/.aws/credentials If you want to use AWS S3 credentials from ~/.aws/credentials you have to do some configuration. in the previous cluster example one have to specify credentials provider. Add the following line to spark-defaults.conf:\nspark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.profile.ProfileCredentialsProvider Let’s say we want to run some code like this:\n1 2 3 4 5 6 7 8 from pyspark.sql import SparkSession spark = SparkSession \\ .builder \\ .getOrCreate() data = spark.read.parquet(\"s3a://your_bucket/serge-m-test/data.parquet\") data.show() Now if you configure the rest properly and run the cluster you can access your s3 data from local spark/docker container.\nWithout the configuration we would get the following error:\nCaused by: com.amazonaws.AmazonClientException: No AWS Credentials provided by BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint That basically means that spark has three credentials proveders: BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider. But none of them worked.\nEnvironmentVariableCredentialsProvider - one that loads the credentials from environment variables InstanceProfileCredentialsProvider - one that works on AWS instances. BasicAWSCredentialsProvider - I don’t know what it is. What we need is ProfileCredentialsProvider. It reads the credentials from ~/.aws directory.\nMore about spark and aws aws hadoop libraries (copying)\nApache Spark and Amazon S3 — Gotchas and best practices\nabout profile credentials provider 1 about profile credentials provider 2\nabout hadoop aws s3 access\nabout drivers\nanalytics with airflow and spark\n","wordCount":"730","inLanguage":"en","datePublished":"2018-07-29T22:07:00Z","dateModified":"2018-07-29T22:07:00Z","author":{"@type":"Person","name":"SergeM"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/"},"publisher":{"@type":"Organization","name":"sergem's personal public notebook","logo":{"@type":"ImageObject","url":"https://serge-m.github.io/favicon.ico"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://serge-m.github.io/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://serge-m.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://serge-m.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://serge-m.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://serge-m.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://serge-m.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://serge-m.github.io/posts/>Posts</a></div><h1 class=post-title>Spark in Docker with AWS credentials</h1><div class=post-meta>July 29, 2018&nbsp;·&nbsp;SergeM</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#running-spark-in-docker-container aria-label="Running spark in docker container">Running spark in docker container</a><ul><li><a href=#examples aria-label=Examples>Examples</a><ul><li><a href=#running-a-cluster-with-docker-compose aria-label="Running a cluster with docker-compose">Running a cluster with <code>docker-compose</code></a></li></ul></li><li><a href=#accessing-s3-from-local-spark aria-label="Accessing S3 from local Spark">Accessing S3 from local Spark</a><ul><li><a href=#loading-credentials-from-awscredentials aria-label="Loading credentials from ~/.aws/credentials">Loading credentials from <code>~/.aws/credentials</code></a></li></ul></li><li><a href=#more-about-spark-and-aws aria-label="More about spark and aws">More about spark and aws</a></li></ul></li></ul></div></details></div><div class=post-content><h1 id=running-spark-in-docker-container>Running spark in docker container<a hidden class=anchor aria-hidden=true href=#running-spark-in-docker-container>#</a></h1><p>Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.</p><p>Docker is of a good help here.
There is a great docker image to play with spark locally.
<a href=https://github.com/gettyimages/docker-spark/>gettyimages/docker-spark</a></p><h2 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h2><p>Running <code>SparkPi</code> sample program (one of the examples from the docs of Spark):</p><pre tabindex=0><code>docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10
</code></pre><p>Running a small example with Pyspark:</p><pre tabindex=0><code>echo -e &#34;import pyspark\n\nprint(pyspark.SparkContext().parallelize(range(0, 10)).count())&#34; &gt; count.py
docker run --rm -it -p 4040:4040 -v $(pwd)/count.py:/count.py gettyimages/spark bin/spark-submit /count.py
</code></pre><p>Here we create a file with a python program outside of the docker. During <code>docker run</code> we map this file to the file inside the docker container with path <code>/count.py</code> and the we execute <code>bin/spark-submit</code> command that executes our code.</p><p>You can also run PySpark in interactive mode:</p><pre tabindex=0><code>$ docker run --rm -it -p 4040:4040  gettyimages/spark bin/pyspark

Python 3.5.3 (default, Jan 19 2017, 14:11:04) 
[GCC 6.3.0 20170118] on linux
Type &#34;help&#34;, &#34;copyright&#34;, &#34;credits&#34; or &#34;license&#34; for more information.
2018-07-29 20:03:59 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to &#34;WARN&#34;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &#39;_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.1
      /_/

Using Python version 3.5.3 (default, Jan 19 2017 14:11:04)
SparkSession available as &#39;spark&#39;.
&gt;&gt;&gt; sc
&lt;SparkContext master=local[*] appName=PySparkShell&gt;
&gt;&gt;&gt; 
</code></pre><p>Now you can enter commands and evaluate your code in interactive mode.</p><h3 id=running-a-cluster-with-docker-compose>Running a cluster with <code>docker-compose</code><a hidden class=anchor aria-hidden=true href=#running-a-cluster-with-docker-compose>#</a></h3><p>One can use docker-compose.yaml file from <a href=https://github.com/gettyimages/docker-spark.git>https://github.com/gettyimages/docker-spark.git</a> to run a cluster locally.</p><p>docker-compose.yaml file looks like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>master</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>gettyimages/spark</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>bin/spark-class org.apache.spark.deploy.master.Master -h master</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hostname</span><span class=p>:</span><span class=w> </span><span class=l>master</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>MASTER</span><span class=p>:</span><span class=w> </span><span class=l>spark://master:7077</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_CONF_DIR</span><span class=p>:</span><span class=w> </span><span class=l>/conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_PUBLIC_DNS</span><span class=p>:</span><span class=w> </span><span class=l>localhost</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>expose</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7001</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7002</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7003</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7004</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7005</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7006</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7077</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>6066</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>4040</span><span class=p>:</span><span class=m>4040</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>6066</span><span class=p>:</span><span class=m>6066</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7077</span><span class=p>:</span><span class=m>7077</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>8080</span><span class=p>:</span><span class=m>8080</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>./conf/master:/conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>./data:/tmp/data</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>worker</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>gettyimages/spark</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=l>bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hostname</span><span class=p>:</span><span class=w> </span><span class=l>worker</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>environment</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_CONF_DIR</span><span class=p>:</span><span class=w> </span><span class=l>/conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_WORKER_CORES</span><span class=p>:</span><span class=w> </span><span class=m>2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_WORKER_MEMORY</span><span class=p>:</span><span class=w> </span><span class=l>1g</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_WORKER_PORT</span><span class=p>:</span><span class=w> </span><span class=m>8881</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_WORKER_WEBUI_PORT</span><span class=p>:</span><span class=w> </span><span class=m>8081</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>SPARK_PUBLIC_DNS</span><span class=p>:</span><span class=w> </span><span class=l>localhost</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>links</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>master</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>expose</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7012</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7013</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7014</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7015</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>7016</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>8881</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=m>8081</span><span class=p>:</span><span class=m>8081</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>./conf/worker:/conf</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span>- <span class=l>./data:/tmp/data</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>Run it with command.</p><pre tabindex=0><code>docker-compose up
</code></pre><p>It uses configs for master and worker nodes from <code>conf</code> directory.</p><h2 id=accessing-s3-from-local-spark>Accessing S3 from local Spark<a hidden class=anchor aria-hidden=true href=#accessing-s3-from-local-spark>#</a></h2><p>I want to do experiments locally on spark but my data is stored in the cloud - AWS S3. If I deploy spark on EMR credentials are automatically passed to spark from AWS. But locally it is not the case. In the simple case one can use environment variables to pass AWS credentials:</p><pre tabindex=0><code>docker run --rm -it -e &#34;AWS_ACCESS_KEY_ID=YOURKEY&#34; -e &#34;AWS_SECRET_ACCESS_KEY=YOURSECRET&#34; -p 4040:4040 gettyimages/spark bin/spark-shell
</code></pre><h3 id=loading-credentials-from-awscredentials>Loading credentials from <code>~/.aws/credentials</code><a hidden class=anchor aria-hidden=true href=#loading-credentials-from-awscredentials>#</a></h3><p>If you want to use AWS S3 credentials from <code>~/.aws/credentials</code> you have to do some configuration.
in the previous cluster example one have to specify credentials provider. Add the following line to <code>spark-defaults.conf</code>:</p><pre tabindex=0><code>spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.profile.ProfileCredentialsProvider
</code></pre><p>Let&rsquo;s say we want to run some code like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>builder</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>parquet</span><span class=p>(</span><span class=s2>&#34;s3a://your_bucket/serge-m-test/data.parquet&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>data</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>Now if you configure the rest properly and run the cluster you can access your s3 data from local spark/docker container.</p><p>Without the configuration we would get the following error:</p><pre tabindex=0><code>Caused by: com.amazonaws.AmazonClientException: No AWS Credentials provided by BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint
</code></pre><p>That basically means that spark has three credentials proveders: BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider. But none of them worked.</p><ul><li>EnvironmentVariableCredentialsProvider - one that loads the credentials from environment variables</li><li>InstanceProfileCredentialsProvider - one that works on AWS instances.</li><li>BasicAWSCredentialsProvider - I don&rsquo;t know what it is.</li></ul><p>What we need is ProfileCredentialsProvider. It reads the credentials from <code>~/.aws</code> directory.</p><h2 id=more-about-spark-and-aws>More about spark and aws<a hidden class=anchor aria-hidden=true href=#more-about-spark-and-aws>#</a></h2><p><a href=https://github.com/dimajix/docker-hadoop/blob/master/Dockerfile#L23>aws hadoop libraries (copying)</a></p><p><a href=https://medium.com/@subhojit20_27731/apache-spark-and-amazon-s3-gotchas-and-best-practices-a767242f3d98>Apache Spark and Amazon S3 — Gotchas and best practices</a></p><p><a href=https://aws.amazon.com/blogs/developer/secure-local-development-with-the-profilecredentialsprovider/>about profile credentials provider 1</a>
<a href=https://stackoverflow.com/questions/42669246/spark-is-inventing-his-own-aws-secretkey>about profile credentials provider 2</a></p><p><a href=https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Features>about hadoop aws s3 access</a></p><p><a href=https://spark.apache.org/docs/latest/cluster-overview.html>about drivers</a></p><p><a href=https://www.slideshare.net/rjurney/predictive-analytics-with-airflow-and-pyspark>analytics with airflow and spark</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://serge-m.github.io/tags/spark/>spark</a></li><li><a href=https://serge-m.github.io/tags/python/>python</a></li><li><a href=https://serge-m.github.io/tags/aws/>aws</a></li></ul><nav class=paginav><a class=prev href=https://serge-m.github.io/posts/disk-usage-ubuntu/><span class=title>« Prev Page</span><br><span>Reducing disk usage in Ubuntu</span></a>
<a class=next href=https://serge-m.github.io/posts/okrs/><span class=title>Next Page »</span><br><span>OKRs</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on twitter" href="https://twitter.com/intent/tweet/?text=Spark%20in%20Docker%20with%20AWS%20credentials&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f&amp;hashtags=spark%2cpython%2caws"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f&amp;title=Spark%20in%20Docker%20with%20AWS%20credentials&amp;summary=Spark%20in%20Docker%20with%20AWS%20credentials&amp;source=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f&title=Spark%20in%20Docker%20with%20AWS%20credentials"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on whatsapp" href="https://api.whatsapp.com/send?text=Spark%20in%20Docker%20with%20AWS%20credentials%20-%20https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on telegram" href="https://telegram.me/share/url?text=Spark%20in%20Docker%20with%20AWS%20credentials&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://serge-m.github.io/>sergem's personal public notebook</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>