<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Spark in Docker with AWS credentials | sergem&#39;s personal public notebook</title>
<meta name="keywords" content="spark, python, aws" />
<meta name="description" content="Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.
Docker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark
Examples Running SparkPi sample program (one of the examples from the docs of Spark):
docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:">
<meta name="author" content="SergeM">
<link rel="canonical" href="https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/" />
<link href="/assets/css/stylesheet.min.6d98a2276d0cb41ef459267b3ff3ef02df70a8f16b70bbc52b20568702bc90cf.css" integrity="sha256-bZiiJ20MtB70WSZ7P/PvAt9wqPFrcLvFKyBWhwK8kM8=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://serge-m.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://serge-m.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://serge-m.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://serge-m.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://serge-m.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.92.1" />

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-40853494-2', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="Spark in Docker with AWS credentials" />
<meta property="og:description" content="Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.
Docker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark
Examples Running SparkPi sample program (one of the examples from the docs of Spark):
docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/" /><meta property="og:image" content="https://serge-m.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2018-07-29T22:07:00&#43;00:00" />
<meta property="article:modified_time" content="2018-07-29T22:07:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://serge-m.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Spark in Docker with AWS credentials"/>
<meta name="twitter:description" content="Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.
Docker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark
Examples Running SparkPi sample program (one of the examples from the docs of Spark):
docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://serge-m.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Spark in Docker with AWS credentials",
      "item": "https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Spark in Docker with AWS credentials",
  "name": "Spark in Docker with AWS credentials",
  "description": "Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.\nDocker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark\nExamples Running SparkPi sample program (one of the examples from the docs of Spark):\ndocker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:",
  "keywords": [
    "spark", "python", "aws"
  ],
  "articleBody": "Running spark in docker container Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.\nDocker is of a good help here. There is a great docker image to play with spark locally. gettyimages/docker-spark\nExamples Running SparkPi sample program (one of the examples from the docs of Spark):\ndocker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10 Running a small example with Pyspark:\necho -e \"import pyspark\\n\\nprint(pyspark.SparkContext().parallelize(range(0, 10)).count())\"  count.py docker run --rm -it -p 4040:4040 -v $(pwd)/count.py:/count.py gettyimages/spark bin/spark-submit /count.py Here we create a file with a python program outside of the docker. During docker run we map this file to the file inside the docker container with path /count.py and the we execute bin/spark-submit command that executes our code.\nYou can also run PySpark in interactive mode:\n$ docker run --rm -it -p 4040:4040 gettyimages/spark bin/pyspark Python 3.5.3 (default, Jan 19 2017, 14:11:04) [GCC 6.3.0 20170118] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. 2018-07-29 20:03:59 WARN NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 2.3.1 /_/ Using Python version 3.5.3 (default, Jan 19 2017 14:11:04) SparkSession available as 'spark'.  sc  Now you can enter commands and evaluate your code in interactive mode.\nRunning a cluster with docker-compose One can use docker-compose.yaml file from https://github.com/gettyimages/docker-spark.git to run a cluster locally.\ndocker-compose.yaml file looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  master:image:gettyimages/sparkcommand:bin/spark-class org.apache.spark.deploy.master.Master -h masterhostname:masterenvironment:MASTER:spark://master:7077SPARK_CONF_DIR:/confSPARK_PUBLIC_DNS:localhostexpose:- 7001- 7002- 7003- 7004- 7005- 7006- 7077- 6066ports:- 4040:4040- 6066:6066- 7077:7077- 8080:8080volumes:- ./conf/master:/conf- ./data:/tmp/dataworker:image:gettyimages/sparkcommand:bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077hostname:workerenvironment:SPARK_CONF_DIR:/confSPARK_WORKER_CORES:2SPARK_WORKER_MEMORY:1gSPARK_WORKER_PORT:8881SPARK_WORKER_WEBUI_PORT:8081SPARK_PUBLIC_DNS:localhostlinks:- masterexpose:- 7012- 7013- 7014- 7015- 7016- 8881ports:- 8081:8081volumes:- ./conf/worker:/conf- ./data:/tmp/data  Run it with command.\ndocker-compose up It uses configs for master and worker nodes from conf directory.\nAccessing S3 from local Spark I want to do experiments locally on spark but my data is stored in the cloud - AWS S3. If I deploy spark on EMR credentials are automatically passed to spark from AWS. But locally it is not the case. In the simple case one can use environment variables to pass AWS credentials:\ndocker run --rm -it -e \"AWS_ACCESS_KEY_ID=YOURKEY\" -e \"AWS_SECRET_ACCESS_KEY=YOURSECRET\" -p 4040:4040 gettyimages/spark bin/spark-shell Loading credentials from ~/.aws/credentials If you want to use AWS S3 credentials from ~/.aws/credentials you have to do some configuration. in the previous cluster example one have to specify credentials provider. Add the following line to spark-defaults.conf:\nspark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.profile.ProfileCredentialsProvider Let’s say we want to run some code like this:\n1 2 3 4 5 6 7 8  from pyspark.sql import SparkSession spark = SparkSession \\ .builder \\ .getOrCreate() data = spark.read.parquet(\"s3a://your_bucket/serge-m-test/data.parquet\") data.show()   Now if you configure the rest properly and run the cluster you can access your s3 data from local spark/docker container.\nWithout the configuration we would get the following error:\nCaused by: com.amazonaws.AmazonClientException: No AWS Credentials provided by BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint That basically means that spark has three credentials proveders: BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider. But none of them worked.\n EnvironmentVariableCredentialsProvider - one that loads the credentials from environment variables InstanceProfileCredentialsProvider - one that works on AWS instances. BasicAWSCredentialsProvider - I don’t know what it is.  What we need is ProfileCredentialsProvider. It reads the credentials from ~/.aws directory.\nMore about spark and aws aws hadoop libraries (copying)\nApache Spark and Amazon S3 — Gotchas and best practices\nabout profile credentials provider 1 about profile credentials provider 2\nabout hadoop aws s3 access\nabout drivers\nanalytics with airflow and spark\n",
  "wordCount" : "667",
  "inLanguage": "en",
  "datePublished": "2018-07-29T22:07:00Z",
  "dateModified": "2018-07-29T22:07:00Z",
  "author":{
    "@type": "Person",
    "name": "SergeM"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://serge-m.github.io/posts/spark-in-docker-with-aws-credentials/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "sergem's personal public notebook",
    "logo": {
      "@type": "ImageObject",
      "url": "https://serge-m.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://serge-m.github.io/" accesskey="h" title="Home (Alt + H)">Home</a>
            <span class="logo-switches">
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://serge-m.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://serge-m.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://serge-m.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Spark in Docker with AWS credentials
    </h1>
    <div class="post-meta">July 29, 2018&nbsp;·&nbsp;SergeM
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#running-spark-in-docker-container" aria-label="Running spark in docker container">Running spark in docker container</a><ul>
                        
                <li>
                    <a href="#examples" aria-label="Examples">Examples</a><ul>
                        
                <li>
                    <a href="#running-a-cluster-with-docker-compose" aria-label="Running a cluster with docker-compose">Running a cluster with <code>docker-compose</code></a></li></ul>
                </li>
                <li>
                    <a href="#accessing-s3-from-local-spark" aria-label="Accessing S3 from local Spark">Accessing S3 from local Spark</a><ul>
                        
                <li>
                    <a href="#loading-credentials-from-awscredentials" aria-label="Loading credentials from ~/.aws/credentials">Loading credentials from <code>~/.aws/credentials</code></a></li></ul>
                </li>
                <li>
                    <a href="#more-about-spark-and-aws" aria-label="More about spark and aws">More about spark and aws</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="running-spark-in-docker-container">Running spark in docker container<a hidden class="anchor" aria-hidden="true" href="#running-spark-in-docker-container">#</a></h1>
<p>Setting up spark is tricky. Therefore it is useful to try out things locally before deploying to the cluster.</p>
<p>Docker is of a good help here.
There is a great docker image to play with spark locally.
<a href="https://github.com/gettyimages/docker-spark/">gettyimages/docker-spark</a></p>
<h2 id="examples">Examples<a hidden class="anchor" aria-hidden="true" href="#examples">#</a></h2>
<p>Running <code>SparkPi</code> sample program (one of the examples from the docs of Spark):</p>
<pre tabindex="0"><code>docker run --rm -it -p 4040:4040 gettyimages/spark bin/run-example SparkPi 10
</code></pre><p>Running a small example with Pyspark:</p>
<pre tabindex="0"><code>echo -e &quot;import pyspark\n\nprint(pyspark.SparkContext().parallelize(range(0, 10)).count())&quot; &gt; count.py
docker run --rm -it -p 4040:4040 -v $(pwd)/count.py:/count.py gettyimages/spark bin/spark-submit /count.py
</code></pre><p>Here we create a file with a python program outside of the docker. During <code>docker run</code> we map this file to the file inside the docker container with path <code>/count.py</code> and the we execute <code>bin/spark-submit</code> command that executes our code.</p>
<p>You can also run PySpark in interactive mode:</p>
<pre tabindex="0"><code>$ docker run --rm -it -p 4040:4040  gettyimages/spark bin/pyspark

Python 3.5.3 (default, Jan 19 2017, 14:11:04) 
[GCC 6.3.0 20170118] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
2018-07-29 20:03:59 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.1
      /_/

Using Python version 3.5.3 (default, Jan 19 2017 14:11:04)
SparkSession available as 'spark'.
&gt;&gt;&gt; sc
&lt;SparkContext master=local[*] appName=PySparkShell&gt;
&gt;&gt;&gt; 

</code></pre><p>Now you can enter commands and evaluate your code in interactive mode.</p>
<h3 id="running-a-cluster-with-docker-compose">Running a cluster with <code>docker-compose</code><a hidden class="anchor" aria-hidden="true" href="#running-a-cluster-with-docker-compose">#</a></h3>
<p>One can use docker-compose.yaml file from <a href="https://github.com/gettyimages/docker-spark.git">https://github.com/gettyimages/docker-spark.git</a> to run a cluster locally.</p>
<p>docker-compose.yaml file looks like this:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">master</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">gettyimages/spark</span><span class="w">
</span><span class="w">  </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l">bin/spark-class org.apache.spark.deploy.master.Master -h master</span><span class="w">
</span><span class="w">  </span><span class="nt">hostname</span><span class="p">:</span><span class="w"> </span><span class="l">master</span><span class="w">
</span><span class="w">  </span><span class="nt">environment</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">MASTER</span><span class="p">:</span><span class="w"> </span><span class="l">spark://master:7077</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_CONF_DIR</span><span class="p">:</span><span class="w"> </span><span class="l">/conf</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_PUBLIC_DNS</span><span class="p">:</span><span class="w"> </span><span class="l">localhost</span><span class="w">
</span><span class="w">  </span><span class="nt">expose</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="m">7001</span><span class="w">
</span><span class="w">    </span>- <span class="m">7002</span><span class="w">
</span><span class="w">    </span>- <span class="m">7003</span><span class="w">
</span><span class="w">    </span>- <span class="m">7004</span><span class="w">
</span><span class="w">    </span>- <span class="m">7005</span><span class="w">
</span><span class="w">    </span>- <span class="m">7006</span><span class="w">
</span><span class="w">    </span>- <span class="m">7077</span><span class="w">
</span><span class="w">    </span>- <span class="m">6066</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="m">4040</span><span class="p">:</span><span class="m">4040</span><span class="w">
</span><span class="w">    </span>- <span class="m">6066</span><span class="p">:</span><span class="m">6066</span><span class="w">
</span><span class="w">    </span>- <span class="m">7077</span><span class="p">:</span><span class="m">7077</span><span class="w">
</span><span class="w">    </span>- <span class="m">8080</span><span class="p">:</span><span class="m">8080</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">./conf/master:/conf</span><span class="w">
</span><span class="w">    </span>- <span class="l">./data:/tmp/data</span><span class="w">
</span><span class="w">
</span><span class="w"></span><span class="nt">worker</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">gettyimages/spark</span><span class="w">
</span><span class="w">  </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l">bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077</span><span class="w">
</span><span class="w">  </span><span class="nt">hostname</span><span class="p">:</span><span class="w"> </span><span class="l">worker</span><span class="w">
</span><span class="w">  </span><span class="nt">environment</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_CONF_DIR</span><span class="p">:</span><span class="w"> </span><span class="l">/conf</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_WORKER_CORES</span><span class="p">:</span><span class="w"> </span><span class="m">2</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_WORKER_MEMORY</span><span class="p">:</span><span class="w"> </span><span class="l">1g</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_WORKER_PORT</span><span class="p">:</span><span class="w"> </span><span class="m">8881</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_WORKER_WEBUI_PORT</span><span class="p">:</span><span class="w"> </span><span class="m">8081</span><span class="w">
</span><span class="w">    </span><span class="nt">SPARK_PUBLIC_DNS</span><span class="p">:</span><span class="w"> </span><span class="l">localhost</span><span class="w">
</span><span class="w">  </span><span class="nt">links</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">master</span><span class="w">
</span><span class="w">  </span><span class="nt">expose</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="m">7012</span><span class="w">
</span><span class="w">    </span>- <span class="m">7013</span><span class="w">
</span><span class="w">    </span>- <span class="m">7014</span><span class="w">
</span><span class="w">    </span>- <span class="m">7015</span><span class="w">
</span><span class="w">    </span>- <span class="m">7016</span><span class="w">
</span><span class="w">    </span>- <span class="m">8881</span><span class="w">
</span><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="m">8081</span><span class="p">:</span><span class="m">8081</span><span class="w">
</span><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">    </span>- <span class="l">./conf/worker:/conf</span><span class="w">
</span><span class="w">    </span>- <span class="l">./data:/tmp/data</span><span class="w">
</span><span class="w">
</span></code></pre></td></tr></table>
</div>
</div><p>Run it with command.</p>
<pre tabindex="0"><code>docker-compose up
</code></pre><p>It uses configs for master and worker nodes from <code>conf</code> directory.</p>
<h2 id="accessing-s3-from-local-spark">Accessing S3 from local Spark<a hidden class="anchor" aria-hidden="true" href="#accessing-s3-from-local-spark">#</a></h2>
<p>I want to do experiments locally on spark but my data is stored in the cloud - AWS S3. If I deploy spark on EMR credentials are automatically passed to spark from AWS. But locally it is not the case. In the simple case one can use environment variables to pass AWS credentials:</p>
<pre tabindex="0"><code>docker run --rm -it -e &quot;AWS_ACCESS_KEY_ID=YOURKEY&quot; -e &quot;AWS_SECRET_ACCESS_KEY=YOURSECRET&quot; -p 4040:4040 gettyimages/spark bin/spark-shell
</code></pre><h3 id="loading-credentials-from-awscredentials">Loading credentials from <code>~/.aws/credentials</code><a hidden class="anchor" aria-hidden="true" href="#loading-credentials-from-awscredentials">#</a></h3>
<p>If you want to use AWS S3 credentials from <code>~/.aws/credentials</code> you have to do some configuration.
in the previous cluster example one have to specify credentials provider. Add the following line to <code>spark-defaults.conf</code>:</p>
<pre tabindex="0"><code>spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.profile.ProfileCredentialsProvider
</code></pre><p>Let&rsquo;s say we want to run some code like this:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
    <span class="o">.</span><span class="n">builder</span> \
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&#34;s3a://your_bucket/serge-m-test/data.parquet&#34;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>Now if you configure the rest properly and run the cluster you can access your s3 data from local spark/docker container.</p>
<p>Without the configuration we would get the following error:</p>
<pre tabindex="0"><code>Caused by: com.amazonaws.AmazonClientException: No AWS Credentials provided by BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: Unable to load credentials from service endpoint
</code></pre><p>That basically means that spark has three credentials proveders: BasicAWSCredentialsProvider EnvironmentVariableCredentialsProvider InstanceProfileCredentialsProvider. But none of them worked.</p>
<ul>
<li>EnvironmentVariableCredentialsProvider - one that loads the credentials from environment variables</li>
<li>InstanceProfileCredentialsProvider - one that works on AWS instances.</li>
<li>BasicAWSCredentialsProvider - I don&rsquo;t know what it is.</li>
</ul>
<p>What we need is ProfileCredentialsProvider. It reads the credentials from <code>~/.aws</code> directory.</p>
<h2 id="more-about-spark-and-aws">More about spark and aws<a hidden class="anchor" aria-hidden="true" href="#more-about-spark-and-aws">#</a></h2>
<p><a href="https://github.com/dimajix/docker-hadoop/blob/master/Dockerfile#L23">aws hadoop libraries (copying)</a></p>
<p><a href="https://medium.com/@subhojit20_27731/apache-spark-and-amazon-s3-gotchas-and-best-practices-a767242f3d98">Apache Spark and Amazon S3 — Gotchas and best practices</a></p>
<p><a href="https://aws.amazon.com/blogs/developer/secure-local-development-with-the-profilecredentialsprovider/">about profile credentials provider 1</a>
<a href="https://stackoverflow.com/questions/42669246/spark-is-inventing-his-own-aws-secretkey">about profile credentials provider 2</a></p>
<p><a href="https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Features">about hadoop aws s3 access</a></p>
<p><a href="https://spark.apache.org/docs/latest/cluster-overview.html">about drivers</a></p>
<p><a href="https://www.slideshare.net/rjurney/predictive-analytics-with-airflow-and-pyspark">analytics with airflow and spark</a></p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://serge-m.github.io/tags/spark/">spark</a></li>
      <li><a href="https://serge-m.github.io/tags/python/">python</a></li>
      <li><a href="https://serge-m.github.io/tags/aws/">aws</a></li>
    </ul>
    <nav class="paginav">
      <a class="prev" href="https://serge-m.github.io/posts/disk-usage-ubuntu/">
        <span class="title">« Prev Page</span>
        <br>
        <span>Reducing disk usage in Ubuntu</span>
      </a>
      <a class="next" href="https://serge-m.github.io/posts/okrs/">
        <span class="title">Next Page »</span>
        <br>
        <span>OKRs</span>
      </a>
    </nav>
<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on twitter"
        href="https://twitter.com/intent/tweet/?text=Spark%20in%20Docker%20with%20AWS%20credentials&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f&amp;hashtags=spark%2cpython%2caws">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f&amp;title=Spark%20in%20Docker%20with%20AWS%20credentials&amp;summary=Spark%20in%20Docker%20with%20AWS%20credentials&amp;source=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f&title=Spark%20in%20Docker%20with%20AWS%20credentials">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on whatsapp"
        href="https://api.whatsapp.com/send?text=Spark%20in%20Docker%20with%20AWS%20credentials%20-%20https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Spark in Docker with AWS credentials on telegram"
        href="https://telegram.me/share/url?text=Spark%20in%20Docker%20with%20AWS%20credentials&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fspark-in-docker-with-aws-credentials%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2022 <a href="https://serge-m.github.io/">sergem&#39;s personal public notebook</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>
<script defer src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>

<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>
