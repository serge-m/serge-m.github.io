<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Which pretrained backbone to choose | sergem's personal public notebook</title><meta name=keywords content="machine learning,computer vision,DNN,CNN,deep learning,backbone,pytorch"><meta name=description content="In 2020 which architecture should I use for my image classification/tracking/segmentation/&mldr; task?
I was asked on an interview that and I didn&rsquo;t have a prepared answer.
I made a small research and want to write down some thoughts.
Most of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015
Here is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017."><meta name=author content="SergeM"><link rel=canonical href=https://serge-m.github.io/posts/which-backbone-to-choose/><link href=/assets/css/stylesheet.min.6d98a2276d0cb41ef459267b3ff3ef02df70a8f16b70bbc52b20568702bc90cf.css integrity="sha256-bZiiJ20MtB70WSZ7P/PvAt9wqPFrcLvFKyBWhwK8kM8=" rel="preload stylesheet" as=style><link rel=icon href=https://serge-m.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://serge-m.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://serge-m.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://serge-m.github.io/apple-touch-icon.png><link rel=mask-icon href=https://serge-m.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.96.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-40853494-2","auto"),ga("send","pageview"))</script><meta property="og:title" content="Which pretrained backbone to choose"><meta property="og:description" content="In 2020 which architecture should I use for my image classification/tracking/segmentation/&mldr; task?
I was asked on an interview that and I didn&rsquo;t have a prepared answer.
I made a small research and want to write down some thoughts.
Most of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015
Here is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017."><meta property="og:type" content="article"><meta property="og:url" content="https://serge-m.github.io/posts/which-backbone-to-choose/"><meta property="og:image" content="https://serge-m.github.io/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-07-01T19:00:00+00:00"><meta property="article:modified_time" content="2020-07-01T19:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://serge-m.github.io/papermod-cover.png"><meta name=twitter:title content="Which pretrained backbone to choose"><meta name=twitter:description content="In 2020 which architecture should I use for my image classification/tracking/segmentation/&mldr; task?
I was asked on an interview that and I didn&rsquo;t have a prepared answer.
I made a small research and want to write down some thoughts.
Most of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015
Here is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://serge-m.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Which pretrained backbone to choose","item":"https://serge-m.github.io/posts/which-backbone-to-choose/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Which pretrained backbone to choose","name":"Which pretrained backbone to choose","description":"In 2020 which architecture should I use for my image classification/tracking/segmentation/\u0026hellip; task?\nI was asked on an interview that and I didn\u0026rsquo;t have a prepared answer.\nI made a small research and want to write down some thoughts.\nMost of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015\nHere is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017.","keywords":["machine learning","computer vision","DNN","CNN","deep learning","backbone","pytorch"],"articleBody":"In 2020 which architecture should I use for my image classification/tracking/segmentation/… task?\nI was asked on an interview that and I didn’t have a prepared answer.\nI made a small research and want to write down some thoughts.\nMost of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015\nHere is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017. It is a bit dated already.\nEfficientNet A recent paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le, 2019 explores the scaling of model hyperpameters to achieve computational effectiveness and performance in standard image classification tasks.\nThe architecture is pretty simple. They focus on the scaling parameters of the network rather than finding good building blocks.\nPretrained weights can be found for Pytorch and Tensortflow:\n  Repo for pytorch: pytorch-image-models\n  another repo for pytorch: imgclsmob\n  The performance with transfer learning seems pretty good. I have seen some doubts about using it on mobile devices. The gists is that EfficientNet is optimized with respect to the number of parameters, not actual Addition/Multiplications or FLOPS on the device. Here is a notebook with some experiments and critique of the efficient net: on google colab, on github\nGreat discussion thread on fast.ai: https://forums.fast.ai/t/efficientnet/46978/79\nMNasNet MnasNet paper MnasNet: Platform-Aware Neural Architecture Search for Mobile, Tan et. al 2019\nGood - Pareto analysis, provide a range of networks for different speed/quality tradeoff. AutoML. They use MobilenetV2 as a base.\nThose guise optimized for real mobile hardware.(see picture) They claim to be 1.8 times faster than Mobilenet V2 with the same performance.\n  our slightly larger MnasNet-A3 modelachieves better accuracy than ResNet-50 [9], but with4.8×fewerparameters and10×fewermultiply-add cost\n  They also beat YOLOv2 for object detection on COCO in terms of quality.\nMobileNet V2 Paper: MobileNetV2: Inverted Residuals and Linear Bottlenecks, Sander rt al, CVPR 2018\nArxive version\nFocus on the mobile platforms.\nThey implemented several important improvements to reduce the complexity:\n  Depthwise Separable Convolutions - instead of using C_out filters of size W x H x C_in we apply C_in filters of size W x H x 1 and then C_out filters of size 1 x 1 x C_in. Some kid of factorization.\n The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a1×1convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the in-put channels.\n   Linear Bottlenecks - for dimensionality reduction\n  Inverted residuals. Well, nothing is really inverted. Just skip connections are binding low-resoltion bottleneck layers instead of upscaled layers of hi resolution.\n  RegNet Designing Network Design Spaces paper by Facebook AI Research (FAIR).\nMedium paper: RegNet or How to methodologically design effective networks.\nA class of models that is designed for fast training and inference. The implementation and the trained weights can be found at pycls repo: https://github.com/facebookresearch/pycls\nThe authors take a rather broad space of possible architectures an sample architectures from that space. They train and evaluate the sampled architectures to find what works best.\nThey claim the network 5 times faster than EfficientNet for some configurations. For the smaller networks the difference is much lower (~2 times). That fact is in sync with the experiments from EffResNetComparison.ipynb (see above).\nInteresting example from RegNet or How to methodologically design effective networks:\n MNASNets including MobileNets and EfficientNets extensively use Depthwise convolutions to achieve SoTA performances. These convolutions could be understood as group convolutions with group width of 1. The fact that AnyNetXb populations showed that g 1 is best, does not conflict with this fact. That such networks can and do perform excellently is not under question. The paper is empirically showing, with statistical backing to back the claim, that as a design space, g = 1 might be best avoided even though the MNAS search has found particular instances in which there are good performing models to build upon.\n One more vitation from Facebook AI RegNet Models Outperform EfficientNet Models, Run 5x Faster on GPUs\n While it is common to see modern mobile networks employ inverted bottlenecks, researchers noticed that using inverted bottlenecks degrades performance. The best models do not use either a bottleneck or an inverted bottleneck.\n ResNeSt ResNeSt: Split-Attention Networks paper\nresnest code\nA pretty recent paper. The authors propose to split the channels into groups, process them with a separate sets of convolutions, and then concat them.\nThey achieve a comparable (and better) performance with EfficientNet, while having less parameters and having better frame rate.\nConclusion I would start with ResNest given it’s great performance.\nIf I want to have a smaller network with less parameters I would go for Efficient net. That would probably make sense if I run without a GPU. The architecture of EfficientNet is proven to work well in several domains.\nRegnet is not trained for the same amount of time and reports lower accuracy. The comparison provided in the paper uses re-trained Efficient Net Weights. It’s unclear whether it will be comparable with on the full-blown training.\nThe backup plan is MNas and Mobilenet. If I need to run on a smaller device I would be more careful and considered MNasNet or Mobilenet V2/V3.\n","wordCount":"897","inLanguage":"en","datePublished":"2020-07-01T19:00:00Z","dateModified":"2020-07-01T19:00:00Z","author":{"@type":"Person","name":"SergeM"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://serge-m.github.io/posts/which-backbone-to-choose/"},"publisher":{"@type":"Organization","name":"sergem's personal public notebook","logo":{"@type":"ImageObject","url":"https://serge-m.github.io/favicon.ico"}}}</script></head><body id=top><script>window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://serge-m.github.io/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://serge-m.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://serge-m.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://serge-m.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://serge-m.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://serge-m.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://serge-m.github.io/posts/>Posts</a></div><h1 class=post-title>Which pretrained backbone to choose</h1><div class=post-meta>July 1, 2020&nbsp;·&nbsp;SergeM</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#efficientnet aria-label=EfficientNet>EfficientNet</a></li><li><a href=#mnasnet aria-label=MNasNet>MNasNet</a></li><li><a href=#mobilenet-v2 aria-label="MobileNet V2">MobileNet V2</a></li><li><a href=#regnet aria-label=RegNet>RegNet</a></li><li><a href=#resnest aria-label=ResNeSt>ResNeSt</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>In 2020 which architecture should I use for my image classification/tracking/segmentation/&mldr; task?</p><p>I was asked on an interview that and I didn&rsquo;t have a prepared answer.</p><p>I made a small research and want to write down some thoughts.</p><p>Most of the architectures build upon ideas from ResNet paper <a href=https://arxiv.org/abs/1512.03385>Deep Residual Learning for Image Recognition, 2015</a></p><p>Here is some explanation of resnet family:<a href=%5Bhttps://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035>An Overview of ResNet and its Variants</a> by Vincent Fung, 2017. It is a bit dated already.</p><h2 id=efficientnet>EfficientNet<a hidden class=anchor aria-hidden=true href=#efficientnet>#</a></h2><p>A recent paper
<a href=https://arxiv.org/abs/1905.11946>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
by Mingxing Tan, Quoc V. Le, 2019</a>
explores the scaling of model hyperpameters to achieve computational effectiveness and performance in standard image classification tasks.</p><p><img loading=lazy src=/media/2020-07-01-which-backbone-to-choose/efficientnet.png alt=efficientnet></p><p>The architecture is pretty simple. They focus on the scaling parameters of the network rather than finding good building blocks.</p><p>Pretrained weights can be found for Pytorch and Tensortflow:</p><ul><li><p>Repo for pytorch: <a href=https://github.com/rwightman/pytorch-image-models>pytorch-image-models</a></p></li><li><p>another repo for pytorch: <a href=https://github.com/osmr/imgclsmob/tree/master/pytorch>imgclsmob</a></p></li></ul><p>The performance with transfer learning seems pretty good. I have seen some doubts about using it on mobile devices. The gists is that EfficientNet is optimized with respect to the number of parameters, not actual Addition/Multiplications or FLOPS on the device.
Here is a notebook with some experiments and critique of the efficient net: <a href="https://colab.research.google.com/github/rwightman/pytorch-image-models/blob/master/notebooks/EffResNetComparison.ipynb#scrollTo=iapzkrt2gBwR">on google colab</a>, <a href=https://github.com/rwightman/pytorch-image-models/blob/master/notebooks/EffResNetComparison.ipynb>on github</a></p><p>Great discussion thread on fast.ai: <a href=https://forums.fast.ai/t/efficientnet/46978/79>https://forums.fast.ai/t/efficientnet/46978/79</a></p><h2 id=mnasnet>MNasNet<a hidden class=anchor aria-hidden=true href=#mnasnet>#</a></h2><p>MnasNet paper <a href=https://arxiv.org/pdf/1807.11626.pdf>MnasNet: Platform-Aware Neural Architecture Search for Mobile</a>, Tan et. al 2019</p><p><img loading=lazy src=/media/2020-07-01-which-backbone-to-choose/mnas-scheme.png alt=mnasnet></p><p><img loading=lazy src=/media/2020-07-01-which-backbone-to-choose/mnas-performance.png alt=mnasnet></p><p>Good - Pareto analysis, provide a range of networks for different speed/quality tradeoff. AutoML. They use MobilenetV2 as a base.</p><p>Those guise optimized for real mobile hardware.(see picture)
They claim to be 1.8 times faster than Mobilenet V2 with the same performance.</p><blockquote><blockquote><p>our slightly larger MnasNet-A3 modelachieves better accuracy than ResNet-50 [9], but with4.8×fewerparameters and10×fewermultiply-add cost</p></blockquote></blockquote><p>They also beat YOLOv2 for object detection on COCO in terms of quality.</p><h2 id=mobilenet-v2>MobileNet V2<a hidden class=anchor aria-hidden=true href=#mobilenet-v2>#</a></h2><p>Paper: <a href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf>MobileNetV2: Inverted Residuals and Linear Bottlenecks, Sander rt al, CVPR 2018</a></p><p><a href=https://arxiv.org/pdf/1801.04381.pdf>Arxive version</a></p><p>Focus on the mobile platforms.</p><p>They implemented several important improvements to reduce the complexity:</p><ul><li><p>Depthwise Separable Convolutions - instead of using C_out filters of size W x H x C_in we apply C_in filters of size W x H x 1
and then C_out filters of size 1 x 1 x C_in. Some kid of factorization.</p><blockquote><p>The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a1×1convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the in-put channels.</p></blockquote></li><li><p>Linear Bottlenecks - for dimensionality reduction</p></li><li><p>Inverted residuals.
Well, nothing is really inverted. Just skip connections are binding low-resoltion bottleneck
layers instead of upscaled layers of hi resolution.</p></li></ul><h2 id=regnet>RegNet<a hidden class=anchor aria-hidden=true href=#regnet>#</a></h2><p>Designing Network Design Spaces <a href=https://arxiv.org/abs/2003.13678>paper</a> by Facebook AI Research (FAIR).</p><p>Medium paper: <a href=https://medium.com/analytics-vidhya/regnet-or-how-to-methodologically-design-effective-networks-c3560c1cf436>RegNet or How to methodologically design effective networks.</a></p><p>A class of models that is designed for fast training and inference.
The implementation and the trained weights can be found at pycls repo: <a href=https://github.com/facebookresearch/pycls>https://github.com/facebookresearch/pycls</a></p><p>The authors take a rather broad space of possible architectures an sample architectures from that space.
They train and evaluate the sampled architectures to find what works best.</p><p>They claim the network 5 times faster than EfficientNet for some configurations.
For the smaller networks the difference is much lower (~2 times).
That fact is in sync with the experiments from <code>EffResNetComparison.ipynb</code> (see above).</p><p>Interesting example from <a href=https://medium.com/analytics-vidhya/regnet-or-how-to-methodologically-design-effective-networks-c3560c1cf436>RegNet or How to methodologically design effective networks</a>:</p><blockquote><p>MNASNets including MobileNets and EfficientNets extensively use Depthwise convolutions to achieve SoTA performances. These convolutions could be understood as group convolutions with group width of 1. The fact that AnyNetXb populations showed that g >1 is best, does not conflict with this fact. That such networks can and do perform excellently is not under question. The paper is empirically showing, with statistical backing to back the claim, that as a design space, g = 1 might be best avoided even though the MNAS search has found particular instances in which there are good performing models to build upon.</p></blockquote><p>One more vitation from <a href=https://medium.com/syncedreview/facebook-ai-regnet-models-outperform-efficientnet-models-run-5x-faster-on-gpus-7bdc3ea577ae>Facebook AI RegNet Models Outperform EfficientNet Models, Run 5x Faster on GPUs</a></p><blockquote><p>While it is common to see modern mobile networks employ inverted bottlenecks, researchers noticed that using inverted bottlenecks degrades performance. The best models do not use either a bottleneck or an inverted bottleneck.</p></blockquote><h2 id=resnest>ResNeSt<a hidden class=anchor aria-hidden=true href=#resnest>#</a></h2><p>ResNeSt: Split-Attention Networks <a href=https://arxiv.org/abs/2004.08955>paper</a></p><p><a href=https://github.com/zhanghang1989/ResNeSt>resnest code</a></p><p>A pretty recent paper. The authors propose to split the channels into groups, process them with a separate sets of convolutions, and then concat them.</p><p>They achieve a comparable (and better) performance with EfficientNet, while having less parameters and having better frame rate.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>I would start with ResNest given it&rsquo;s great performance.</p><p>If I want to have a smaller network with less parameters I would go for Efficient net. That would probably make sense if I run without a GPU. The architecture of EfficientNet is proven to work well in several domains.</p><p>Regnet is not trained for the same amount of time and reports lower accuracy. The comparison provided in the paper uses re-trained Efficient Net Weights. It&rsquo;s unclear whether it will be comparable with on the full-blown training.</p><p>The backup plan is MNas and Mobilenet. If I need to run on a smaller device I would be more careful and considered MNasNet or Mobilenet V2/V3.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://serge-m.github.io/tags/machine-learning/>machine learning</a></li><li><a href=https://serge-m.github.io/tags/computer-vision/>computer vision</a></li><li><a href=https://serge-m.github.io/tags/dnn/>DNN</a></li><li><a href=https://serge-m.github.io/tags/cnn/>CNN</a></li><li><a href=https://serge-m.github.io/tags/deep-learning/>deep learning</a></li><li><a href=https://serge-m.github.io/tags/backbone/>backbone</a></li><li><a href=https://serge-m.github.io/tags/pytorch/>pytorch</a></li></ul><nav class=paginav><a class=prev href=https://serge-m.github.io/posts/self-supervised-depth-and-ego-motion/><span class=title>« Prev Page</span><br><span>Self-supervised depth and ego motion estimation</span></a>
<a class=next href=https://serge-m.github.io/posts/parameters-parsing-for-python-applications/><span class=title>Next Page »</span><br><span>Parameters parsing for python applications</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on twitter" href="https://twitter.com/intent/tweet/?text=Which%20pretrained%20backbone%20to%20choose&url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f&hashtags=machinelearning%2ccomputervision%2cDNN%2cCNN%2cdeeplearning%2cbackbone%2cpytorch"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f&title=Which%20pretrained%20backbone%20to%20choose&summary=Which%20pretrained%20backbone%20to%20choose&source=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f&title=Which%20pretrained%20backbone%20to%20choose"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on whatsapp" href="https://api.whatsapp.com/send?text=Which%20pretrained%20backbone%20to%20choose%20-%20https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on telegram" href="https://telegram.me/share/url?text=Which%20pretrained%20backbone%20to%20choose&url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://serge-m.github.io/>sergem's personal public notebook</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>