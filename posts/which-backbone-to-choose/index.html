<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Which pretrained backbone to choose | sergem&#39;s personal public notebook</title>
<meta name="keywords" content="machine learning, computer vision, DNN, CNN, deep learning, backbone, pytorch" />
<meta name="description" content="In 2020 which architecture should I use for my image classification/tracking/segmentation/&hellip; task?
I was asked on an interview that and I didn&rsquo;t have a prepared answer.
I made a small research and want to write down some thoughts.
Most of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015
Here is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017.">
<meta name="author" content="SergeM">
<link rel="canonical" href="https://serge-m.github.io/posts/which-backbone-to-choose/" />
<link href="/assets/css/stylesheet.min.4a7c1baa41934a41353b5c02dd9ecd335d4e6b47f3fa5a187a77574ac682b8ac.css" integrity="sha256-SnwbqkGTSkE1O1wC3Z7NM11Oa0fz&#43;loYendXSsaCuKw=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://serge-m.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://serge-m.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://serge-m.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://serge-m.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://serge-m.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.82.0" />

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-40853494-2', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="Which pretrained backbone to choose" />
<meta property="og:description" content="In 2020 which architecture should I use for my image classification/tracking/segmentation/&hellip; task?
I was asked on an interview that and I didn&rsquo;t have a prepared answer.
I made a small research and want to write down some thoughts.
Most of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015
Here is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://serge-m.github.io/posts/which-backbone-to-choose/" /><meta property="og:image" content="https://serge-m.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-07-01T19:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-07-01T19:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://serge-m.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Which pretrained backbone to choose"/>
<meta name="twitter:description" content="In 2020 which architecture should I use for my image classification/tracking/segmentation/&hellip; task?
I was asked on an interview that and I didn&rsquo;t have a prepared answer.
I made a small research and want to write down some thoughts.
Most of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015
Here is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://serge-m.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Which pretrained backbone to choose",
      "item": "https://serge-m.github.io/posts/which-backbone-to-choose/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Which pretrained backbone to choose",
  "name": "Which pretrained backbone to choose",
  "description": "In 2020 which architecture should I use for my image classification/tracking/segmentation/\u0026hellip; task?\nI was asked on an interview that and I didn\u0026rsquo;t have a prepared answer.\nI made a small research and want to write down some thoughts.\nMost of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015\nHere is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017.",
  "keywords": [
    "machine learning", "computer vision", "DNN", "CNN", "deep learning", "backbone", "pytorch"
  ],
  "articleBody": "In 2020 which architecture should I use for my image classification/tracking/segmentation/… task?\nI was asked on an interview that and I didn’t have a prepared answer.\nI made a small research and want to write down some thoughts.\nMost of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015\nHere is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017. It is a bit dated already.\nEfficientNet A recent paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le, 2019 explores the scaling of model hyperpameters to achieve computational effectiveness and performance in standard image classification tasks.\nThe architecture is pretty simple. They focus on the scaling parameters of the network rather than finding good building blocks.\nPretrained weights can be found for Pytorch and Tensortflow:\n  Repo for pytorch: pytorch-image-models\n  another repo for pytorch: imgclsmob\n  The performance with transfer learning seems pretty good. I have seen some doubts about using it on mobile devices. The gists is that EfficientNet is optimized with respect to the number of parameters, not actual Addition/Multiplications or FLOPS on the device. Here is a notebook with some experiments and critique of the efficient net: on google colab, on github\nGreat discussion thread on fast.ai: https://forums.fast.ai/t/efficientnet/46978/79\nMNasNet MnasNet paper MnasNet: Platform-Aware Neural Architecture Search for Mobile, Tan et. al 2019\nGood - Pareto analysis, provide a range of networks for different speed/quality tradeoff. AutoML. They use MobilenetV2 as a base.\nThose guise optimized for real mobile hardware.(see picture) They claim to be 1.8 times faster than Mobilenet V2 with the same performance.\n  our slightly larger MnasNet-A3 modelachieves better accuracy than ResNet-50 [9], but with4.8×fewerparameters and10×fewermultiply-add cost\n  They also beat YOLOv2 for object detection on COCO in terms of quality.\nMobileNet V2 Paper: MobileNetV2: Inverted Residuals and Linear Bottlenecks, Sander rt al, CVPR 2018\nArxive version\nFocus on the mobile platforms.\nThey implemented several important improvements to reduce the complexity:\n  Depthwise Separable Convolutions - instead of using C_out filters of size W x H x C_in we apply C_in filters of size W x H x 1 and then C_out filters of size 1 x 1 x C_in. Some kid of factorization.\n The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel. The second layer is a1×1convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the in-put channels.\n   Linear Bottlenecks - for dimensionality reduction\n  Inverted residuals. Well, nothing is really inverted. Just skip connections are binding low-resoltion bottleneck layers instead of upscaled layers of hi resolution.\n  RegNet Designing Network Design Spaces paper by Facebook AI Research (FAIR).\nMedium paper: RegNet or How to methodologically design effective networks.\nA class of models that is designed for fast training and inference. The implementation and the trained weights can be found at pycls repo: https://github.com/facebookresearch/pycls\nThe authors take a rather broad space of possible architectures an sample architectures from that space. They train and evaluate the sampled architectures to find what works best.\nThey claim the network 5 times faster than EfficientNet for some configurations. For the smaller networks the difference is much lower (~2 times). That fact is in sync with the experiments from EffResNetComparison.ipynb (see above).\nInteresting example from RegNet or How to methodologically design effective networks:\n MNASNets including MobileNets and EfficientNets extensively use Depthwise convolutions to achieve SoTA performances. These convolutions could be understood as group convolutions with group width of 1. The fact that AnyNetXb populations showed that g 1 is best, does not conflict with this fact. That such networks can and do perform excellently is not under question. The paper is empirically showing, with statistical backing to back the claim, that as a design space, g = 1 might be best avoided even though the MNAS search has found particular instances in which there are good performing models to build upon.\n One more vitation from Facebook AI RegNet Models Outperform EfficientNet Models, Run 5x Faster on GPUs\n While it is common to see modern mobile networks employ inverted bottlenecks, researchers noticed that using inverted bottlenecks degrades performance. The best models do not use either a bottleneck or an inverted bottleneck.\n ResNeSt ResNeSt: Split-Attention Networks paper\nresnest code\nA pretty recent paper. The authors propose to split the channels into groups, process them with a separate sets of convolutions, and then concat them.\nThey achieve a comparable (and better) performance with EfficientNet, while having less parameters and having better frame rate.\nConclusion I would start with ResNest given it’s great performance.\nIf I want to have a smaller network with less parameters I would go for Efficient net. That would probably make sense if I run without a GPU. The architecture of EfficientNet is proven to work well in several domains.\nRegnet is not trained for the same amount of time and reports lower accuracy. The comparison provided in the paper uses re-trained Efficient Net Weights. It’s unclear whether it will be comparable with on the full-blown training.\nThe backup plan is MNas and Mobilenet. If I need to run on a smaller device I would be more careful and considered MNasNet or Mobilenet V2/V3.\n",
  "wordCount" : "897",
  "inLanguage": "en",
  "datePublished": "2020-07-01T19:00:00Z",
  "dateModified": "2020-07-01T19:00:00Z",
  "author":{
    "@type": "Person",
    "name": "SergeM"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://serge-m.github.io/posts/which-backbone-to-choose/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "sergem's personal public notebook",
    "logo": {
      "@type": "ImageObject",
      "url": "https://serge-m.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://serge-m.github.io/" accesskey="h" title="Home (Alt + H)">Home</a>
            <span class="logo-switches">
            </span>
        </div>
        <ul id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://serge-m.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://serge-m.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://serge-m.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://serge-m.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Which pretrained backbone to choose
    </h1>
    <div class="post-meta">July 1, 2020&nbsp;·&nbsp;SergeM
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#efficientnet" aria-label="EfficientNet">EfficientNet</a></li>
                <li>
                    <a href="#mnasnet" aria-label="MNasNet">MNasNet</a></li>
                <li>
                    <a href="#mobilenet-v2" aria-label="MobileNet V2">MobileNet V2</a></li>
                <li>
                    <a href="#regnet" aria-label="RegNet">RegNet</a></li>
                <li>
                    <a href="#resnest" aria-label="ResNeSt">ResNeSt</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In 2020 which architecture should I use for my image classification/tracking/segmentation/&hellip; task?</p>
<p>I was asked on an interview that and I didn&rsquo;t have a prepared answer.</p>
<p>I made a small research and want to write down some thoughts.</p>
<p>Most of the architectures build upon ideas from ResNet paper <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition, 2015</a></p>
<p>Here is some explanation of resnet family:<a href="%5Bhttps://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035">An Overview of ResNet and its Variants</a> by Vincent Fung, 2017. It is a bit dated already.</p>
<h2 id="efficientnet">EfficientNet<a hidden class="anchor" aria-hidden="true" href="#efficientnet">#</a></h2>
<p>A recent paper
<a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
by Mingxing Tan, Quoc V. Le, 2019</a>
explores the scaling of model hyperpameters to achieve computational effectiveness and performance in standard image classification tasks.</p>
<p><img loading="lazy" src="/media/2020-07-01-which-backbone-to-choose/efficientnet.png" alt="efficientnet"  />
</p>
<p>The architecture is pretty simple. They focus on the scaling parameters of the network rather than finding good building blocks.</p>
<p>Pretrained weights can be found for Pytorch and Tensortflow:</p>
<ul>
<li>
<p>Repo for pytorch: <a href="https://github.com/rwightman/pytorch-image-models">pytorch-image-models</a></p>
</li>
<li>
<p>another repo for pytorch: <a href="https://github.com/osmr/imgclsmob/tree/master/pytorch">imgclsmob</a></p>
</li>
</ul>
<p>The performance with transfer learning seems pretty good. I have seen some doubts about using it on mobile devices. The gists is that EfficientNet is optimized with respect to the number of parameters, not actual Addition/Multiplications or FLOPS on the device.
Here is a notebook with some experiments and critique of the efficient net: <a href="https://colab.research.google.com/github/rwightman/pytorch-image-models/blob/master/notebooks/EffResNetComparison.ipynb#scrollTo=iapzkrt2gBwR">on google colab</a>, <a href="https://github.com/rwightman/pytorch-image-models/blob/master/notebooks/EffResNetComparison.ipynb">on github</a></p>
<p>Great discussion thread on fast.ai: <a href="https://forums.fast.ai/t/efficientnet/46978/79">https://forums.fast.ai/t/efficientnet/46978/79</a></p>
<h2 id="mnasnet">MNasNet<a hidden class="anchor" aria-hidden="true" href="#mnasnet">#</a></h2>
<p>MnasNet paper <a href="https://arxiv.org/pdf/1807.11626.pdf">MnasNet: Platform-Aware Neural Architecture Search for Mobile</a>, Tan et. al 2019</p>
<p><img loading="lazy" src="/media/2020-07-01-which-backbone-to-choose/mnas-scheme.png" alt="mnasnet"  />
</p>
<p><img loading="lazy" src="/media/2020-07-01-which-backbone-to-choose/mnas-performance.png" alt="mnasnet"  />
</p>
<p>Good - Pareto analysis, provide a range of networks for different speed/quality tradeoff. AutoML. They use MobilenetV2 as a base.</p>
<p>Those guise optimized for real mobile hardware.(see picture)
They claim to be 1.8 times faster  than Mobilenet V2 with the same performance.</p>
<blockquote>
<blockquote>
<p>our  slightly  larger  MnasNet-A3  modelachieves better accuracy than ResNet-50 [9], but with4.8×fewerparameters and10×fewermultiply-add cost</p>
</blockquote>
</blockquote>
<p>They also beat YOLOv2 for object detection on COCO in terms of quality.</p>
<h2 id="mobilenet-v2">MobileNet V2<a hidden class="anchor" aria-hidden="true" href="#mobilenet-v2">#</a></h2>
<p>Paper:  <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf">MobileNetV2: Inverted Residuals and Linear Bottlenecks, Sander rt al, CVPR 2018</a></p>
<p><a href="https://arxiv.org/pdf/1801.04381.pdf">Arxive version</a></p>
<p>Focus on the mobile platforms.</p>
<p>They implemented several important improvements to reduce the complexity:</p>
<ul>
<li>
<p>Depthwise Separable Convolutions - instead of using C_out filters  of size W x H x C_in  we apply C_in filters of size W x H x 1
and then C_out filters of size 1 x 1 x C_in. Some kid of factorization.</p>
<blockquote>
<p>The basic idea is to replace a full convolutional operator with a factorized version that splits convolution into two separate layers. The first layer is called a depthwise convolution, it performs lightweight filtering by applying a single convolutional filter per input channel.  The second layer is a1×1convolution, called a pointwise convolution, which is responsible for building new features through computing linear combinations of the in-put channels.</p>
</blockquote>
</li>
<li>
<p>Linear Bottlenecks - for dimensionality reduction</p>
</li>
<li>
<p>Inverted residuals.
Well, nothing is really inverted. Just skip connections are binding low-resoltion bottleneck
layers instead of upscaled layers of hi resolution.</p>
</li>
</ul>
<h2 id="regnet">RegNet<a hidden class="anchor" aria-hidden="true" href="#regnet">#</a></h2>
<p>Designing Network Design Spaces <a href="https://arxiv.org/abs/2003.13678">paper</a> by Facebook AI Research (FAIR).</p>
<p>Medium paper: <a href="https://medium.com/analytics-vidhya/regnet-or-how-to-methodologically-design-effective-networks-c3560c1cf436">RegNet or How to methodologically design effective networks.</a></p>
<p>A class of models that is designed for fast training and inference.
The implementation and the trained weights can be found at pycls repo: <a href="https://github.com/facebookresearch/pycls">https://github.com/facebookresearch/pycls</a></p>
<p>The authors take a rather broad space of possible architectures an sample architectures from that space.
They train and evaluate the sampled architectures to find what works best.</p>
<p>They claim the network 5 times faster than EfficientNet for some configurations.
For the smaller networks the difference is much lower (~2 times).
That fact is in sync with the experiments from <code>EffResNetComparison.ipynb</code> (see above).</p>
<p>Interesting example from <a href="https://medium.com/analytics-vidhya/regnet-or-how-to-methodologically-design-effective-networks-c3560c1cf436">RegNet or How to methodologically design effective networks</a>:</p>
<blockquote>
<p>MNASNets including MobileNets and EfficientNets extensively use Depthwise convolutions to achieve SoTA performances. These convolutions could be understood as group convolutions with group width of 1. The fact that AnyNetXb populations showed that g &gt;1 is best, does not conflict with this fact. That such networks can and do perform excellently is not under question. The paper is empirically showing, with statistical backing to back the claim, that as a design space, g = 1 might be best avoided even though the MNAS search has found particular instances in which there are good performing models to build upon.</p>
</blockquote>
<p>One more vitation from  <a href="https://medium.com/syncedreview/facebook-ai-regnet-models-outperform-efficientnet-models-run-5x-faster-on-gpus-7bdc3ea577ae">Facebook AI RegNet Models Outperform EfficientNet Models, Run 5x Faster on GPUs</a></p>
<blockquote>
<p>While it is common to see modern mobile networks employ inverted bottlenecks, researchers noticed that using inverted bottlenecks degrades performance. The best models do not use either a bottleneck or an inverted bottleneck.</p>
</blockquote>
<h2 id="resnest">ResNeSt<a hidden class="anchor" aria-hidden="true" href="#resnest">#</a></h2>
<p>ResNeSt: Split-Attention Networks <a href="https://arxiv.org/abs/2004.08955">paper</a></p>
<p><a href="https://github.com/zhanghang1989/ResNeSt">resnest code</a></p>
<p>A pretty recent paper. The authors propose to split the channels into groups, process them with a separate sets of convolutions, and then concat them.</p>
<p>They achieve a comparable (and better) performance with EfficientNet, while having less parameters and having better frame rate.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>I would start with ResNest given it&rsquo;s great performance.</p>
<p>If I want to have a smaller network with less parameters I would go for Efficient net. That would probably make sense if I run without a GPU. The architecture of EfficientNet is proven to work well in several domains.</p>
<p>Regnet is not trained for the same amount of time and reports lower accuracy. The comparison provided in the paper uses re-trained Efficient Net Weights. It&rsquo;s unclear whether it will be comparable with on the full-blown training.</p>
<p>The backup plan is MNas and Mobilenet. If I need to run on a smaller device I would be more careful and considered MNasNet or Mobilenet V2/V3.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://serge-m.github.io/tags/machine-learning/">machine learning</a></li>
      <li><a href="https://serge-m.github.io/tags/computer-vision/">computer vision</a></li>
      <li><a href="https://serge-m.github.io/tags/dnn/">DNN</a></li>
      <li><a href="https://serge-m.github.io/tags/cnn/">CNN</a></li>
      <li><a href="https://serge-m.github.io/tags/deep-learning/">deep learning</a></li>
      <li><a href="https://serge-m.github.io/tags/backbone/">backbone</a></li>
      <li><a href="https://serge-m.github.io/tags/pytorch/">pytorch</a></li>
    </ul>
    <nav class="paginav">
      <a class="prev" href="https://serge-m.github.io/posts/self-supervised-depth-and-ego-motion/">
        <span class="title">« Prev Page</span>
        <br>
        <span>Self-supervised depth and ego motion estimation</span>
      </a>
      <a class="next" href="https://serge-m.github.io/posts/parameters-parsing-for-python-applications/">
        <span class="title">Next Page »</span>
        <br>
        <span>Parameters parsing for python applications</span>
      </a>
    </nav>
<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on twitter"
        href="https://twitter.com/intent/tweet/?text=Which%20pretrained%20backbone%20to%20choose&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f&amp;hashtags=machinelearning%2ccomputervision%2cDNN%2cCNN%2cdeeplearning%2cbackbone%2cpytorch">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f&amp;title=Which%20pretrained%20backbone%20to%20choose&amp;summary=Which%20pretrained%20backbone%20to%20choose&amp;source=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f&title=Which%20pretrained%20backbone%20to%20choose">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on whatsapp"
        href="https://api.whatsapp.com/send?text=Which%20pretrained%20backbone%20to%20choose%20-%20https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Which pretrained backbone to choose on telegram"
        href="https://telegram.me/share/url?text=Which%20pretrained%20backbone%20to%20choose&amp;url=https%3a%2f%2fserge-m.github.io%2fposts%2fwhich-backbone-to-choose%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://serge-m.github.io/">sergem&#39;s personal public notebook</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>
<script defer src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>

<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
</body>

</html>
