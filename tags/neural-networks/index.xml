<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>neural networks on sergem&#39;s personal public notebook</title>
    <link>https://serge-m.github.io/tags/neural-networks/</link>
    <description>Recent content in neural networks on sergem&#39;s personal public notebook</description>
    <image>
      <url>https://serge-m.github.io/papermod-cover.png</url>
      <link>https://serge-m.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Nov 2020 10:00:00 +0000</lastBuildDate><atom:link href="https://serge-m.github.io/tags/neural-networks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Effect of techniques from Fast.ai</title>
      <link>https://serge-m.github.io/posts/effect-of-techniques-from-fastai/</link>
      <pubDate>Sun, 15 Nov 2020 10:00:00 +0000</pubDate>
      
      <guid>https://serge-m.github.io/posts/effect-of-techniques-from-fastai/</guid>
      <description>fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.
Here I want to reproduce some of the techniques in order to understand what is the effect they bring.</description>
    </item>
    
    <item>
      <title>Multistage NN training experiment</title>
      <link>https://serge-m.github.io/posts/multistage-nn-training-experiment/</link>
      <pubDate>Wed, 01 Jan 2020 10:00:00 +0000</pubDate>
      
      <guid>https://serge-m.github.io/posts/multistage-nn-training-experiment/</guid>
      <description>Ideas for multistage NN training.
There is some research on continuous learning without catastrophic forgetting . For example ANML: Learning to Continually Learn (ECAI 2020) arxiv code video
The code for the paper is based on another one: OML (Online-aware Meta-learning) ~ NeurIPS19 code video
OML paper derives some code from MAML:
Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks pdf official tf code, also includes some links to other implementations.</description>
    </item>
    
    <item>
      <title>deep learning</title>
      <link>https://serge-m.github.io/posts/deep-learning/</link>
      <pubDate>Fri, 03 Jun 2016 23:08:00 +0000</pubDate>
      
      <guid>https://serge-m.github.io/posts/deep-learning/</guid>
      <description>Rest API example for tensorflow. It works: demo
Trained models for tensorflow
TF-slim - high-level API of TensorFlow for defining, training and evaluating complex models. Doesn&amp;rsquo;t work for python 3 (see here)
VGG16 and VGG19 in Tensorflow. One more here. And one more
Deep learning for lazybones
Inception-like CNN model based on 1d convolutions http://arxiv.org/pdf/1512.00567v3.pdf
Chat (in russian) http://closedcircles.com/?invite=99b1ac08509c560137b2e3c54d4398b0fa4c175e</description>
    </item>
    
  </channel>
</rss>
