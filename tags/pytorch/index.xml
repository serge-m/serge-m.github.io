<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pytorch on sergem&#39;s personal public notebook</title>
    <link>https://serge-m.github.io/tags/pytorch/</link>
    <description>Recent content in pytorch on sergem&#39;s personal public notebook</description>
    <image>
      <url>https://serge-m.github.io/papermod-cover.png</url>
      <link>https://serge-m.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 15 Nov 2020 10:00:00 +0000</lastBuildDate><atom:link href="https://serge-m.github.io/tags/pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Effect of techniques from Fast.ai</title>
      <link>https://serge-m.github.io/posts/effect-of-techniques-from-fastai/</link>
      <pubDate>Sun, 15 Nov 2020 10:00:00 +0000</pubDate>
      
      <guid>https://serge-m.github.io/posts/effect-of-techniques-from-fastai/</guid>
      <description>fast.ai is a brilliant library and a course by Jeremy Howard an co. They use pytorch as a base and explain deep learning from the foundations to a very decent level. In his course Jeremy Howard demonstrates a lot of interesting techniques that he finds in papers and that do NN training faster/better/cheaper.
Here I want to reproduce some of the techniques in order to understand what is the effect they bring.</description>
    </item>
    
    <item>
      <title>Which pretrained backbone to choose</title>
      <link>https://serge-m.github.io/posts/which-backbone-to-choose/</link>
      <pubDate>Wed, 01 Jul 2020 19:00:00 +0000</pubDate>
      
      <guid>https://serge-m.github.io/posts/which-backbone-to-choose/</guid>
      <description>In 2020 which architecture should I use for my image classification/tracking/segmentation/&amp;hellip; task?
I was asked on an interview that and I didn&amp;rsquo;t have a prepared answer.
I made a small research and want to write down some thoughts.
Most of the architectures build upon ideas from ResNet paper Deep Residual Learning for Image Recognition, 2015
Here is some explanation of resnet family:An Overview of ResNet and its Variants by Vincent Fung, 2017.</description>
    </item>
    
    <item>
      <title>Multistage NN training experiment</title>
      <link>https://serge-m.github.io/posts/multistage-nn-training-experiment/</link>
      <pubDate>Wed, 01 Jan 2020 10:00:00 +0000</pubDate>
      
      <guid>https://serge-m.github.io/posts/multistage-nn-training-experiment/</guid>
      <description>Ideas for multistage NN training.
There is some research on continuous learning without catastrophic forgetting . For example ANML: Learning to Continually Learn (ECAI 2020) arxiv code video
The code for the paper is based on another one: OML (Online-aware Meta-learning) ~ NeurIPS19 code video
OML paper derives some code from MAML:
Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks pdf official tf code, also includes some links to other implementations.</description>
    </item>
    
    <item>
      <title>Torch-Lightning library (draft)</title>
      <link>https://serge-m.github.io/posts/torch-lightning-library/</link>
      <pubDate>Sat, 29 Apr 2000 19:00:00 +0000</pubDate>
      
      <guid>https://serge-m.github.io/posts/torch-lightning-library/</guid>
      <description>How to visualize gradients with torch-lightning and tensorboard in your model class define a optimizer_step.
class Model(pl.LightningModule): # ... def optimizer_step( self, epoch: int, batch_idx: int, optimizer, optimizer_idx: int, second_order_closure = None, ) -&amp;gt; None: if self.trainer.use_tpu and XLA_AVAILABLE: xm.optimizer_step(optimizer) elif isinstance(optimizer, torch.optim.LBFGS): optimizer.step(second_order_closure) else: optimizer.step() #### Gradient reporting start ### if batch_idx % 500 == 0: for tag, param in self.model.named_parameters(): self.logger.experiment.add_histogram(&#39;{}_grad&#39;.format(tag), param.grad.cpu().detach()) #### Gradient reporting end ### # clear gradients optimizer.</description>
    </item>
    
  </channel>
</rss>
